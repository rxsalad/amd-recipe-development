root@rs-amd-validation-test1:~/data/recipes# kubectl get pods
NAME                                 READY   STATUS    RESTARTS   AGE
mi325-general-5788f487b4-kk6tn       1/1     Running   0          27h
mi350-vllm-latest-7b55975566-jvqmm   1/1     Running   0          101s


root@rs-amd-validation-test1:~/data/recipes# kubectl exec -it mi350-vllm-latest-7b55975566-jvqmm -- /bin/bash
root@mi350-vllm-latest-7b55975566-jvqmm:/app# 


root@mi350-vllm-latest-7b55975566-jvqmm:/app# vllm --version
[aiter] import [module_aiter_enum] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_aiter_enum.so
0.15.1+rocm700


root@mi350-vllm-latest-7b55975566-jvqmm:/app# curl http://localhost:8000/v1/chat/completions   -H "Content-Type: application/json"   -d '{
    "model": "amd/DeepSeek-R1-MXFP4",
    "messages": [{"role": "user", "content": "who are you?"}]
  }'
{"id":"chatcmpl-aaeffca2fd03ba45","object":"chat.completion","created":1771195720,"model":"amd/DeepSeek-R1-MXFP4","choices":[{"index":0,"message":{"role":"assistant","content":"<think>\n\n</think>\n\nGreetings! I'm DeepSeek-R1, an artificial intelligence assistant created by DeepSeek. I'm at your service and would be delighted to assist you with any inquiries or tasks you may have.","refusal":null,"annotations":null,"audio":null,"function_call":null,"tool_calls":[],"reasoning":null,"reasoning_content":null},"logprobs":null,"finish_reason":"stop","stop_reason":null,"token_ids":null}],"service_tier":null,"system_fingerprint":null,"usage":{"prompt_tokens":7,"total_tokens":54,"completion_tokens":47,"prompt_tokens_details":null},"prompt_logprobs":null,"prompt_token_ids":null,"kv_transfer_params":null}


root@mi350-vllm-latest-7b55975566-jvqmm:/app# curl http://localhost:8000/v1/chat/completions   -H "Content-Type: application/json"   -d '{
    "model": "amd/DeepSeek-R1-MXFP4",
    "messages": [{"role": "user", "content": "Are you based on DeepSeek V3?"}]
  }'
{"id":"chatcmpl-900b1e0e1d324285","object":"chat.completion","created":1771195803,"model":"amd/DeepSeek-R1-MXFP4","choices":[{"index":0,"message":{"role":"assistant","content":"<think>\n\n</think>\n\nHi! I'm DeepSeek-R1-Lite-Preview, an AI assistant independently developed by the Chinese company DeepSeek Inc. For detailed information about models and products, please refer to the official documentation or official announcements.","refusal":null,"annotations":null,"audio":null,"function_call":null,"tool_calls":[],"reasoning":null,"reasoning_content":null},"logprobs":null,"finish_reason":"stop","stop_reason":null,"token_ids":null}],"service_tier":null,"system_fingerprint":null,"usage":{"prompt_tokens":13,"total_tokens":64,"completion_tokens":51,"prompt_tokens_details":null},"prompt_logprobs":null,"prompt_token_ids":null,"kv_transfer_params":null}


root@mi350-vllm-latest-7b55975566-jvqmm:/app# vllm bench serve \
  --model "amd/DeepSeek-R1-MXFP4" \
  --dataset-name random \
  --random-input-len 8192 \
  --random-output-len 1024 \
  --request-rate 10000 \
  --num-prompts 4 \
  --ignore-eos \
  --trust-remote-code 
[aiter] import [module_aiter_enum] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_aiter_enum.so
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7ef4443b9ee0>, seed=0, num_prompts=4, dataset_name='random', no_stream=False, dataset_path=None, no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=8192, random_output_len=1024, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=8000, endpoint='/v1/completions', header=None, max_concurrency=None, model='amd/DeepSeek-R1-MXFP4', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=10000.0, burstiness=1.0, trust_remote_code=True, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=True, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-3883f030-', top_p=None, top_k=None, min_p=None, temperature=None, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=0, extra_body=None)
INFO 02-15 22:50:35 [datasets.py:612] Sampling input_len from [8191, 8191] and output_len from [1024, 1024]
WARNING: vllm bench serve no longer sets temperature==0 (greedy) in requests by default. The default will be determined on the server side and can be model/API specific. For the old behavior, include --temperature=0.
Starting initial single prompt test run...
Skipping endpoint ready check.
Starting main benchmark run...
Traffic request rate: 10000.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: None
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:25<00:00,  6.46s/it]
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     4         
Failed requests:                         0         
Request rate configured (RPS):           10000.00  
Benchmark duration (s):                  25.82     
Total input tokens:                      32764     
Total generated tokens:                  4096      
Request throughput (req/s):              0.15      
Output token throughput (tok/s):         158.63    
Peak output token throughput (tok/s):    188.00    
Peak concurrent requests:                4.00      
Total token throughput (tok/s):          1427.56   
---------------Time to First Token----------------
Mean TTFT (ms):                          3120.00   
Median TTFT (ms):                        3380.07   
P99 TTFT (ms):                           3380.63   
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          22.18     
Median TPOT (ms):                        21.93     
P99 TPOT (ms):                           22.88     
---------------Inter-token Latency----------------
Mean ITL (ms):                           22.18     
Median ITL (ms):                         21.95     
P99 ITL (ms):                            22.82     
==================================================


root@rs-amd-validation-test1:~/data/recipes# kubectl logs -f mi350-vllm-latest-7b55975566-jvqmm
+ max_num_seqs=32
+ max_num_batched_tokens=163840
+ max_seq_len_to_capture=1024
+ tensor_parallel_size=8
+ max_model_len=70000
+ MODEL=amd/DeepSeek-R1-MXFP4
+ unset FLATMM_HIP_CLANG_PATH
+ VLLM_USE_V1=1
+ VLLM_DISABLE_COMPILE_CACHE=1
+ AMDGCN_USE_BUFFER_OPS=1
+ VLLM_TORCH_PROFILER_RECORD_SHAPES=1
+ VLLM_ROCM_USE_AITER=1
+ VLLM_TRITON_FP4_GEMM_USE_ASM=0
+ VLLM_ROCM_USE_AITER_FP4_ASM_GEMM=0
+ VLLM_ROCM_USE_AITER_MHA=1
+ VLLM_ROCM_USE_AITER_MLA=0
+ VLLM_ROCM_USE_CK_MXFP4_MOE=1
+ VLLM_TORCH_PROFILER_DIR=.
+ VLLM_TORCH_PROFILER_WITH_STACK=0
+ VLLM_ROCM_USE_AITER_TRITON_MLA=0
+ VLLM_ROCM_USE_AITER_TRITON_FUSED_SHARED_EXPERTS=1
+ VLLM_ROCM_USE_AITER_TRITON_FUSED_RMSNORM_FP4_QUANT=1
+ VLLM_ROCM_USE_AITER_TRITON_FUSED_ROPE_ZEROS_KV_CACHE=1
+ VLLM_ROCM_USE_AITER_TRITON_MXFP4_BMM=1
+ VLLM_ROCM_USE_AITER_TRITON_FUSED_MUL_ADD=1
+ VLLM_ROCM_USE_AITER_TRITON_FP8_BMM=0
+ vllm serve amd/DeepSeek-R1-MXFP4 --host localhost --port 8000 --swap-space 64 --disable-log-requests --dtype auto --tensor-parallel-size 8 --max-num-seqs 32 --distributed-executor-backend mp --trust-remote-code --block-size 1 '--compilation-config={"cudagraph_mode":"FULL","custom_ops":["+rms_norm","+silu_and_mul","+quant_fp8"],"splitting_ops":[]}' --gpu-memory-utilization 0.95 --max-model-len 70000 --kv-cache-dtype fp8 --max-num-batched-tokens 163840 --async-scheduling
[aiter] import [module_aiter_enum] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_aiter_enum.so
WARNING 02-15 22:44:20 [profiler.py:110] Using VLLM_TORCH_PROFILER_DIR environment variable is deprecated and will be removed in v0.15.0 or v1.0.0, whichever is soonest. Please use --profiler-config.torch_profiler_dir command line argument or ProfilerConfig(torch_profiler_dir=...) config field instead.
WARNING 02-15 22:44:20 [profiler.py:110] Using VLLM_TORCH_PROFILER_RECORD_SHAPES environment variable is deprecated and will be removed in v0.15.0 or v1.0.0, whichever is soonest. Please use --profiler-config.torch_profiler_record_shapes command line argument or ProfilerConfig(torch_profiler_record_shapes=...) config field instead.
WARNING 02-15 22:44:20 [profiler.py:110] Using VLLM_TORCH_PROFILER_WITH_STACK environment variable is deprecated and will be removed in v0.15.0 or v1.0.0, whichever is soonest. Please use --profiler-config.torch_profiler_with_stack command line argument or ProfilerConfig(torch_profiler_with_stack=...) config field instead.
(APIServer pid=1) INFO 02-15 22:44:20 [utils.py:325] 
(APIServer pid=1) INFO 02-15 22:44:20 [utils.py:325]        █     █     █▄   ▄█
(APIServer pid=1) INFO 02-15 22:44:20 [utils.py:325]  ▄▄ ▄█ █     █     █ ▀▄▀ █  version 0.15.1
(APIServer pid=1) INFO 02-15 22:44:20 [utils.py:325]   █▄█▀ █     █     █     █  model   amd/DeepSeek-R1-MXFP4
(APIServer pid=1) INFO 02-15 22:44:20 [utils.py:325]    ▀▀  ▀▀▀▀▀ ▀▀▀▀▀ ▀     ▀
(APIServer pid=1) INFO 02-15 22:44:20 [utils.py:325] 
(APIServer pid=1) INFO 02-15 22:44:20 [utils.py:261] non-default args: {'model_tag': 'amd/DeepSeek-R1-MXFP4', 'api_server_count': 1, 'host': 'localhost', 'model': 'amd/DeepSeek-R1-MXFP4', 'trust_remote_code': True, 'max_model_len': 70000, 'distributed_executor_backend': 'mp', 'tensor_parallel_size': 8, 'block_size': 1, 'gpu_memory_utilization': 0.95, 'swap_space': 64.0, 'kv_cache_dtype': 'fp8', 'max_num_batched_tokens': 163840, 'max_num_seqs': 32, 'async_scheduling': True, 'compilation_config': {'level': None, 'mode': None, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['+rms_norm', '+silu_and_mul', '+quant_fp8'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': None, 'compile_ranges_split_points': None, 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL: 2>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': None, 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': None, 'pass_config': {}, 'max_cudagraph_capture_size': None, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None, 'static_all_moe_layers': []}}
(APIServer pid=1) The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
(APIServer pid=1) The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
(APIServer pid=1) INFO 02-15 22:44:21 [config.py:391] Replacing legacy 'type' key with 'rope_type'
(APIServer pid=1) INFO 02-15 22:44:28 [model.py:541] Resolved architecture: DeepseekV3ForCausalLM
(APIServer pid=1) INFO 02-15 22:44:28 [model.py:1561] Using max model len 70000
(APIServer pid=1) INFO 02-15 22:44:28 [cache.py:216] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. Meanwhile, it may cause accuracy drop without a proper scaling factor.
(APIServer pid=1) INFO 02-15 22:44:28 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=163840.
(APIServer pid=1) INFO 02-15 22:44:28 [vllm.py:624] Asynchronous scheduling is enabled.
(APIServer pid=1) WARNING 02-15 22:44:28 [vllm.py:905] No piecewise cudagraph for executing cascade attention. Will fall back to eager execution if a batch runs into cascade attentions.
[aiter] import [module_aiter_enum] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_aiter_enum.so
(EngineCore_DP0 pid=303) INFO 02-15 22:44:35 [core.py:96] Initializing a V1 LLM engine (v0.15.1) with config: model='amd/DeepSeek-R1-MXFP4', speculative_config=None, tokenizer='amd/DeepSeek-R1-MXFP4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=70000, download_dir=None, load_format=auto, tensor_parallel_size=8, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=quark, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=fp8, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=amd/DeepSeek-R1-MXFP4, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['+rms_norm', '+silu_and_mul', '+quant_fp8', 'none', '+rms_norm', '+quant_fp8', '+grouped_topk', '+sparse_attn_indexer'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [163840], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL: 2>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': True, 'fuse_act_quant': True, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 64, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None, 'static_all_moe_layers': []}
(EngineCore_DP0 pid=303) WARNING 02-15 22:44:35 [multiproc_executor.py:910] Reducing Torch parallelism from 192 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[aiter] import [module_aiter_enum] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_aiter_enum.so
[aiter] import [module_aiter_enum] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_aiter_enum.so
[aiter] import [module_aiter_enum] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_aiter_enum.so
[aiter] import [module_aiter_enum] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_aiter_enum.so
[aiter] import [module_aiter_enum] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_aiter_enum.so
[aiter] import [module_aiter_enum] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_aiter_enum.so
[aiter] import [module_aiter_enum] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_aiter_enum.so
[aiter] import [module_aiter_enum] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_aiter_enum.so
INFO 02-15 22:44:42 [parallel_state.py:1212] world_size=8 rank=4 local_rank=4 distributed_init_method=tcp://127.0.0.1:59111 backend=nccl
INFO 02-15 22:44:42 [wrapper.py:164] Torch profiling enabled. Traces will be saved to: /app
INFO 02-15 22:44:42 [parallel_state.py:1212] world_size=8 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:59111 backend=nccl
INFO 02-15 22:44:42 [parallel_state.py:1212] world_size=8 rank=5 local_rank=5 distributed_init_method=tcp://127.0.0.1:59111 backend=nccl
INFO 02-15 22:44:42 [parallel_state.py:1212] world_size=8 rank=2 local_rank=2 distributed_init_method=tcp://127.0.0.1:59111 backend=nccl
INFO 02-15 22:44:42 [parallel_state.py:1212] world_size=8 rank=3 local_rank=3 distributed_init_method=tcp://127.0.0.1:59111 backend=nccl
INFO 02-15 22:44:42 [parallel_state.py:1212] world_size=8 rank=7 local_rank=7 distributed_init_method=tcp://127.0.0.1:59111 backend=nccl
INFO 02-15 22:44:42 [parallel_state.py:1212] world_size=8 rank=6 local_rank=6 distributed_init_method=tcp://127.0.0.1:59111 backend=nccl
INFO 02-15 22:44:42 [parallel_state.py:1212] world_size=8 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:59111 backend=nccl
INFO 02-15 22:44:42 [pynccl.py:111] vLLM is using nccl==2.26.6
INFO 02-15 22:44:53 [parallel_state.py:1423] rank 5 in world size 8 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 5, EP rank 5
INFO 02-15 22:44:53 [parallel_state.py:1423] rank 7 in world size 8 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 7, EP rank 7
INFO 02-15 22:44:53 [parallel_state.py:1423] rank 1 in world size 8 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank 1
INFO 02-15 22:44:53 [parallel_state.py:1423] rank 4 in world size 8 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 4, EP rank 4
INFO 02-15 22:44:53 [parallel_state.py:1423] rank 6 in world size 8 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 6, EP rank 6
INFO 02-15 22:44:53 [parallel_state.py:1423] rank 2 in world size 8 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 2, EP rank 2
INFO 02-15 22:44:53 [parallel_state.py:1423] rank 0 in world size 8 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
INFO 02-15 22:44:53 [parallel_state.py:1423] rank 3 in world size 8 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 3, EP rank 3
INFO 02-15 22:44:54 [topk_topp_sampler.py:77] Using aiter sampler on ROCm (lazy import, sampling-only).
INFO 02-15 22:44:54 [topk_topp_sampler.py:77] Using aiter sampler on ROCm (lazy import, sampling-only).
INFO 02-15 22:44:54 [topk_topp_sampler.py:77] Using aiter sampler on ROCm (lazy import, sampling-only).
INFO 02-15 22:44:54 [topk_topp_sampler.py:77] Using aiter sampler on ROCm (lazy import, sampling-only).
INFO 02-15 22:44:54 [topk_topp_sampler.py:77] Using aiter sampler on ROCm (lazy import, sampling-only).
INFO 02-15 22:44:54 [topk_topp_sampler.py:77] Using aiter sampler on ROCm (lazy import, sampling-only).
INFO 02-15 22:44:54 [topk_topp_sampler.py:77] Using aiter sampler on ROCm (lazy import, sampling-only).
INFO 02-15 22:44:54 [topk_topp_sampler.py:77] Using aiter sampler on ROCm (lazy import, sampling-only).
(Worker_TP0 pid=520) INFO 02-15 22:44:54 [gpu_model_runner.py:4033] Starting to load model amd/DeepSeek-R1-MXFP4...
(Worker_TP5 pid=525) INFO 02-15 22:44:55 [rocm.py:266] Using AITER MLA backend.
(Worker_TP6 pid=526) INFO 02-15 22:44:55 [rocm.py:266] Using AITER MLA backend.
(Worker_TP7 pid=527) INFO 02-15 22:44:55 [rocm.py:266] Using AITER MLA backend.
(Worker_TP1 pid=521) INFO 02-15 22:44:55 [rocm.py:266] Using AITER MLA backend.
(Worker_TP2 pid=522) INFO 02-15 22:44:55 [rocm.py:266] Using AITER MLA backend.
(Worker_TP4 pid=524) INFO 02-15 22:44:55 [rocm.py:266] Using AITER MLA backend.
(Worker_TP0 pid=520) INFO 02-15 22:44:55 [rocm.py:266] Using AITER MLA backend.
(Worker_TP0 pid=520) INFO 02-15 22:44:55 [mla_attention.py:1399] Using FlashAttention prefill for MLA
(Worker_TP5 pid=525) WARNING 02-15 22:44:55 [quark_moe.py:613] The current mode supports native MoE MXFP4 computation
(Worker_TP1 pid=521) WARNING 02-15 22:44:55 [quark_moe.py:613] The current mode supports native MoE MXFP4 computation
(Worker_TP7 pid=527) WARNING 02-15 22:44:55 [quark_moe.py:613] The current mode supports native MoE MXFP4 computation
(Worker_TP6 pid=526) WARNING 02-15 22:44:55 [quark_moe.py:613] The current mode supports native MoE MXFP4 computation
(Worker_TP2 pid=522) WARNING 02-15 22:44:55 [quark_moe.py:613] The current mode supports native MoE MXFP4 computation
(Worker_TP3 pid=523) INFO 02-15 22:44:55 [rocm.py:266] Using AITER MLA backend.
(Worker_TP4 pid=524) WARNING 02-15 22:44:55 [quark_moe.py:613] The current mode supports native MoE MXFP4 computation
(Worker_TP0 pid=520) WARNING 02-15 22:44:55 [quark_moe.py:613] The current mode supports native MoE MXFP4 computation
(Worker_TP3 pid=523) WARNING 02-15 22:44:55 [quark_moe.py:613] The current mode supports native MoE MXFP4 computation
(Worker_TP2 pid=522) WARNING 02-15 22:44:55 [compilation.py:1078] Op 'quant_fp8' not present in model, enabling with '+quant_fp8' has no effect
(Worker_TP2 pid=522) WARNING 02-15 22:44:55 [compilation.py:1078] Op 'grouped_topk' not present in model, enabling with '+grouped_topk' has no effect
(Worker_TP2 pid=522) WARNING 02-15 22:44:55 [compilation.py:1078] Op 'sparse_attn_indexer' not present in model, enabling with '+sparse_attn_indexer' has no effect
(Worker_TP5 pid=525) WARNING 02-15 22:44:55 [compilation.py:1078] Op 'quant_fp8' not present in model, enabling with '+quant_fp8' has no effect
(Worker_TP5 pid=525) WARNING 02-15 22:44:55 [compilation.py:1078] Op 'grouped_topk' not present in model, enabling with '+grouped_topk' has no effect
(Worker_TP5 pid=525) WARNING 02-15 22:44:55 [compilation.py:1078] Op 'sparse_attn_indexer' not present in model, enabling with '+sparse_attn_indexer' has no effect
(Worker_TP1 pid=521) WARNING 02-15 22:44:55 [compilation.py:1078] Op 'quant_fp8' not present in model, enabling with '+quant_fp8' has no effect
(Worker_TP1 pid=521) WARNING 02-15 22:44:55 [compilation.py:1078] Op 'grouped_topk' not present in model, enabling with '+grouped_topk' has no effect
(Worker_TP1 pid=521) WARNING 02-15 22:44:55 [compilation.py:1078] Op 'sparse_attn_indexer' not present in model, enabling with '+sparse_attn_indexer' has no effect
(Worker_TP6 pid=526) WARNING 02-15 22:44:55 [compilation.py:1078] Op 'quant_fp8' not present in model, enabling with '+quant_fp8' has no effect
(Worker_TP6 pid=526) WARNING 02-15 22:44:55 [compilation.py:1078] Op 'grouped_topk' not present in model, enabling with '+grouped_topk' has no effect
(Worker_TP6 pid=526) WARNING 02-15 22:44:55 [compilation.py:1078] Op 'sparse_attn_indexer' not present in model, enabling with '+sparse_attn_indexer' has no effect
(Worker_TP7 pid=527) WARNING 02-15 22:44:55 [compilation.py:1078] Op 'quant_fp8' not present in model, enabling with '+quant_fp8' has no effect
(Worker_TP7 pid=527) WARNING 02-15 22:44:55 [compilation.py:1078] Op 'grouped_topk' not present in model, enabling with '+grouped_topk' has no effect
(Worker_TP7 pid=527) WARNING 02-15 22:44:55 [compilation.py:1078] Op 'sparse_attn_indexer' not present in model, enabling with '+sparse_attn_indexer' has no effect
(Worker_TP4 pid=524) WARNING 02-15 22:44:55 [compilation.py:1078] Op 'quant_fp8' not present in model, enabling with '+quant_fp8' has no effect
(Worker_TP4 pid=524) WARNING 02-15 22:44:55 [compilation.py:1078] Op 'grouped_topk' not present in model, enabling with '+grouped_topk' has no effect
(Worker_TP4 pid=524) WARNING 02-15 22:44:55 [compilation.py:1078] Op 'sparse_attn_indexer' not present in model, enabling with '+sparse_attn_indexer' has no effect
(Worker_TP0 pid=520) WARNING 02-15 22:44:55 [compilation.py:1078] Op 'quant_fp8' not present in model, enabling with '+quant_fp8' has no effect
(Worker_TP0 pid=520) WARNING 02-15 22:44:55 [compilation.py:1078] Op 'grouped_topk' not present in model, enabling with '+grouped_topk' has no effect
(Worker_TP0 pid=520) WARNING 02-15 22:44:55 [compilation.py:1078] Op 'sparse_attn_indexer' not present in model, enabling with '+sparse_attn_indexer' has no effect
(Worker_TP3 pid=523) WARNING 02-15 22:44:55 [compilation.py:1078] Op 'quant_fp8' not present in model, enabling with '+quant_fp8' has no effect
(Worker_TP3 pid=523) WARNING 02-15 22:44:55 [compilation.py:1078] Op 'grouped_topk' not present in model, enabling with '+grouped_topk' has no effect
(Worker_TP3 pid=523) WARNING 02-15 22:44:55 [compilation.py:1078] Op 'sparse_attn_indexer' not present in model, enabling with '+sparse_attn_indexer' has no effect
Loading safetensors checkpoint shards:   0% Completed | 0/73 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:   1% Completed | 1/73 [00:00<00:17,  4.13it/s]
Loading safetensors checkpoint shards:   3% Completed | 2/73 [00:01<00:44,  1.61it/s]
Loading safetensors checkpoint shards:   4% Completed | 3/73 [00:02<00:59,  1.17it/s]
Loading safetensors checkpoint shards:   5% Completed | 4/73 [00:03<01:04,  1.07it/s]
Loading safetensors checkpoint shards:   7% Completed | 5/73 [00:04<01:05,  1.04it/s]
Loading safetensors checkpoint shards:   8% Completed | 6/73 [00:04<00:54,  1.24it/s]
Loading safetensors checkpoint shards:  10% Completed | 7/73 [00:05<00:58,  1.12it/s]
Loading safetensors checkpoint shards:  11% Completed | 8/73 [00:07<01:04,  1.01it/s]
Loading safetensors checkpoint shards:  12% Completed | 9/73 [00:08<01:09,  1.09s/it]
Loading safetensors checkpoint shards:  14% Completed | 10/73 [00:09<01:08,  1.08s/it]
Loading safetensors checkpoint shards:  15% Completed | 11/73 [00:10<01:07,  1.08s/it]
Loading safetensors checkpoint shards:  16% Completed | 12/73 [00:11<01:07,  1.11s/it]
Loading safetensors checkpoint shards:  18% Completed | 13/73 [00:12<01:07,  1.12s/it]
Loading safetensors checkpoint shards:  19% Completed | 14/73 [00:14<01:07,  1.14s/it]
Loading safetensors checkpoint shards:  21% Completed | 15/73 [00:15<01:06,  1.15s/it]
Loading safetensors checkpoint shards:  22% Completed | 16/73 [00:16<01:04,  1.13s/it]
Loading safetensors checkpoint shards:  23% Completed | 17/73 [00:17<01:00,  1.08s/it]
Loading safetensors checkpoint shards:  25% Completed | 18/73 [00:18<00:59,  1.08s/it]
Loading safetensors checkpoint shards:  26% Completed | 19/73 [00:19<00:56,  1.05s/it]
Loading safetensors checkpoint shards:  27% Completed | 20/73 [00:20<00:54,  1.03s/it]
Loading safetensors checkpoint shards:  29% Completed | 21/73 [00:20<00:45,  1.14it/s]
Loading safetensors checkpoint shards:  30% Completed | 22/73 [00:21<00:48,  1.06it/s]
Loading safetensors checkpoint shards:  32% Completed | 23/73 [00:22<00:41,  1.21it/s]
Loading safetensors checkpoint shards:  33% Completed | 24/73 [00:23<00:43,  1.13it/s]
Loading safetensors checkpoint shards:  34% Completed | 25/73 [00:24<00:43,  1.10it/s]
Loading safetensors checkpoint shards:  36% Completed | 26/73 [00:25<00:45,  1.04it/s]
Loading safetensors checkpoint shards:  37% Completed | 27/73 [00:26<00:44,  1.03it/s]
Loading safetensors checkpoint shards:  40% Completed | 29/73 [00:27<00:33,  1.33it/s]
Loading safetensors checkpoint shards:  41% Completed | 30/73 [00:28<00:34,  1.25it/s]
Loading safetensors checkpoint shards:  42% Completed | 31/73 [00:29<00:30,  1.37it/s]
Loading safetensors checkpoint shards:  44% Completed | 32/73 [00:29<00:30,  1.33it/s]
Loading safetensors checkpoint shards:  45% Completed | 33/73 [00:30<00:30,  1.29it/s]
Loading safetensors checkpoint shards:  47% Completed | 34/73 [00:31<00:26,  1.45it/s]
Loading safetensors checkpoint shards:  48% Completed | 35/73 [00:32<00:29,  1.28it/s]
Loading safetensors checkpoint shards:  49% Completed | 36/73 [00:33<00:31,  1.18it/s]
Loading safetensors checkpoint shards:  51% Completed | 37/73 [00:33<00:27,  1.31it/s]
Loading safetensors checkpoint shards:  52% Completed | 38/73 [00:34<00:24,  1.42it/s]
Loading safetensors checkpoint shards:  53% Completed | 39/73 [00:35<00:28,  1.21it/s]
Loading safetensors checkpoint shards:  55% Completed | 40/73 [00:36<00:28,  1.14it/s]
Loading safetensors checkpoint shards:  56% Completed | 41/73 [00:37<00:28,  1.12it/s]
Loading safetensors checkpoint shards:  58% Completed | 42/73 [00:37<00:24,  1.26it/s]
Loading safetensors checkpoint shards:  59% Completed | 43/73 [00:39<00:27,  1.11it/s]
Loading safetensors checkpoint shards:  60% Completed | 44/73 [00:39<00:23,  1.25it/s]
Loading safetensors checkpoint shards:  62% Completed | 45/73 [00:40<00:24,  1.14it/s]
Loading safetensors checkpoint shards:  63% Completed | 46/73 [00:41<00:24,  1.09it/s]
Loading safetensors checkpoint shards:  64% Completed | 47/73 [00:42<00:26,  1.00s/it]
Loading safetensors checkpoint shards:  66% Completed | 48/73 [00:44<00:26,  1.05s/it]
Loading safetensors checkpoint shards:  67% Completed | 49/73 [00:45<00:25,  1.05s/it]
Loading safetensors checkpoint shards:  68% Completed | 50/73 [00:46<00:25,  1.10s/it]
Loading safetensors checkpoint shards:  70% Completed | 51/73 [00:46<00:21,  1.04it/s]
Loading safetensors checkpoint shards:  71% Completed | 52/73 [00:48<00:21,  1.02s/it]
Loading safetensors checkpoint shards:  73% Completed | 53/73 [00:48<00:17,  1.14it/s]
Loading safetensors checkpoint shards:  74% Completed | 54/73 [00:49<00:17,  1.08it/s]
Loading safetensors checkpoint shards:  75% Completed | 55/73 [00:50<00:17,  1.00it/s]
Loading safetensors checkpoint shards:  77% Completed | 56/73 [00:52<00:17,  1.05s/it]
Loading safetensors checkpoint shards:  78% Completed | 57/73 [00:53<00:17,  1.07s/it]
Loading safetensors checkpoint shards:  79% Completed | 58/73 [00:56<00:24,  1.65s/it]
Loading safetensors checkpoint shards:  81% Completed | 59/73 [00:57<00:20,  1.44s/it]
Loading safetensors checkpoint shards:  82% Completed | 60/73 [00:58<00:16,  1.30s/it]
Loading safetensors checkpoint shards:  84% Completed | 61/73 [00:59<00:14,  1.24s/it]
Loading safetensors checkpoint shards:  85% Completed | 62/73 [01:00<00:12,  1.16s/it]
Loading safetensors checkpoint shards:  86% Completed | 63/73 [01:00<00:09,  1.10it/s]
Loading safetensors checkpoint shards:  88% Completed | 64/73 [01:01<00:07,  1.13it/s]
Loading safetensors checkpoint shards:  89% Completed | 65/73 [01:02<00:06,  1.16it/s]
Loading safetensors checkpoint shards:  90% Completed | 66/73 [01:02<00:05,  1.17it/s]
Loading safetensors checkpoint shards:  92% Completed | 67/73 [01:03<00:05,  1.18it/s]
Loading safetensors checkpoint shards:  93% Completed | 68/73 [01:04<00:04,  1.18it/s]
Loading safetensors checkpoint shards:  95% Completed | 69/73 [01:05<00:03,  1.18it/s]
Loading safetensors checkpoint shards:  96% Completed | 70/73 [01:06<00:02,  1.20it/s]
Loading safetensors checkpoint shards:  97% Completed | 71/73 [01:07<00:01,  1.21it/s]
Loading safetensors checkpoint shards:  99% Completed | 72/73 [01:07<00:00,  1.21it/s]
Loading safetensors checkpoint shards: 100% Completed | 73/73 [01:08<00:00,  1.22it/s]
Loading safetensors checkpoint shards: 100% Completed | 73/73 [01:08<00:00,  1.06it/s]
(Worker_TP0 pid=520) 
(Worker_TP0 pid=520) INFO 02-15 22:46:04 [default_loader.py:291] Loading weights took 68.76 seconds
[Aiter Triton] Pre-compiling fp8 BMM kernel: 100%|██████████| 1024/1024 [00:02<00:00, 347.23it/s]
[Aiter Triton] Pre-compiling fp8 BMM kernel: 100%|██████████| 1024/1024 [00:00<00:00, 13821.38it/s]
[Aiter Triton] Pre-compiling fp8 BMM kernel: 100%|██████████| 1024/1024 [00:00<00:00, 13801.84it/s]
[Aiter Triton] Pre-compiling fp8 BMM kernel: 100%|██████████| 1024/1024 [00:00<00:00, 13812.45it/s]
[Aiter Triton] Pre-compiling fp8 BMM kernel: 100%|██████████| 1024/1024 [00:00<00:00, 13867.30it/s]
[Aiter Triton] Pre-compiling fp8 BMM kernel: 100%|██████████| 1024/1024 [00:00<00:00, 13847.90it/s]
[Aiter Triton] Pre-compiling fp8 BMM kernel: 100%|██████████| 1024/1024 [00:00<00:00, 13750.89it/s]
[Aiter Triton] Pre-compiling fp8 BMM kernel: 100%|██████████| 1024/1024 [00:00<00:00, 13755.30it/s]
[Aiter Triton] Pre-compiling fp8 BMM kernel: 100%|██████████| 1024/1024 [00:00<00:00, 13760.06it/s]
[Aiter Triton] Pre-compiling fp8 BMM kernel: 100%|██████████| 1024/1024 [00:00<00:00, 13781.96it/s]
[Aiter Triton] Pre-compiling fp8 BMM kernel: 100%|██████████| 1024/1024 [00:00<00:00, 13750.28it/s]
[Aiter Triton] Pre-compiling fp8 BMM kernel: 100%|██████████| 1024/1024 [00:00<00:00, 13847.63it/s]
[Aiter Triton] Pre-compiling fp8 BMM kernel: 100%|██████████| 1024/1024 [00:00<00:00, 13750.36it/s]
[Aiter Triton] Pre-compiling fp8 BMM kernel: 100%|██████████| 1024/1024 [00:00<00:00, 13870.30it/s]
[Aiter Triton] Pre-compiling fp8 BMM kernel: 100%|██████████| 1024/1024 [00:00<00:00, 13735.37it/s]
[Aiter Triton] Pre-compiling fp8 BMM kernel: 100%|██████████| 1024/1024 [00:00<00:00, 13834.52it/s]
[Aiter Triton] Pre-compiling fp8 BMM kernel: 100%|██████████| 1024/1024 [00:00<00:00, 13839.29it/s]
[Aiter Triton] Pre-compiling fp8 BMM kernel: 100%|██████████| 1024/1024 [00:00<00:00, 13769.72it/s]
[Aiter Triton] Pre-compiling fp8 BMM kernel: 100%|██████████| 1024/1024 [00:00<00:00, 13765.13it/s]
[Aiter Triton] Pre-compiling fp8 BMM kernel: 100%|██████████| 1024/1024 [00:00<00:00, 13838.80it/s]
[Aiter Triton] Pre-compiling fp8 BMM kernel: 100%|██████████| 1024/1024 [00:00<00:00, 13768.35it/s]
[Aiter Triton] Pre-compiling fp8 BMM kernel: 100%|██████████| 1024/1024 [00:00<00:00, 13752.92it/s]
[Aiter Triton] Pre-compiling fp8 BMM kernel: 100%|██████████| 1024/1024 [00:00<00:00, 13850.53it/s]
[Aiter Triton] Pre-compiling fp8 BMM kernel: 100%|██████████| 1024/1024 [00:00<00:00, 13770.64it/s]
[Aiter Triton] Pre-compiling fp8 BMM kernel: 100%|██████████| 1024/1024 [00:00<00:00, 13808.59it/s]
[Aiter Triton] Pre-compiling fp8 BMM kernel: 100%|██████████| 1024/1024 [00:00<00:00, 13704.30it/s]
[Aiter Triton] Pre-compiling fp8 BMM kernel: 100%|██████████| 1024/1024 [00:00<00:00, 13756.93it/s]
[Aiter Triton] Pre-compiling fp8 BMM kernel: 100%|██████████| 1024/1024 [00:00<00:00, 13828.82it/s]
[Aiter Triton] Pre-compiling fp8 BMM kernel: 100%|██████████| 1024/1024 [00:00<00:00, 13848.35it/s]
[Aiter Triton] Pre-compiling fp8 BMM kernel: 100%|██████████| 1024/1024 [00:00<00:00, 13692.37it/s]
[Aiter Triton] Pre-compiling fp8 BMM kernel: 100%|██████████| 1024/1024 [00:00<00:00, 13826.77it/s]
[Aiter Triton] Pre-compiling fp8 BMM kernel: 100%|██████████| 1024/1024 [00:00<00:00, 13394.19it/s]
[Aiter Triton] Pre-compiling fp8 BMM kernel: 100%|██████████| 1024/1024 [00:00<00:00, 13801.62it/s]
[Aiter Triton] Pre-compiling fp8 BMM kernel: 100%|██████████| 1024/1024 [00:00<00:00, 13757.01it/s]
[Aiter Triton] Pre-compiling fp8 BMM kernel: 100%|██████████| 1024/1024 [00:00<00:00, 13828.99it/s]
[Aiter Triton] Pre-compiling fp8 BMM kernel: 100%|██████████| 1024/1024 [00:00<00:00, 13759.79it/s]
[Aiter Triton] Pre-compiling fp8 BMM kernel: 100%|██████████| 1024/1024 [00:00<00:00, 13801.22it/s]
[Aiter Triton] Pre-compiling fp8 BMM kernel: 100%|██████████| 1024/1024 [00:00<00:00, 13746.71it/s]
[Aiter Triton] Pre-compiling fp8 BMM kernel: 100%|██████████| 1024/1024 [00:00<00:00, 13786.69it/s]
[Aiter Triton] Pre-compiling fp8 BMM kernel: 100%|██████████| 1024/1024 [00:00<00:00, 13747.41it/s]
[Aiter Triton] Pre-compiling fp8 BMM kernel: 100%|██████████| 1024/1024 [00:00<00:00, 13772.28it/s]
[Aiter Triton] Pre-compiling fp8 BMM kernel: 100%|██████████| 1024/1024 [00:00<00:00, 13752.17it/s]
[Aiter Triton] Pre-compiling fp8 BMM kernel: 100%|██████████| 1024/1024 [00:00<00:00, 13042.64it/s]
[Aiter Triton] Pre-compiling fp8 BMM kernel: 100%|██████████| 1024/1024 [00:00<00:00, 13716.94it/s]
[Aiter Triton] Pre-compiling fp8 BMM kernel: 100%|██████████| 1024/1024 [00:00<00:00, 13784.74it/s]
[Aiter Triton] Pre-compiling fp8 BMM kernel: 100%|██████████| 1024/1024 [00:00<00:00, 13790.94it/s]
[Aiter Triton] Pre-compiling fp8 BMM kernel: 100%|██████████| 1024/1024 [00:00<00:00, 13847.85it/s]
[Aiter Triton] Pre-compiling fp8 BMM kernel: 100%|██████████| 1024/1024 [00:00<00:00, 13698.39it/s]
[Aiter Triton] Pre-compiling fp8 BMM kernel: 100%|██████████| 1024/1024 [00:00<00:00, 13757.15it/s]
[Aiter Triton] Pre-compiling fp8 BMM kernel: 100%|██████████| 1024/1024 [00:00<00:00, 13838.39it/s]
[Aiter Triton] Pre-compiling fp8 BMM kernel: 100%|██████████| 1024/1024 [00:00<00:00, 13735.28it/s]
[Aiter Triton] Pre-compiling fp8 BMM kernel: 100%|██████████| 1024/1024 [00:00<00:00, 13705.52it/s]
[Aiter Triton] Pre-compiling fp8 BMM kernel: 100%|██████████| 1024/1024 [00:00<00:00, 13493.20it/s]
[Aiter Triton] Pre-compiling fp8 BMM kernel: 100%|██████████| 1024/1024 [00:00<00:00, 13295.42it/s]
[Aiter Triton] Pre-compiling fp8 BMM kernel: 100%|██████████| 1024/1024 [00:00<00:00, 13569.04it/s]
[Aiter Triton] Pre-compiling fp8 BMM kernel: 100%|██████████| 1024/1024 [00:00<00:00, 13785.98it/s]
[Aiter Triton] Pre-compiling fp8 BMM kernel: 100%|██████████| 1024/1024 [00:00<00:00, 13756.22it/s]
[Aiter Triton] Pre-compiling fp8 BMM kernel: 100%|██████████| 1024/1024 [00:00<00:00, 13759.70it/s]
[Aiter Triton] Pre-compiling fp8 BMM kernel: 100%|██████████| 1024/1024 [00:00<00:00, 13744.47it/s]
[Aiter Triton] Pre-compiling fp8 BMM kernel: 100%|██████████| 1024/1024 [00:00<00:00, 13719.53it/s]
[Aiter Triton] Pre-compiling fp8 BMM kernel: 100%|██████████| 1024/1024 [00:00<00:00, 13763.36it/s]
(Worker_TP0 pid=520) INFO 02-15 22:46:13 [gpu_model_runner.py:4130] Model loading took 42.97 GiB memory and 77.786521 seconds
(Worker_TP0 pid=520) INFO 02-15 22:46:19 [backends.py:810] vLLM's torch.compile cache is disabled.
(Worker_TP0 pid=520) INFO 02-15 22:46:19 [backends.py:872] Dynamo bytecode transform time: 4.81 s
(Worker_TP5 pid=525) [aiter] import [module_rmsnorm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_rmsnorm.so
(Worker_TP5 pid=525) [2026-02-15 22:46:42] INFO core.py:477: import [module_rmsnorm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_rmsnorm.so
(Worker_TP0 pid=520) INFO 02-15 22:46:42 [backends.py:319] Compiling a graph for compile range (1, 163840) takes 21.72 s
(Worker_TP0 pid=520) INFO 02-15 22:46:42 [monitor.py:34] torch.compile takes 26.53 s in total
(Worker_TP1 pid=521) [aiter] import [module_rmsnorm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_rmsnorm.so
(Worker_TP1 pid=521) [2026-02-15 22:46:43] INFO core.py:477: import [module_rmsnorm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_rmsnorm.so
(Worker_TP2 pid=522) [aiter] import [module_rmsnorm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_rmsnorm.so
(Worker_TP2 pid=522) [2026-02-15 22:46:43] INFO core.py:477: import [module_rmsnorm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_rmsnorm.so
(Worker_TP0 pid=520) [aiter] import [module_rmsnorm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_rmsnorm.so
(Worker_TP0 pid=520) [2026-02-15 22:46:43] INFO core.py:477: import [module_rmsnorm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_rmsnorm.so
(Worker_TP7 pid=527) [aiter] import [module_rmsnorm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_rmsnorm.so
(Worker_TP7 pid=527) [2026-02-15 22:46:43] INFO core.py:477: import [module_rmsnorm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_rmsnorm.so
(Worker_TP6 pid=526) [aiter] import [module_rmsnorm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_rmsnorm.so
(Worker_TP6 pid=526) [2026-02-15 22:46:43] INFO core.py:477: import [module_rmsnorm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_rmsnorm.so
(Worker_TP3 pid=523) [aiter] import [module_rmsnorm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_rmsnorm.so
(Worker_TP3 pid=523) [2026-02-15 22:46:43] INFO core.py:477: import [module_rmsnorm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_rmsnorm.so
(Worker_TP4 pid=524) [aiter] import [module_rmsnorm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_rmsnorm.so
(Worker_TP4 pid=524) [2026-02-15 22:46:43] INFO core.py:477: import [module_rmsnorm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_rmsnorm.so
(Worker_TP5 pid=525) [aiter] import [module_moe_asm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_asm.so
(Worker_TP5 pid=525) [2026-02-15 22:46:45] INFO core.py:477: import [module_moe_asm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_asm.so
(Worker_TP5 pid=525) [aiter] type hints mismatch, override to --> moe_fused_gate(input: torch.Tensor, bias: torch.Tensor, topk_weights: torch.Tensor, topk_ids: torch.Tensor, num_expert_group: int, topk_group: int, topk: int, n_share_experts_fusion: int, routed_scaling_factor: float = 1.0) -> List[torch.Tensor]
(Worker_TP5 pid=525) [2026-02-15 22:46:45] WARNING core.py:927: type hints mismatch, override to --> moe_fused_gate(input: torch.Tensor, bias: torch.Tensor, topk_weights: torch.Tensor, topk_ids: torch.Tensor, num_expert_group: int, topk_group: int, topk: int, n_share_experts_fusion: int, routed_scaling_factor: float = 1.0) -> List[torch.Tensor]
(Worker_TP5 pid=525) [aiter] merge tuned file under model_configs/ and configs/ /usr/local/lib/python3.12/dist-packages/aiter/configs/tuned_fmoe.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
(Worker_TP5 pid=525) [2026-02-15 22:46:45] INFO core.py:267: merge tuned file under model_configs/ and configs/ /usr/local/lib/python3.12/dist-packages/aiter/configs/tuned_fmoe.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
(Worker_TP5 pid=525) [aiter] [fused_moe] using 2stage default for (256, 1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
(Worker_TP5 pid=525) [2026-02-15 22:46:45] INFO fused_moe.py:672: [fused_moe] using 2stage default for (256, 1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
(Worker_TP5 pid=525) [aiter] import [module_moe_sorting] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_sorting.so
(Worker_TP5 pid=525) [2026-02-15 22:46:45] INFO core.py:477: import [module_moe_sorting] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_sorting.so
(Worker_TP5 pid=525) [aiter] import [module_quant] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_quant.so
(Worker_TP5 pid=525) [2026-02-15 22:46:45] INFO core.py:477: import [module_quant] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_quant.so
(Worker_TP5 pid=525) [aiter] type hints mismatch, override to --> dynamic_per_group_scaled_quant_fp4(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, group_size: int = 32, shuffle_scale: bool = True, num_rows: Optional[torch.Tensor] = None, num_rows_factor: int = 1) -> None
(Worker_TP5 pid=525) [2026-02-15 22:46:45] WARNING core.py:927: type hints mismatch, override to --> dynamic_per_group_scaled_quant_fp4(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, group_size: int = 32, shuffle_scale: bool = True, num_rows: Optional[torch.Tensor] = None, num_rows_factor: int = 1) -> None
(Worker_TP1 pid=521) [aiter] import [module_moe_asm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_asm.so
(Worker_TP1 pid=521) [2026-02-15 22:46:45] INFO core.py:477: import [module_moe_asm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_asm.so
(Worker_TP1 pid=521) [aiter] type hints mismatch, override to --> moe_fused_gate(input: torch.Tensor, bias: torch.Tensor, topk_weights: torch.Tensor, topk_ids: torch.Tensor, num_expert_group: int, topk_group: int, topk: int, n_share_experts_fusion: int, routed_scaling_factor: float = 1.0) -> List[torch.Tensor]
(Worker_TP1 pid=521) [2026-02-15 22:46:45] WARNING core.py:927: type hints mismatch, override to --> moe_fused_gate(input: torch.Tensor, bias: torch.Tensor, topk_weights: torch.Tensor, topk_ids: torch.Tensor, num_expert_group: int, topk_group: int, topk: int, n_share_experts_fusion: int, routed_scaling_factor: float = 1.0) -> List[torch.Tensor]
(Worker_TP1 pid=521) [aiter] merge tuned file under model_configs/ and configs/ /usr/local/lib/python3.12/dist-packages/aiter/configs/tuned_fmoe.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
(Worker_TP1 pid=521) [2026-02-15 22:46:45] INFO core.py:267: merge tuned file under model_configs/ and configs/ /usr/local/lib/python3.12/dist-packages/aiter/configs/tuned_fmoe.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
(Worker_TP1 pid=521) [aiter] [fused_moe] using 2stage default for (256, 1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
(Worker_TP1 pid=521) [2026-02-15 22:46:45] INFO fused_moe.py:672: [fused_moe] using 2stage default for (256, 1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
(Worker_TP1 pid=521) [aiter] import [module_moe_sorting] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_sorting.so
(Worker_TP1 pid=521) [2026-02-15 22:46:45] INFO core.py:477: import [module_moe_sorting] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_sorting.so
(Worker_TP1 pid=521) [aiter] import [module_quant] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_quant.so
(Worker_TP1 pid=521) [2026-02-15 22:46:45] INFO core.py:477: import [module_quant] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_quant.so
(Worker_TP1 pid=521) [aiter] type hints mismatch, override to --> dynamic_per_group_scaled_quant_fp4(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, group_size: int = 32, shuffle_scale: bool = True, num_rows: Optional[torch.Tensor] = None, num_rows_factor: int = 1) -> None
(Worker_TP1 pid=521) [2026-02-15 22:46:45] WARNING core.py:927: type hints mismatch, override to --> dynamic_per_group_scaled_quant_fp4(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, group_size: int = 32, shuffle_scale: bool = True, num_rows: Optional[torch.Tensor] = None, num_rows_factor: int = 1) -> None
(Worker_TP5 pid=525) [aiter] import [module_moe_ck2stages] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_ck2stages.so
(Worker_TP5 pid=525) [2026-02-15 22:46:46] INFO core.py:477: import [module_moe_ck2stages] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_ck2stages.so
(Worker_TP5 pid=525) [aiter] type hints mismatch, override to --> ck_moe_stage1(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w1_scale: Optional[torch.Tensor] = None, a1_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
(Worker_TP5 pid=525) [2026-02-15 22:46:46] WARNING core.py:927: type hints mismatch, override to --> ck_moe_stage1(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w1_scale: Optional[torch.Tensor] = None, a1_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
(Worker_TP0 pid=520) [aiter] import [module_moe_asm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_asm.so
(Worker_TP0 pid=520) [2026-02-15 22:46:46] INFO core.py:477: import [module_moe_asm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_asm.so
(Worker_TP0 pid=520) [aiter] type hints mismatch, override to --> moe_fused_gate(input: torch.Tensor, bias: torch.Tensor, topk_weights: torch.Tensor, topk_ids: torch.Tensor, num_expert_group: int, topk_group: int, topk: int, n_share_experts_fusion: int, routed_scaling_factor: float = 1.0) -> List[torch.Tensor]
(Worker_TP0 pid=520) [2026-02-15 22:46:46] WARNING core.py:927: type hints mismatch, override to --> moe_fused_gate(input: torch.Tensor, bias: torch.Tensor, topk_weights: torch.Tensor, topk_ids: torch.Tensor, num_expert_group: int, topk_group: int, topk: int, n_share_experts_fusion: int, routed_scaling_factor: float = 1.0) -> List[torch.Tensor]
(Worker_TP5 pid=525) [aiter] type hints mismatch, override to --> ck_moe_stage2(inter_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w2_scale: Optional[torch.Tensor] = None, a2_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
(Worker_TP5 pid=525) [2026-02-15 22:46:46] WARNING core.py:927: type hints mismatch, override to --> ck_moe_stage2(inter_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w2_scale: Optional[torch.Tensor] = None, a2_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
(Worker_TP0 pid=520) [aiter] merge tuned file under model_configs/ and configs/ /usr/local/lib/python3.12/dist-packages/aiter/configs/tuned_fmoe.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
(Worker_TP0 pid=520) [2026-02-15 22:46:46] INFO core.py:267: merge tuned file under model_configs/ and configs/ /usr/local/lib/python3.12/dist-packages/aiter/configs/tuned_fmoe.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
(Worker_TP0 pid=520) [aiter] [fused_moe] using 2stage default for (256, 1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
(Worker_TP0 pid=520) [2026-02-15 22:46:46] INFO fused_moe.py:672: [fused_moe] using 2stage default for (256, 1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
(Worker_TP0 pid=520) [aiter] import [module_moe_sorting] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_sorting.so
(Worker_TP0 pid=520) [2026-02-15 22:46:46] INFO core.py:477: import [module_moe_sorting] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_sorting.so
(Worker_TP0 pid=520) [aiter] import [module_quant] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_quant.so
(Worker_TP0 pid=520) [2026-02-15 22:46:46] INFO core.py:477: import [module_quant] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_quant.so
(Worker_TP0 pid=520) [aiter] type hints mismatch, override to --> dynamic_per_group_scaled_quant_fp4(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, group_size: int = 32, shuffle_scale: bool = True, num_rows: Optional[torch.Tensor] = None, num_rows_factor: int = 1) -> None
(Worker_TP0 pid=520) [2026-02-15 22:46:46] WARNING core.py:927: type hints mismatch, override to --> dynamic_per_group_scaled_quant_fp4(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, group_size: int = 32, shuffle_scale: bool = True, num_rows: Optional[torch.Tensor] = None, num_rows_factor: int = 1) -> None
(Worker_TP1 pid=521) [aiter] import [module_moe_ck2stages] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_ck2stages.so
(Worker_TP1 pid=521) [2026-02-15 22:46:46] INFO core.py:477: import [module_moe_ck2stages] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_ck2stages.so
(Worker_TP1 pid=521) [aiter] type hints mismatch, override to --> ck_moe_stage1(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w1_scale: Optional[torch.Tensor] = None, a1_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
(Worker_TP1 pid=521) [2026-02-15 22:46:46] WARNING core.py:927: type hints mismatch, override to --> ck_moe_stage1(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w1_scale: Optional[torch.Tensor] = None, a1_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
(Worker_TP1 pid=521) [aiter] type hints mismatch, override to --> ck_moe_stage2(inter_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w2_scale: Optional[torch.Tensor] = None, a2_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
(Worker_TP1 pid=521) [2026-02-15 22:46:46] WARNING core.py:927: type hints mismatch, override to --> ck_moe_stage2(inter_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w2_scale: Optional[torch.Tensor] = None, a2_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
(Worker_TP0 pid=520) [aiter] import [module_moe_ck2stages] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_ck2stages.so
(Worker_TP0 pid=520) [2026-02-15 22:46:46] INFO core.py:477: import [module_moe_ck2stages] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_ck2stages.so
(Worker_TP0 pid=520) [aiter] type hints mismatch, override to --> ck_moe_stage1(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w1_scale: Optional[torch.Tensor] = None, a1_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
(Worker_TP0 pid=520) [2026-02-15 22:46:46] WARNING core.py:927: type hints mismatch, override to --> ck_moe_stage1(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w1_scale: Optional[torch.Tensor] = None, a1_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
(Worker_TP0 pid=520) [aiter] type hints mismatch, override to --> ck_moe_stage2(inter_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w2_scale: Optional[torch.Tensor] = None, a2_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
(Worker_TP0 pid=520) [2026-02-15 22:46:46] WARNING core.py:927: type hints mismatch, override to --> ck_moe_stage2(inter_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w2_scale: Optional[torch.Tensor] = None, a2_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
(Worker_TP2 pid=522) [aiter] import [module_moe_asm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_asm.so
(Worker_TP2 pid=522) [2026-02-15 22:46:46] INFO core.py:477: import [module_moe_asm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_asm.so
(Worker_TP2 pid=522) [aiter] type hints mismatch, override to --> moe_fused_gate(input: torch.Tensor, bias: torch.Tensor, topk_weights: torch.Tensor, topk_ids: torch.Tensor, num_expert_group: int, topk_group: int, topk: int, n_share_experts_fusion: int, routed_scaling_factor: float = 1.0) -> List[torch.Tensor]
(Worker_TP2 pid=522) [2026-02-15 22:46:46] WARNING core.py:927: type hints mismatch, override to --> moe_fused_gate(input: torch.Tensor, bias: torch.Tensor, topk_weights: torch.Tensor, topk_ids: torch.Tensor, num_expert_group: int, topk_group: int, topk: int, n_share_experts_fusion: int, routed_scaling_factor: float = 1.0) -> List[torch.Tensor]
(Worker_TP2 pid=522) [aiter] merge tuned file under model_configs/ and configs/ /usr/local/lib/python3.12/dist-packages/aiter/configs/tuned_fmoe.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
(Worker_TP2 pid=522) [2026-02-15 22:46:46] INFO core.py:267: merge tuned file under model_configs/ and configs/ /usr/local/lib/python3.12/dist-packages/aiter/configs/tuned_fmoe.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
(Worker_TP2 pid=522) [aiter] [fused_moe] using 2stage default for (256, 1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
(Worker_TP2 pid=522) [2026-02-15 22:46:46] INFO fused_moe.py:672: [fused_moe] using 2stage default for (256, 1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
(Worker_TP7 pid=527) [aiter] import [module_moe_asm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_asm.so
(Worker_TP7 pid=527) [2026-02-15 22:46:46] INFO core.py:477: import [module_moe_asm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_asm.so
(Worker_TP7 pid=527) [aiter] type hints mismatch, override to --> moe_fused_gate(input: torch.Tensor, bias: torch.Tensor, topk_weights: torch.Tensor, topk_ids: torch.Tensor, num_expert_group: int, topk_group: int, topk: int, n_share_experts_fusion: int, routed_scaling_factor: float = 1.0) -> List[torch.Tensor]
(Worker_TP7 pid=527) [2026-02-15 22:46:46] WARNING core.py:927: type hints mismatch, override to --> moe_fused_gate(input: torch.Tensor, bias: torch.Tensor, topk_weights: torch.Tensor, topk_ids: torch.Tensor, num_expert_group: int, topk_group: int, topk: int, n_share_experts_fusion: int, routed_scaling_factor: float = 1.0) -> List[torch.Tensor]
(Worker_TP2 pid=522) [aiter] import [module_moe_sorting] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_sorting.so
(Worker_TP2 pid=522) [2026-02-15 22:46:46] INFO core.py:477: import [module_moe_sorting] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_sorting.so
(Worker_TP2 pid=522) [aiter] import [module_quant] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_quant.so
(Worker_TP2 pid=522) [2026-02-15 22:46:46] INFO core.py:477: import [module_quant] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_quant.so
(Worker_TP2 pid=522) [aiter] type hints mismatch, override to --> dynamic_per_group_scaled_quant_fp4(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, group_size: int = 32, shuffle_scale: bool = True, num_rows: Optional[torch.Tensor] = None, num_rows_factor: int = 1) -> None
(Worker_TP2 pid=522) [2026-02-15 22:46:46] WARNING core.py:927: type hints mismatch, override to --> dynamic_per_group_scaled_quant_fp4(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, group_size: int = 32, shuffle_scale: bool = True, num_rows: Optional[torch.Tensor] = None, num_rows_factor: int = 1) -> None
(Worker_TP7 pid=527) [aiter] merge tuned file under model_configs/ and configs/ /usr/local/lib/python3.12/dist-packages/aiter/configs/tuned_fmoe.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
(Worker_TP7 pid=527) [2026-02-15 22:46:46] INFO core.py:267: merge tuned file under model_configs/ and configs/ /usr/local/lib/python3.12/dist-packages/aiter/configs/tuned_fmoe.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
(Worker_TP7 pid=527) [aiter] [fused_moe] using 2stage default for (256, 1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
(Worker_TP7 pid=527) [2026-02-15 22:46:46] INFO fused_moe.py:672: [fused_moe] using 2stage default for (256, 1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
(Worker_TP7 pid=527) [aiter] import [module_moe_sorting] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_sorting.so
(Worker_TP7 pid=527) [2026-02-15 22:46:46] INFO core.py:477: import [module_moe_sorting] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_sorting.so
(Worker_TP7 pid=527) [aiter] import [module_quant] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_quant.so
(Worker_TP7 pid=527) [2026-02-15 22:46:46] INFO core.py:477: import [module_quant] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_quant.so
(Worker_TP7 pid=527) [aiter] type hints mismatch, override to --> dynamic_per_group_scaled_quant_fp4(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, group_size: int = 32, shuffle_scale: bool = True, num_rows: Optional[torch.Tensor] = None, num_rows_factor: int = 1) -> None
(Worker_TP7 pid=527) [2026-02-15 22:46:46] WARNING core.py:927: type hints mismatch, override to --> dynamic_per_group_scaled_quant_fp4(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, group_size: int = 32, shuffle_scale: bool = True, num_rows: Optional[torch.Tensor] = None, num_rows_factor: int = 1) -> None
(Worker_TP6 pid=526) [aiter] import [module_moe_asm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_asm.so
(Worker_TP6 pid=526) [2026-02-15 22:46:46] INFO core.py:477: import [module_moe_asm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_asm.so
(Worker_TP6 pid=526) [aiter] type hints mismatch, override to --> moe_fused_gate(input: torch.Tensor, bias: torch.Tensor, topk_weights: torch.Tensor, topk_ids: torch.Tensor, num_expert_group: int, topk_group: int, topk: int, n_share_experts_fusion: int, routed_scaling_factor: float = 1.0) -> List[torch.Tensor]
(Worker_TP6 pid=526) [2026-02-15 22:46:46] WARNING core.py:927: type hints mismatch, override to --> moe_fused_gate(input: torch.Tensor, bias: torch.Tensor, topk_weights: torch.Tensor, topk_ids: torch.Tensor, num_expert_group: int, topk_group: int, topk: int, n_share_experts_fusion: int, routed_scaling_factor: float = 1.0) -> List[torch.Tensor]
(Worker_TP2 pid=522) [aiter] import [module_moe_ck2stages] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_ck2stages.so
(Worker_TP2 pid=522) [2026-02-15 22:46:46] INFO core.py:477: import [module_moe_ck2stages] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_ck2stages.so
(Worker_TP2 pid=522) [aiter] type hints mismatch, override to --> ck_moe_stage1(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w1_scale: Optional[torch.Tensor] = None, a1_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
(Worker_TP2 pid=522) [2026-02-15 22:46:46] WARNING core.py:927: type hints mismatch, override to --> ck_moe_stage1(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w1_scale: Optional[torch.Tensor] = None, a1_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
(Worker_TP6 pid=526) [aiter] merge tuned file under model_configs/ and configs/ /usr/local/lib/python3.12/dist-packages/aiter/configs/tuned_fmoe.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
(Worker_TP6 pid=526) [2026-02-15 22:46:46] INFO core.py:267: merge tuned file under model_configs/ and configs/ /usr/local/lib/python3.12/dist-packages/aiter/configs/tuned_fmoe.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
(Worker_TP6 pid=526) [aiter] [fused_moe] using 2stage default for (256, 1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
(Worker_TP6 pid=526) [2026-02-15 22:46:46] INFO fused_moe.py:672: [fused_moe] using 2stage default for (256, 1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
(Worker_TP6 pid=526) [aiter] import [module_moe_sorting] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_sorting.so
(Worker_TP6 pid=526) [2026-02-15 22:46:46] INFO core.py:477: import [module_moe_sorting] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_sorting.so
(Worker_TP7 pid=527) [aiter] import [module_moe_ck2stages] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_ck2stages.so
(Worker_TP7 pid=527) [2026-02-15 22:46:46] INFO core.py:477: import [module_moe_ck2stages] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_ck2stages.so
(Worker_TP7 pid=527) [aiter] type hints mismatch, override to --> ck_moe_stage1(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w1_scale: Optional[torch.Tensor] = None, a1_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
(Worker_TP7 pid=527) [2026-02-15 22:46:46] WARNING core.py:927: type hints mismatch, override to --> ck_moe_stage1(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w1_scale: Optional[torch.Tensor] = None, a1_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
(Worker_TP6 pid=526) [aiter] import [module_quant] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_quant.so
(Worker_TP6 pid=526) [2026-02-15 22:46:46] INFO core.py:477: import [module_quant] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_quant.so
(Worker_TP6 pid=526) [aiter] type hints mismatch, override to --> dynamic_per_group_scaled_quant_fp4(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, group_size: int = 32, shuffle_scale: bool = True, num_rows: Optional[torch.Tensor] = None, num_rows_factor: int = 1) -> None
(Worker_TP6 pid=526) [2026-02-15 22:46:46] WARNING core.py:927: type hints mismatch, override to --> dynamic_per_group_scaled_quant_fp4(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, group_size: int = 32, shuffle_scale: bool = True, num_rows: Optional[torch.Tensor] = None, num_rows_factor: int = 1) -> None
(Worker_TP2 pid=522) [aiter] type hints mismatch, override to --> ck_moe_stage2(inter_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w2_scale: Optional[torch.Tensor] = None, a2_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
(Worker_TP2 pid=522) [2026-02-15 22:46:46] WARNING core.py:927: type hints mismatch, override to --> ck_moe_stage2(inter_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w2_scale: Optional[torch.Tensor] = None, a2_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
(Worker_TP7 pid=527) [aiter] type hints mismatch, override to --> ck_moe_stage2(inter_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w2_scale: Optional[torch.Tensor] = None, a2_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
(Worker_TP7 pid=527) [2026-02-15 22:46:46] WARNING core.py:927: type hints mismatch, override to --> ck_moe_stage2(inter_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w2_scale: Optional[torch.Tensor] = None, a2_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
(Worker_TP4 pid=524) [aiter] import [module_moe_asm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_asm.so
(Worker_TP4 pid=524) [2026-02-15 22:46:46] INFO core.py:477: import [module_moe_asm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_asm.so
(Worker_TP4 pid=524) [aiter] type hints mismatch, override to --> moe_fused_gate(input: torch.Tensor, bias: torch.Tensor, topk_weights: torch.Tensor, topk_ids: torch.Tensor, num_expert_group: int, topk_group: int, topk: int, n_share_experts_fusion: int, routed_scaling_factor: float = 1.0) -> List[torch.Tensor]
(Worker_TP4 pid=524) [2026-02-15 22:46:46] WARNING core.py:927: type hints mismatch, override to --> moe_fused_gate(input: torch.Tensor, bias: torch.Tensor, topk_weights: torch.Tensor, topk_ids: torch.Tensor, num_expert_group: int, topk_group: int, topk: int, n_share_experts_fusion: int, routed_scaling_factor: float = 1.0) -> List[torch.Tensor]
(Worker_TP4 pid=524) [aiter] merge tuned file under model_configs/ and configs/ /usr/local/lib/python3.12/dist-packages/aiter/configs/tuned_fmoe.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
(Worker_TP4 pid=524) [2026-02-15 22:46:46] INFO core.py:267: merge tuned file under model_configs/ and configs/ /usr/local/lib/python3.12/dist-packages/aiter/configs/tuned_fmoe.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
(Worker_TP4 pid=524) [aiter] [fused_moe] using 2stage default for (256, 1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
(Worker_TP4 pid=524) [2026-02-15 22:46:46] INFO fused_moe.py:672: [fused_moe] using 2stage default for (256, 1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
(Worker_TP4 pid=524) [aiter] import [module_moe_sorting] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_sorting.so
(Worker_TP4 pid=524) [2026-02-15 22:46:46] INFO core.py:477: import [module_moe_sorting] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_sorting.so
(Worker_TP4 pid=524) [aiter] import [module_quant] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_quant.so
(Worker_TP4 pid=524) [2026-02-15 22:46:46] INFO core.py:477: import [module_quant] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_quant.so
(Worker_TP4 pid=524) [aiter] type hints mismatch, override to --> dynamic_per_group_scaled_quant_fp4(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, group_size: int = 32, shuffle_scale: bool = True, num_rows: Optional[torch.Tensor] = None, num_rows_factor: int = 1) -> None
(Worker_TP4 pid=524) [2026-02-15 22:46:46] WARNING core.py:927: type hints mismatch, override to --> dynamic_per_group_scaled_quant_fp4(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, group_size: int = 32, shuffle_scale: bool = True, num_rows: Optional[torch.Tensor] = None, num_rows_factor: int = 1) -> None
(Worker_TP6 pid=526) [aiter] import [module_moe_ck2stages] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_ck2stages.so
(Worker_TP6 pid=526) [2026-02-15 22:46:47] INFO core.py:477: import [module_moe_ck2stages] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_ck2stages.so
(Worker_TP6 pid=526) [aiter] type hints mismatch, override to --> ck_moe_stage1(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w1_scale: Optional[torch.Tensor] = None, a1_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
(Worker_TP6 pid=526) [2026-02-15 22:46:47] WARNING core.py:927: type hints mismatch, override to --> ck_moe_stage1(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w1_scale: Optional[torch.Tensor] = None, a1_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
(Worker_TP3 pid=523) [aiter] import [module_moe_asm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_asm.so
(Worker_TP3 pid=523) [2026-02-15 22:46:47] INFO core.py:477: import [module_moe_asm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_asm.so
(Worker_TP3 pid=523) [aiter] type hints mismatch, override to --> moe_fused_gate(input: torch.Tensor, bias: torch.Tensor, topk_weights: torch.Tensor, topk_ids: torch.Tensor, num_expert_group: int, topk_group: int, topk: int, n_share_experts_fusion: int, routed_scaling_factor: float = 1.0) -> List[torch.Tensor]
(Worker_TP3 pid=523) [2026-02-15 22:46:47] WARNING core.py:927: type hints mismatch, override to --> moe_fused_gate(input: torch.Tensor, bias: torch.Tensor, topk_weights: torch.Tensor, topk_ids: torch.Tensor, num_expert_group: int, topk_group: int, topk: int, n_share_experts_fusion: int, routed_scaling_factor: float = 1.0) -> List[torch.Tensor]
(Worker_TP3 pid=523) [aiter] merge tuned file under model_configs/ and configs/ /usr/local/lib/python3.12/dist-packages/aiter/configs/tuned_fmoe.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
(Worker_TP3 pid=523) [2026-02-15 22:46:47] INFO core.py:267: merge tuned file under model_configs/ and configs/ /usr/local/lib/python3.12/dist-packages/aiter/configs/tuned_fmoe.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
(Worker_TP6 pid=526) [aiter] type hints mismatch, override to --> ck_moe_stage2(inter_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w2_scale: Optional[torch.Tensor] = None, a2_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
(Worker_TP6 pid=526) [2026-02-15 22:46:47] WARNING core.py:927: type hints mismatch, override to --> ck_moe_stage2(inter_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w2_scale: Optional[torch.Tensor] = None, a2_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
(Worker_TP3 pid=523) [aiter] [fused_moe] using 2stage default for (256, 1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
(Worker_TP3 pid=523) [2026-02-15 22:46:47] INFO fused_moe.py:672: [fused_moe] using 2stage default for (256, 1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
(Worker_TP3 pid=523) [aiter] import [module_moe_sorting] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_sorting.so
(Worker_TP3 pid=523) [2026-02-15 22:46:47] INFO core.py:477: import [module_moe_sorting] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_sorting.so
(Worker_TP3 pid=523) [aiter] import [module_quant] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_quant.so
(Worker_TP3 pid=523) [2026-02-15 22:46:47] INFO core.py:477: import [module_quant] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_quant.so
(Worker_TP3 pid=523) [aiter] type hints mismatch, override to --> dynamic_per_group_scaled_quant_fp4(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, group_size: int = 32, shuffle_scale: bool = True, num_rows: Optional[torch.Tensor] = None, num_rows_factor: int = 1) -> None
(Worker_TP3 pid=523) [2026-02-15 22:46:47] WARNING core.py:927: type hints mismatch, override to --> dynamic_per_group_scaled_quant_fp4(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, group_size: int = 32, shuffle_scale: bool = True, num_rows: Optional[torch.Tensor] = None, num_rows_factor: int = 1) -> None
(Worker_TP4 pid=524) [aiter] import [module_moe_ck2stages] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_ck2stages.so
(Worker_TP4 pid=524) [2026-02-15 22:46:47] INFO core.py:477: import [module_moe_ck2stages] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_ck2stages.so
(Worker_TP4 pid=524) [aiter] type hints mismatch, override to --> ck_moe_stage1(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w1_scale: Optional[torch.Tensor] = None, a1_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
(Worker_TP4 pid=524) [2026-02-15 22:46:47] WARNING core.py:927: type hints mismatch, override to --> ck_moe_stage1(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w1_scale: Optional[torch.Tensor] = None, a1_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
(Worker_TP4 pid=524) [aiter] type hints mismatch, override to --> ck_moe_stage2(inter_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w2_scale: Optional[torch.Tensor] = None, a2_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
(Worker_TP4 pid=524) [2026-02-15 22:46:47] WARNING core.py:927: type hints mismatch, override to --> ck_moe_stage2(inter_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w2_scale: Optional[torch.Tensor] = None, a2_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
(Worker_TP3 pid=523) [aiter] import [module_moe_ck2stages] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_ck2stages.so
(Worker_TP3 pid=523) [2026-02-15 22:46:47] INFO core.py:477: import [module_moe_ck2stages] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_ck2stages.so
(Worker_TP3 pid=523) [aiter] type hints mismatch, override to --> ck_moe_stage1(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w1_scale: Optional[torch.Tensor] = None, a1_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
(Worker_TP3 pid=523) [2026-02-15 22:46:47] WARNING core.py:927: type hints mismatch, override to --> ck_moe_stage1(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w1_scale: Optional[torch.Tensor] = None, a1_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
(Worker_TP3 pid=523) [aiter] type hints mismatch, override to --> ck_moe_stage2(inter_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w2_scale: Optional[torch.Tensor] = None, a2_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
(Worker_TP3 pid=523) [2026-02-15 22:46:47] WARNING core.py:927: type hints mismatch, override to --> ck_moe_stage2(inter_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w2_scale: Optional[torch.Tensor] = None, a2_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
(Worker_TP0 pid=520) INFO 02-15 22:46:57 [gpu_worker.py:356] Available KV cache memory: 216.78 GiB
(EngineCore_DP0 pid=303) INFO 02-15 22:46:58 [kv_cache_utils.py:1307] GPU KV cache size: 6,611,696 tokens
(EngineCore_DP0 pid=303) INFO 02-15 22:46:58 [kv_cache_utils.py:1312] Maximum concurrency for 70,000 tokens per request: 94.45x
(Worker_TP7 pid=527) WARNING 02-15 22:46:58 [gpu_model_runner.py:5311] CUDAGraphMode.FULL is not supported with AiterMLABackend backend (support: AttentionCGSupport.UNIFORM_BATCH); setting cudagraph_mode=FULL_DECODE_ONLY
(Worker_TP0 pid=520) WARNING 02-15 22:46:58 [gpu_model_runner.py:5311] CUDAGraphMode.FULL is not supported with AiterMLABackend backend (support: AttentionCGSupport.UNIFORM_BATCH); setting cudagraph_mode=FULL_DECODE_ONLY
(Worker_TP6 pid=526) WARNING 02-15 22:46:58 [gpu_model_runner.py:5311] CUDAGraphMode.FULL is not supported with AiterMLABackend backend (support: AttentionCGSupport.UNIFORM_BATCH); setting cudagraph_mode=FULL_DECODE_ONLY
(Worker_TP4 pid=524) WARNING 02-15 22:46:58 [gpu_model_runner.py:5311] CUDAGraphMode.FULL is not supported with AiterMLABackend backend (support: AttentionCGSupport.UNIFORM_BATCH); setting cudagraph_mode=FULL_DECODE_ONLY
(Worker_TP2 pid=522) WARNING 02-15 22:46:58 [gpu_model_runner.py:5311] CUDAGraphMode.FULL is not supported with AiterMLABackend backend (support: AttentionCGSupport.UNIFORM_BATCH); setting cudagraph_mode=FULL_DECODE_ONLY
(Worker_TP3 pid=523) WARNING 02-15 22:46:58 [gpu_model_runner.py:5311] CUDAGraphMode.FULL is not supported with AiterMLABackend backend (support: AttentionCGSupport.UNIFORM_BATCH); setting cudagraph_mode=FULL_DECODE_ONLY
(Worker_TP1 pid=521) WARNING 02-15 22:46:58 [gpu_model_runner.py:5311] CUDAGraphMode.FULL is not supported with AiterMLABackend backend (support: AttentionCGSupport.UNIFORM_BATCH); setting cudagraph_mode=FULL_DECODE_ONLY
(Worker_TP5 pid=525) WARNING 02-15 22:46:58 [gpu_model_runner.py:5311] CUDAGraphMode.FULL is not supported with AiterMLABackend backend (support: AttentionCGSupport.UNIFORM_BATCH); setting cudagraph_mode=FULL_DECODE_ONLY
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s](Worker_TP1 pid=521) [aiter] import [module_mla_asm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_mla_asm.so
(Worker_TP1 pid=521) [2026-02-15 22:47:01] INFO core.py:477: import [module_mla_asm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_mla_asm.so
(Worker_TP1 pid=521) [aiter] type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
(Worker_TP1 pid=521) [2026-02-15 22:47:01] WARNING core.py:927: type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
[aiter] hipModuleLoad: /usr/local/lib/python3.12/dist-packages/aiter_meta/hsa/gfx950//mla/mla_a8w8_qh16_qseqlen1_gqaratio16.co GetFunction: _ZN5aiter33mla_a8w8_qh16_qseqlen1_gqaratio16E Success
[aiter] import [module_mla_asm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_mla_asm.so
(Worker_TP0 pid=520) [2026-02-15 22:47:01] INFO core.py:477: import [module_mla_asm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_mla_asm.so
(Worker_TP0 pid=520) [aiter] type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
(Worker_TP0 pid=520) [2026-02-15 22:47:01] WARNING core.py:927: type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
[aiter] hipModuleLoad: /usr/local/lib/python3.12/dist-packages/aiter_meta/hsa/gfx950//mla/mla_a8w8_qh16_qseqlen1_gqaratio16.co GetFunction: _ZN5aiter33mla_a8w8_qh16_qseqlen1_gqaratio16E Success
(Worker_TP6 pid=526) [aiter] import [module_mla_asm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_mla_asm.so
(Worker_TP6 pid=526) [2026-02-15 22:47:01] INFO core.py:477: import [module_mla_asm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_mla_asm.so
(Worker_TP6 pid=526) [aiter] type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
(Worker_TP6 pid=526) [2026-02-15 22:47:01] WARNING core.py:927: type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
[aiter] hipModuleLoad: /usr/local/lib/python3.12/dist-packages/aiter_meta/hsa/gfx950//mla/mla_a8w8_qh16_qseqlen1_gqaratio16.co GetFunction: _ZN5aiter33mla_a8w8_qh16_qseqlen1_gqaratio16E Success
(Worker_TP7 pid=527) [aiter] import [module_mla_asm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_mla_asm.so
(Worker_TP7 pid=527) [2026-02-15 22:47:01] INFO core.py:477: import [module_mla_asm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_mla_asm.so
(Worker_TP7 pid=527) [aiter] type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
(Worker_TP7 pid=527) [2026-02-15 22:47:01] WARNING core.py:927: type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
[aiter] hipModuleLoad: /usr/local/lib/python3.12/dist-packages/aiter_meta/hsa/gfx950//mla/mla_a8w8_qh16_qseqlen1_gqaratio16.co GetFunction: _ZN5aiter33mla_a8w8_qh16_qseqlen1_gqaratio16E Success
(Worker_TP2 pid=522) [aiter] import [module_mla_asm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_mla_asm.so
(Worker_TP2 pid=522) [2026-02-15 22:47:01] INFO core.py:477: import [module_mla_asm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_mla_asm.so
(Worker_TP2 pid=522) [aiter] type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
(Worker_TP2 pid=522) [2026-02-15 22:47:01] WARNING core.py:927: type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
[aiter] hipModuleLoad: /usr/local/lib/python3.12/dist-packages/aiter_meta/hsa/gfx950//mla/mla_a8w8_qh16_qseqlen1_gqaratio16.co GetFunction: _ZN5aiter33mla_a8w8_qh16_qseqlen1_gqaratio16E Success
(Worker_TP4 pid=524) [aiter] import [module_mla_asm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_mla_asm.so
(Worker_TP4 pid=524) [2026-02-15 22:47:02] INFO core.py:477: import [module_mla_asm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_mla_asm.so
(Worker_TP4 pid=524) [aiter] type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
(Worker_TP4 pid=524) [2026-02-15 22:47:02] WARNING core.py:927: type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
[aiter] hipModuleLoad: /usr/local/lib/python3.12/dist-packages/aiter_meta/hsa/gfx950//mla/mla_a8w8_qh16_qseqlen1_gqaratio16.co GetFunction: _ZN5aiter33mla_a8w8_qh16_qseqlen1_gqaratio16E Success
(Worker_TP3 pid=523) [aiter] import [module_mla_asm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_mla_asm.so
(Worker_TP3 pid=523) [2026-02-15 22:47:02] INFO core.py:477: import [module_mla_asm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_mla_asm.so
(Worker_TP3 pid=523) [aiter] type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
(Worker_TP3 pid=523) [2026-02-15 22:47:02] WARNING core.py:927: type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
[aiter] hipModuleLoad: /usr/local/lib/python3.12/dist-packages/aiter_meta/hsa/gfx950//mla/mla_a8w8_qh16_qseqlen1_gqaratio16.co GetFunction: _ZN5aiter33mla_a8w8_qh16_qseqlen1_gqaratio16E Success
(Worker_TP5 pid=525) [aiter] import [module_mla_asm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_mla_asm.so
(Worker_TP5 pid=525) [2026-02-15 22:47:02] INFO core.py:477: import [module_mla_asm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_mla_asm.so
(Worker_TP5 pid=525) [aiter] type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
(Worker_TP5 pid=525) [2026-02-15 22:47:02] WARNING core.py:927: type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
[aiter] hipModuleLoad: /usr/local/lib/python3.12/dist-packages/aiter_meta/hsa/gfx950//mla/mla_a8w8_qh16_qseqlen1_gqaratio16.co GetFunction: _ZN5aiter33mla_a8w8_qh16_qseqlen1_gqaratio16E Success
(Worker_TP1 pid=521) [aiter] [fused_moe] using 2stage default for (256, 32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
(Worker_TP1 pid=521) [2026-02-15 22:47:02] INFO fused_moe.py:672: [fused_moe] using 2stage default for (256, 32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
(Worker_TP0 pid=520) [aiter] [fused_moe] using 2stage default for (256, 32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
(Worker_TP0 pid=520) [2026-02-15 22:47:02] INFO fused_moe.py:672: [fused_moe] using 2stage default for (256, 32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
(Worker_TP6 pid=526) [aiter] [fused_moe] using 2stage default for (256, 32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
(Worker_TP6 pid=526) [2026-02-15 22:47:02] INFO fused_moe.py:672: [fused_moe] using 2stage default for (256, 32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
(Worker_TP7 pid=527) [aiter] [fused_moe] using 2stage default for (256, 32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
(Worker_TP7 pid=527) [2026-02-15 22:47:02] INFO fused_moe.py:672: [fused_moe] using 2stage default for (256, 32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
(Worker_TP2 pid=522) [aiter] [fused_moe] using 2stage default for (256, 32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
(Worker_TP2 pid=522) [2026-02-15 22:47:02] INFO fused_moe.py:672: [fused_moe] using 2stage default for (256, 32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
(Worker_TP4 pid=524) [aiter] [fused_moe] using 2stage default for (256, 32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
(Worker_TP4 pid=524) [2026-02-15 22:47:02] INFO fused_moe.py:672: [fused_moe] using 2stage default for (256, 32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
(Worker_TP3 pid=523) [aiter] [fused_moe] using 2stage default for (256, 32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
(Worker_TP3 pid=523) [2026-02-15 22:47:02] INFO fused_moe.py:672: [fused_moe] using 2stage default for (256, 32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
(Worker_TP5 pid=525) [aiter] [fused_moe] using 2stage default for (256, 32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
(Worker_TP5 pid=525) [2026-02-15 22:47:02] INFO fused_moe.py:672: [fused_moe] using 2stage default for (256, 32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:05<00:12,  2.53s/it](Worker_TP1 pid=521) [aiter] [fused_moe] using 2stage default for (256, 16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
(Worker_TP1 pid=521) [2026-02-15 22:47:05] INFO fused_moe.py:672: [fused_moe] using 2stage default for (256, 16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
(Worker_TP0 pid=520) [2026-02-15 22:47:05] INFO fused_moe.py:672: [fused_moe] using 2stage default for (256, 16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 3/7 [00:06<00:07,  1.84s/it](Worker_TP7 pid=527) [aiter] [fused_moe] using 2stage default for (256, 16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
(Worker_TP7 pid=527) [2026-02-15 22:47:05] INFO fused_moe.py:672: [fused_moe] using 2stage default for (256, 16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
(Worker_TP6 pid=526) [aiter] [fused_moe] using 2stage default for (256, 16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
(Worker_TP6 pid=526) [2026-02-15 22:47:05] INFO fused_moe.py:672: [fused_moe] using 2stage default for (256, 16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
(Worker_TP2 pid=522) [aiter] [fused_moe] using 2stage default for (256, 16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
(Worker_TP2 pid=522) [2026-02-15 22:47:05] INFO fused_moe.py:672: [fused_moe] using 2stage default for (256, 16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:06<00:03,  1.21s/it](Worker_TP3 pid=523) [aiter] [fused_moe] using 2stage default for (256, 16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
(Worker_TP3 pid=523) [2026-02-15 22:47:05] INFO fused_moe.py:672: [fused_moe] using 2stage default for (256, 16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
(Worker_TP4 pid=524) [aiter] [fused_moe] using 2stage default for (256, 16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
(Worker_TP4 pid=524) [2026-02-15 22:47:05] INFO fused_moe.py:672: [fused_moe] using 2stage default for (256, 16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
(Worker_TP5 pid=525) [aiter] [fused_moe] using 2stage default for (256, 16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
(Worker_TP5 pid=525) [2026-02-15 22:47:06] INFO fused_moe.py:672: [fused_moe] using 2stage default for (256, 16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
(Worker_TP1 pid=521) INFO 02-15 22:47:09 [custom_all_reduce.py:216] Registering 861 cuda graph addresses
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:10<00:00,  1.47s/it]
(Worker_TP0 pid=520) INFO 02-15 22:47:09 [custom_all_reduce.py:216] Registering 861 cuda graph addresses
(Worker_TP6 pid=526) INFO 02-15 22:47:09 [custom_all_reduce.py:216] Registering 861 cuda graph addresses
(Worker_TP2 pid=522) INFO 02-15 22:47:09 [custom_all_reduce.py:216] Registering 861 cuda graph addresses
(Worker_TP7 pid=527) INFO 02-15 22:47:09 [custom_all_reduce.py:216] Registering 861 cuda graph addresses
(Worker_TP3 pid=523) INFO 02-15 22:47:09 [custom_all_reduce.py:216] Registering 861 cuda graph addresses
(Worker_TP4 pid=524) INFO 02-15 22:47:09 [custom_all_reduce.py:216] Registering 861 cuda graph addresses
(Worker_TP5 pid=525) INFO 02-15 22:47:10 [custom_all_reduce.py:216] Registering 861 cuda graph addresses
(Worker_TP0 pid=520) INFO 02-15 22:47:11 [gpu_model_runner.py:5063] Graph capturing finished in 13 secs, took 0.27 GiB
(EngineCore_DP0 pid=303) INFO 02-15 22:47:11 [core.py:272] init engine (profile, create kv cache, warmup model) took 57.73 seconds
(EngineCore_DP0 pid=303) INFO 02-15 22:47:18 [vllm.py:624] Asynchronous scheduling is enabled.
(EngineCore_DP0 pid=303) WARNING 02-15 22:47:18 [vllm.py:905] No piecewise cudagraph for executing cascade attention. Will fall back to eager execution if a batch runs into cascade attentions.
(APIServer pid=1) INFO 02-15 22:47:18 [async_llm.py:194] Torch profiler enabled. AsyncLLM CPU traces will be collected under /app
(APIServer pid=1) WARNING 02-15 22:47:19 [api_router.py:41] Profiler with mode 'torch' is enabled in the API server. This should ONLY be used for local development!
(APIServer pid=1) INFO 02-15 22:47:19 [api_server.py:665] Supported tasks: ['generate']
(APIServer pid=1) WARNING 02-15 22:47:19 [model.py:1371] Default vLLM sampling parameters have been overridden by the model's `generation_config.json`: `{'temperature': 0.6, 'top_p': 0.95}`. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
(APIServer pid=1) INFO 02-15 22:47:19 [serving.py:177] Warming up chat template processing...
(APIServer pid=1) INFO 02-15 22:47:19 [hf.py:310] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
(APIServer pid=1) INFO 02-15 22:47:19 [serving.py:212] Chat template warmup completed in 633.4ms
(APIServer pid=1) INFO 02-15 22:47:19 [api_server.py:946] Starting vLLM API server 0 on http://localhost:8000
(APIServer pid=1) INFO 02-15 22:47:19 [launcher.py:38] Available routes are:
(APIServer pid=1) INFO 02-15 22:47:19 [launcher.py:46] Route: /openapi.json, Methods: GET, HEAD
(APIServer pid=1) INFO 02-15 22:47:19 [launcher.py:46] Route: /docs, Methods: GET, HEAD
(APIServer pid=1) INFO 02-15 22:47:19 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: GET, HEAD
(APIServer pid=1) INFO 02-15 22:47:19 [launcher.py:46] Route: /redoc, Methods: GET, HEAD
(APIServer pid=1) INFO 02-15 22:47:19 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
(APIServer pid=1) INFO 02-15 22:47:19 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
(APIServer pid=1) INFO 02-15 22:47:19 [launcher.py:46] Route: /start_profile, Methods: POST
(APIServer pid=1) INFO 02-15 22:47:19 [launcher.py:46] Route: /stop_profile, Methods: POST
(APIServer pid=1) INFO 02-15 22:47:19 [launcher.py:46] Route: /tokenize, Methods: POST
(APIServer pid=1) INFO 02-15 22:47:19 [launcher.py:46] Route: /detokenize, Methods: POST
(APIServer pid=1) INFO 02-15 22:47:19 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
(APIServer pid=1) INFO 02-15 22:47:19 [launcher.py:46] Route: /pause, Methods: POST
(APIServer pid=1) INFO 02-15 22:47:19 [launcher.py:46] Route: /resume, Methods: POST
(APIServer pid=1) INFO 02-15 22:47:19 [launcher.py:46] Route: /is_paused, Methods: GET
(APIServer pid=1) INFO 02-15 22:47:19 [launcher.py:46] Route: /metrics, Methods: GET
(APIServer pid=1) INFO 02-15 22:47:19 [launcher.py:46] Route: /health, Methods: GET
(APIServer pid=1) INFO 02-15 22:47:19 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
(APIServer pid=1) INFO 02-15 22:47:19 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
(APIServer pid=1) INFO 02-15 22:47:19 [launcher.py:46] Route: /v1/responses, Methods: POST
(APIServer pid=1) INFO 02-15 22:47:19 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
(APIServer pid=1) INFO 02-15 22:47:19 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
(APIServer pid=1) INFO 02-15 22:47:19 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
(APIServer pid=1) INFO 02-15 22:47:19 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
(APIServer pid=1) INFO 02-15 22:47:19 [launcher.py:46] Route: /v1/completions, Methods: POST
(APIServer pid=1) INFO 02-15 22:47:19 [launcher.py:46] Route: /v1/completions/render, Methods: POST
(APIServer pid=1) INFO 02-15 22:47:19 [launcher.py:46] Route: /v1/messages, Methods: POST
(APIServer pid=1) INFO 02-15 22:47:19 [launcher.py:46] Route: /v1/models, Methods: GET
(APIServer pid=1) INFO 02-15 22:47:19 [launcher.py:46] Route: /load, Methods: GET
(APIServer pid=1) INFO 02-15 22:47:19 [launcher.py:46] Route: /version, Methods: GET
(APIServer pid=1) INFO 02-15 22:47:19 [launcher.py:46] Route: /ping, Methods: GET
(APIServer pid=1) INFO 02-15 22:47:19 [launcher.py:46] Route: /ping, Methods: POST
(APIServer pid=1) INFO 02-15 22:47:19 [launcher.py:46] Route: /invocations, Methods: POST
(APIServer pid=1) INFO 02-15 22:47:19 [launcher.py:46] Route: /classify, Methods: POST
(APIServer pid=1) INFO 02-15 22:47:19 [launcher.py:46] Route: /v1/embeddings, Methods: POST
(APIServer pid=1) INFO 02-15 22:47:19 [launcher.py:46] Route: /score, Methods: POST
(APIServer pid=1) INFO 02-15 22:47:19 [launcher.py:46] Route: /v1/score, Methods: POST
(APIServer pid=1) INFO 02-15 22:47:19 [launcher.py:46] Route: /rerank, Methods: POST
(APIServer pid=1) INFO 02-15 22:47:19 [launcher.py:46] Route: /v1/rerank, Methods: POST
(APIServer pid=1) INFO 02-15 22:47:19 [launcher.py:46] Route: /v2/rerank, Methods: POST
(APIServer pid=1) INFO 02-15 22:47:19 [launcher.py:46] Route: /pooling, Methods: POST
(APIServer pid=1) INFO:     Started server process [1]
(APIServer pid=1) INFO:     Waiting for application startup.
(APIServer pid=1) INFO:     Application startup complete.
(Worker_TP1 pid=521) [aiter] start build [module_fmha_v3_varlen_fwd] under /usr/local/lib/python3.12/dist-packages/aiter/jit/build/module_fmha_v3_varlen_fwd
(Worker_TP1 pid=521) [2026-02-15 22:48:41] INFO core.py:524: start build [module_fmha_v3_varlen_fwd] under /usr/local/lib/python3.12/dist-packages/aiter/jit/build/module_fmha_v3_varlen_fwd
(Worker_TP6 pid=526) [aiter] waiting for baton release at /usr/local/lib/python3.12/dist-packages/aiter/jit/build/lock_module_fmha_v3_varlen_fwd
(Worker_TP6 pid=526) [2026-02-15 22:48:41] INFO file_baton.py:48: waiting for baton release at /usr/local/lib/python3.12/dist-packages/aiter/jit/build/lock_module_fmha_v3_varlen_fwd
(Worker_TP2 pid=522) [aiter] waiting for baton release at /usr/local/lib/python3.12/dist-packages/aiter/jit/build/lock_module_fmha_v3_varlen_fwd
(Worker_TP2 pid=522) [2026-02-15 22:48:41] INFO file_baton.py:48: waiting for baton release at /usr/local/lib/python3.12/dist-packages/aiter/jit/build/lock_module_fmha_v3_varlen_fwd
(Worker_TP4 pid=524) [aiter] waiting for baton release at /usr/local/lib/python3.12/dist-packages/aiter/jit/build/lock_module_fmha_v3_varlen_fwd
(Worker_TP4 pid=524) [2026-02-15 22:48:41] INFO file_baton.py:48: waiting for baton release at /usr/local/lib/python3.12/dist-packages/aiter/jit/build/lock_module_fmha_v3_varlen_fwd
(Worker_TP0 pid=520) [aiter] waiting for baton release at /usr/local/lib/python3.12/dist-packages/aiter/jit/build/lock_module_fmha_v3_varlen_fwd
(Worker_TP0 pid=520) [2026-02-15 22:48:41] INFO file_baton.py:48: waiting for baton release at /usr/local/lib/python3.12/dist-packages/aiter/jit/build/lock_module_fmha_v3_varlen_fwd
(Worker_TP3 pid=523) [aiter] waiting for baton release at /usr/local/lib/python3.12/dist-packages/aiter/jit/build/lock_module_fmha_v3_varlen_fwd
(Worker_TP3 pid=523) [2026-02-15 22:48:41] INFO file_baton.py:48: waiting for baton release at /usr/local/lib/python3.12/dist-packages/aiter/jit/build/lock_module_fmha_v3_varlen_fwd
(Worker_TP5 pid=525) [aiter] waiting for baton release at /usr/local/lib/python3.12/dist-packages/aiter/jit/build/lock_module_fmha_v3_varlen_fwd
(Worker_TP5 pid=525) [2026-02-15 22:48:41] INFO file_baton.py:48: waiting for baton release at /usr/local/lib/python3.12/dist-packages/aiter/jit/build/lock_module_fmha_v3_varlen_fwd
(Worker_TP7 pid=527) [aiter] waiting for baton release at /usr/local/lib/python3.12/dist-packages/aiter/jit/build/lock_module_fmha_v3_varlen_fwd
(Worker_TP7 pid=527) [2026-02-15 22:48:41] INFO file_baton.py:48: waiting for baton release at /usr/local/lib/python3.12/dist-packages/aiter/jit/build/lock_module_fmha_v3_varlen_fwd
(Worker_TP1 pid=521) [aiter] finish build [module_fmha_v3_varlen_fwd], cost 26.4s 
(Worker_TP1 pid=521) [2026-02-15 22:49:07] INFO core.py:673: finish build [module_fmha_v3_varlen_fwd], cost 26.4s 
(Worker_TP1 pid=521) [aiter] import [module_fmha_v3_varlen_fwd] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_fmha_v3_varlen_fwd.so
(Worker_TP1 pid=521) [2026-02-15 22:49:07] INFO core.py:477: import [module_fmha_v3_varlen_fwd] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_fmha_v3_varlen_fwd.so
(Worker_TP1 pid=521) [aiter] type hints mismatch, override to --> fmha_v3_varlen_fwd(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, cu_seqlens_k: Optional[torch.Tensor], max_seqlen_q: int, max_seqlen_k: int, min_seqlen_q: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, how_v3_bf16_cvt: int, out: Optional[torch.Tensor] = None, block_table: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> List[torch.Tensor]
(Worker_TP1 pid=521) [2026-02-15 22:49:07] WARNING core.py:927: type hints mismatch, override to --> fmha_v3_varlen_fwd(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, cu_seqlens_k: Optional[torch.Tensor], max_seqlen_q: int, max_seqlen_k: int, min_seqlen_q: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, how_v3_bf16_cvt: int, out: Optional[torch.Tensor] = None, block_table: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> List[torch.Tensor]
(Worker_TP2 pid=522) [aiter] import [module_fmha_v3_varlen_fwd] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_fmha_v3_varlen_fwd.so
(Worker_TP2 pid=522) [2026-02-15 22:49:07] INFO core.py:477: import [module_fmha_v3_varlen_fwd] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_fmha_v3_varlen_fwd.so
(Worker_TP4 pid=524) [aiter] import [module_fmha_v3_varlen_fwd] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_fmha_v3_varlen_fwd.so
(Worker_TP4 pid=524) [2026-02-15 22:49:07] INFO core.py:477: import [module_fmha_v3_varlen_fwd] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_fmha_v3_varlen_fwd.so
(Worker_TP6 pid=526) [aiter] import [module_fmha_v3_varlen_fwd] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_fmha_v3_varlen_fwd.so
(Worker_TP6 pid=526) [2026-02-15 22:49:07] INFO core.py:477: import [module_fmha_v3_varlen_fwd] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_fmha_v3_varlen_fwd.so
(Worker_TP2 pid=522) [aiter] type hints mismatch, override to --> fmha_v3_varlen_fwd(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, cu_seqlens_k: Optional[torch.Tensor], max_seqlen_q: int, max_seqlen_k: int, min_seqlen_q: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, how_v3_bf16_cvt: int, out: Optional[torch.Tensor] = None, block_table: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> List[torch.Tensor]
(Worker_TP4 pid=524) [aiter] type hints mismatch, override to --> fmha_v3_varlen_fwd(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, cu_seqlens_k: Optional[torch.Tensor], max_seqlen_q: int, max_seqlen_k: int, min_seqlen_q: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, how_v3_bf16_cvt: int, out: Optional[torch.Tensor] = None, block_table: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> List[torch.Tensor]
(Worker_TP2 pid=522) [2026-02-15 22:49:07] WARNING core.py:927: type hints mismatch, override to --> fmha_v3_varlen_fwd(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, cu_seqlens_k: Optional[torch.Tensor], max_seqlen_q: int, max_seqlen_k: int, min_seqlen_q: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, how_v3_bf16_cvt: int, out: Optional[torch.Tensor] = None, block_table: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> List[torch.Tensor]
(Worker_TP4 pid=524) [2026-02-15 22:49:07] WARNING core.py:927: type hints mismatch, override to --> fmha_v3_varlen_fwd(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, cu_seqlens_k: Optional[torch.Tensor], max_seqlen_q: int, max_seqlen_k: int, min_seqlen_q: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, how_v3_bf16_cvt: int, out: Optional[torch.Tensor] = None, block_table: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> List[torch.Tensor]
(Worker_TP6 pid=526) [aiter] type hints mismatch, override to --> fmha_v3_varlen_fwd(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, cu_seqlens_k: Optional[torch.Tensor], max_seqlen_q: int, max_seqlen_k: int, min_seqlen_q: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, how_v3_bf16_cvt: int, out: Optional[torch.Tensor] = None, block_table: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> List[torch.Tensor]
(Worker_TP6 pid=526) [2026-02-15 22:49:07] WARNING core.py:927: type hints mismatch, override to --> fmha_v3_varlen_fwd(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, cu_seqlens_k: Optional[torch.Tensor], max_seqlen_q: int, max_seqlen_k: int, min_seqlen_q: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, how_v3_bf16_cvt: int, out: Optional[torch.Tensor] = None, block_table: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> List[torch.Tensor]
(Worker_TP0 pid=520) [aiter] import [module_fmha_v3_varlen_fwd] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_fmha_v3_varlen_fwd.so
(Worker_TP0 pid=520) [2026-02-15 22:49:07] INFO core.py:477: import [module_fmha_v3_varlen_fwd] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_fmha_v3_varlen_fwd.so
(Worker_TP0 pid=520) [aiter] type hints mismatch, override to --> fmha_v3_varlen_fwd(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, cu_seqlens_k: Optional[torch.Tensor], max_seqlen_q: int, max_seqlen_k: int, min_seqlen_q: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, how_v3_bf16_cvt: int, out: Optional[torch.Tensor] = None, block_table: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> List[torch.Tensor]
(Worker_TP0 pid=520) [2026-02-15 22:49:07] WARNING core.py:927: type hints mismatch, override to --> fmha_v3_varlen_fwd(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, cu_seqlens_k: Optional[torch.Tensor], max_seqlen_q: int, max_seqlen_k: int, min_seqlen_q: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, how_v3_bf16_cvt: int, out: Optional[torch.Tensor] = None, block_table: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> List[torch.Tensor]
(Worker_TP3 pid=523) [aiter] import [module_fmha_v3_varlen_fwd] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_fmha_v3_varlen_fwd.so
(Worker_TP3 pid=523) [2026-02-15 22:49:07] INFO core.py:477: import [module_fmha_v3_varlen_fwd] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_fmha_v3_varlen_fwd.so
(Worker_TP3 pid=523) [aiter] type hints mismatch, override to --> fmha_v3_varlen_fwd(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, cu_seqlens_k: Optional[torch.Tensor], max_seqlen_q: int, max_seqlen_k: int, min_seqlen_q: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, how_v3_bf16_cvt: int, out: Optional[torch.Tensor] = None, block_table: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> List[torch.Tensor]
(Worker_TP3 pid=523) [2026-02-15 22:49:07] WARNING core.py:927: type hints mismatch, override to --> fmha_v3_varlen_fwd(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, cu_seqlens_k: Optional[torch.Tensor], max_seqlen_q: int, max_seqlen_k: int, min_seqlen_q: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, how_v3_bf16_cvt: int, out: Optional[torch.Tensor] = None, block_table: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> List[torch.Tensor]
(Worker_TP5 pid=525) [aiter] import [module_fmha_v3_varlen_fwd] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_fmha_v3_varlen_fwd.so
(Worker_TP5 pid=525) [2026-02-15 22:49:07] INFO core.py:477: import [module_fmha_v3_varlen_fwd] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_fmha_v3_varlen_fwd.so
(Worker_TP5 pid=525) [aiter] type hints mismatch, override to --> fmha_v3_varlen_fwd(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, cu_seqlens_k: Optional[torch.Tensor], max_seqlen_q: int, max_seqlen_k: int, min_seqlen_q: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, how_v3_bf16_cvt: int, out: Optional[torch.Tensor] = None, block_table: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> List[torch.Tensor]
(Worker_TP5 pid=525) [2026-02-15 22:49:07] WARNING core.py:927: type hints mismatch, override to --> fmha_v3_varlen_fwd(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, cu_seqlens_k: Optional[torch.Tensor], max_seqlen_q: int, max_seqlen_k: int, min_seqlen_q: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, how_v3_bf16_cvt: int, out: Optional[torch.Tensor] = None, block_table: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> List[torch.Tensor]
(Worker_TP7 pid=527) [aiter] import [module_fmha_v3_varlen_fwd] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_fmha_v3_varlen_fwd.so
(Worker_TP7 pid=527) [2026-02-15 22:49:07] INFO core.py:477: import [module_fmha_v3_varlen_fwd] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_fmha_v3_varlen_fwd.so
(Worker_TP7 pid=527) [aiter] type hints mismatch, override to --> fmha_v3_varlen_fwd(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, cu_seqlens_k: Optional[torch.Tensor], max_seqlen_q: int, max_seqlen_k: int, min_seqlen_q: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, how_v3_bf16_cvt: int, out: Optional[torch.Tensor] = None, block_table: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> List[torch.Tensor]
(Worker_TP7 pid=527) [2026-02-15 22:49:07] WARNING core.py:927: type hints mismatch, override to --> fmha_v3_varlen_fwd(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, cu_seqlens_k: Optional[torch.Tensor], max_seqlen_q: int, max_seqlen_k: int, min_seqlen_q: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, how_v3_bf16_cvt: int, out: Optional[torch.Tensor] = None, block_table: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> List[torch.Tensor]
(APIServer pid=1) INFO:     127.0.0.1:34790 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=1) INFO 02-15 22:49:10 [loggers.py:257] Engine 000: Avg prompt throughput: 0.7 tokens/s, Avg generation throughput: 4.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
(APIServer pid=1) INFO 02-15 22:49:20 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
(APIServer pid=1) INFO:     127.0.0.1:39466 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=1) INFO 02-15 22:50:10 [loggers.py:257] Engine 000: Avg prompt throughput: 1.3 tokens/s, Avg generation throughput: 5.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 10.0%
(APIServer pid=1) INFO 02-15 22:50:20 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 10.0%
(APIServer pid=1) INFO:     127.0.0.1:48166 - "GET /metrics HTTP/1.1" 200 OK
(APIServer pid=1) INFO:     127.0.0.1:48166 - "POST /v1/completions HTTP/1.1" 200 OK
(APIServer pid=1) INFO:     127.0.0.1:48180 - "POST /v1/completions HTTP/1.1" 200 OK
(APIServer pid=1) INFO:     127.0.0.1:48192 - "POST /v1/completions HTTP/1.1" 200 OK
(APIServer pid=1) INFO:     127.0.0.1:48200 - "POST /v1/completions HTTP/1.1" 200 OK
(APIServer pid=1) INFO 02-15 22:50:40 [loggers.py:257] Engine 000: Avg prompt throughput: 3276.6 tokens/s, Avg generation throughput: 8.6 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
(APIServer pid=1) INFO 02-15 22:50:50 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 185.2 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
(APIServer pid=1) INFO 02-15 22:51:00 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.4 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
(APIServer pid=1) INFO:     127.0.0.1:48166 - "GET /metrics HTTP/1.1" 200 OK
(APIServer pid=1) INFO 02-15 22:51:10 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 35.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
(APIServer pid=1) INFO 02-15 22:51:20 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
