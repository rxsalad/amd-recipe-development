root@rs-amd-validation-test1:~/data/recipes# kubectl apply -f 350-vllm-dev.yaml 
deployment.apps/mi350-vllm-dev created


root@rs-amd-validation-test1:~/data/recipes# kubectl get pods
NAME                              READY   STATUS    RESTARTS   AGE
mi325-general-5788f487b4-kk6tn    1/1     Running   0          27h
mi350-vllm-dev-665c9df6bb-gzb29   1/1     Running   0          3m26s


root@rs-amd-validation-test1:~/data/recipes# kubectl exec -it mi350-vllm-dev-665c9df6bb-gzb29 -- /bin/bash
root@mi350-vllm-dev-665c9df6bb-gzb29:/app# 


root@mi350-vllm-dev-665c9df6bb-gzb29:/app# vllm --version
INFO 02-15 22:57:29 [__init__.py:216] Automatically detected platform rocm.
[aiter] merge tuned file under model_configs/ and configs/ /usr/local/lib/python3.12/dist-packages/aiter/configs/a8w8_bpreshuffle_tuned_gemm.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_bpreshuffle_tuned_gemm_dsv3.csv
[aiter] merge tuned file under model_configs/ and configs/ /usr/local/lib/python3.12/dist-packages/aiter/configs/a8w8_blockscale_tuned_gemm.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_ds_v3.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_qwen3_235b.csv
[aiter] merge tuned file under model_configs/ and configs/ /usr/local/lib/python3.12/dist-packages/aiter/configs/tuned_fmoe.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[aiter] merge tuned file under model_configs/ and configs/ /usr/local/lib/python3.12/dist-packages/aiter/configs/a8w8_blockscale_bpreshuffle_tuned_gemm.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_blockscale_bpreshuffle_tuned_gemm_dsv3.csv
[aiter] import [module_aiter_enum] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_aiter_enum.so
0.9.2rc2.dev3441+g1d8c40f20.d20251122.rocm711


root@mi350-vllm-dev-665c9df6bb-gzb29:/app# curl http://localhost:8000/v1/chat/completions   -H "Content-Type: application/json"   -d '{
    "model": "amd/DeepSeek-R1-MXFP4",
    "messages": [{"role": "user", "content": "who are you?"}]
  }'
{"id":"chatcmpl-9acb9e7b89424285a126e1325aad49f2","object":"chat.completion","created":1771194713,"model":"amd/DeepSeek-R1-MXFP4","choices":[{"index":0,"message":{"role":"assistant","content":"<think>\n\n</think>\n\nGreetings! I'm DeepSeek-R1, an artificial intelligence assistant created by DeepSeek. I'm at your service and would be delighted to assist you with any inquiries or tasks you may have.","refusal":null,"annotations":null,"audio":null,"function_call":null,"tool_calls":[],"reasoning_content":null},"logprobs":null,"finish_reason":"stop","stop_reason":null,"token_ids":null}],"service_tier":null,"system_fingerprint":null,"usage":{"prompt_tokens":7,"total_tokens":54,"completion_tokens":47,"prompt_tokens_details":null},"prompt_logprobs":null,"prompt_token_ids":null,"kv_transfer_params":null}root@mi350-vllm-dev-665c9df6bb-gzb29:/app# 


root@mi350-vllm-dev-665c9df6bb-gzb29:/app# curl http://localhost:8000/v1/chat/completions   -H "Content-Type: application/json"   -d '{
    "model": "amd/DeepSeek-R1-MXFP4",
    "messages": [{"role": "user", "content": "Are you based on DeepSeek V3?"}]
  }'
{"id":"chatcmpl-8460303977b44f6b98511445e0d8c80c","object":"chat.completion","created":1771194750,"model":"amd/DeepSeek-R1-MXFP4","choices":[{"index":0,"message":{"role":"assistant","content":"<think>\n\n</think>\n\nHi! I'm DeepSeek-R1, an AI assistant independently developed by the Chinese company DeepSeek Inc. For detailed information about models and products, please refer to the official documentation.","refusal":null,"annotations":null,"audio":null,"function_call":null,"tool_calls":[],"reasoning_content":null},"logprobs":null,"finish_reason":"stop","stop_reason":null,"token_ids":null}],"service_tier":null,"system_fingerprint":null,"usage":{"prompt_tokens":13,"total_tokens":57,"completion_tokens":44,"prompt_tokens_details":null},"prompt_logprobs":null,"prompt_token_ids":null,"kv_transfer_params":null}root@mi350-vllm-dev-665c9df6bb-gzb29:/app# 


root@mi350-vllm-dev-665c9df6bb-gzb29:/app# vllm bench serve \
  --model "amd/DeepSeek-R1-MXFP4" \
  --dataset-name random \
  --random-input-len 8192 \
  --random-output-len 1024 \
  --request-rate 10000 \
  --num-prompts 4 \
  --ignore-eos \
  --trust-remote-code 
INFO 02-15 22:32:43 [__init__.py:216] Automatically detected platform rocm.
[aiter] merge tuned file under model_configs/ and configs/ /usr/local/lib/python3.12/dist-packages/aiter/configs/a8w8_bpreshuffle_tuned_gemm.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_bpreshuffle_tuned_gemm_dsv3.csv
[aiter] merge tuned file under model_configs/ and configs/ /usr/local/lib/python3.12/dist-packages/aiter/configs/a8w8_blockscale_tuned_gemm.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_ds_v3.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_qwen3_235b.csv
[aiter] merge tuned file under model_configs/ and configs/ /usr/local/lib/python3.12/dist-packages/aiter/configs/tuned_fmoe.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[aiter] merge tuned file under model_configs/ and configs/ /usr/local/lib/python3.12/dist-packages/aiter/configs/a8w8_blockscale_bpreshuffle_tuned_gemm.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_blockscale_bpreshuffle_tuned_gemm_dsv3.csv
[aiter] import [module_aiter_enum] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_aiter_enum.so
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f6e56f7a980>, seed=0, num_prompts=4, dataset_name='random', no_stream=False, dataset_path=None, no_oversample=False, custom_output_len=256, custom_skip_chat_template=False, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=8192, random_output_len=1024, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 0}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', endpoint_type=None, base_url=None, host='127.0.0.1', port=8000, endpoint='/v1/completions', header=None, max_concurrency=None, model='amd/DeepSeek-R1-MXFP4', tokenizer=None, use_beam_search=False, logprobs=None, request_rate=10000.0, burstiness=1.0, trust_remote_code=True, disable_tqdm=False, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=True, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, request_id_prefix='benchmark-serving', top_p=None, top_k=None, min_p=None, temperature=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600)
INFO 02-15 22:32:47 [datasets.py:507] Sampling input_len from [8191, 8191] and output_len from [1024, 1024]
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
 |                                                                                                   | 00:15 elapsed, 2731:05:49 remaining
Initial test run completed. Starting main benchmark run...
Traffic request rate: 10000.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: None
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:15<00:00,  3.88s/it]
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     4         
Request rate configured (RPS):           10000.00  
Benchmark duration (s):                  15.53     
Total input tokens:                      32764     
Total generated tokens:                  4096      
Request throughput (req/s):              0.26      
Output token throughput (tok/s):         263.69    
Peak output token throughput (tok/s):    312.00    
Peak concurrent requests:                4.00      
Total Token throughput (tok/s):          2372.91   
---------------Time to First Token----------------
Mean TTFT (ms):                          1549.89   
Median TTFT (ms):                        1887.87   
P99 TTFT (ms):                           2323.21   
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          13.65     
Median TPOT (ms):                        13.32     
P99 TPOT (ms):                           15.01     
---------------Inter-token Latency----------------
Mean ITL (ms):                           13.65     
Median ITL (ms):                         12.88     
P99 ITL (ms):                            13.70     
==================================================
root@mi350-vllm-dev-665c9df6bb-gzb29:/app# 



root@rs-amd-validation-test1:~/data/recipes# kubectl logs -f mi350-vllm-dev-665c9df6bb-gzb29
+ max_num_seqs=32
+ max_num_batched_tokens=163840
+ max_seq_len_to_capture=1024
+ tensor_parallel_size=8
+ max_model_len=70000
+ MODEL=amd/DeepSeek-R1-MXFP4
+ unset FLATMM_HIP_CLANG_PATH
+ VLLM_USE_V1=1
+ VLLM_DISABLE_COMPILE_CACHE=1
+ AMDGCN_USE_BUFFER_OPS=1
+ VLLM_TORCH_PROFILER_RECORD_SHAPES=1
+ VLLM_ROCM_USE_AITER=1
+ VLLM_TRITON_FP4_GEMM_USE_ASM=0
+ VLLM_ROCM_USE_AITER_FP4_ASM_GEMM=0
+ VLLM_ROCM_USE_AITER_MHA=1
+ VLLM_ROCM_USE_AITER_MLA=0
+ VLLM_ROCM_USE_CK_MXFP4_MOE=1
+ VLLM_TORCH_PROFILER_DIR=.
+ VLLM_TORCH_PROFILER_WITH_STACK=0
+ VLLM_ROCM_USE_AITER_TRITON_MLA=0
+ VLLM_ROCM_USE_AITER_TRITON_FUSED_SHARED_EXPERTS=1
+ VLLM_ROCM_USE_AITER_TRITON_FUSED_RMSNORM_FP4_QUANT=1
+ VLLM_ROCM_USE_AITER_TRITON_FUSED_ROPE_ZEROS_KV_CACHE=1
+ VLLM_ROCM_USE_AITER_TRITON_MXFP4_BMM=1
+ VLLM_ROCM_USE_AITER_TRITON_FUSED_MUL_ADD=1
+ VLLM_ROCM_USE_AITER_TRITON_FP8_BMM=0
+ vllm serve amd/DeepSeek-R1-MXFP4 --host localhost --port 8000 --swap-space 64 --disable-log-requests --dtype auto --tensor-parallel-size 8 --max-num-seqs 32 --distributed-executor-backend mp --trust-remote-code --block-size 1 '--compilation-config={"pass_config":{"enable_attn_fusion":true,"enable_noop":true,"enable_fusion":true},"cudagraph_mode":"FULL","custom_ops":["+rms_norm","+silu_and_mul","+quant_fp8"],"splitting_ops":[]}' --gpu-memory-utilization 0.95 --max-model-len 70000 --kv-cache-dtype fp8 --max-seq-len-to-capture 1024 --max-num-batched-tokens 163840 --async-scheduling
INFO 02-15 22:26:57 [__init__.py:216] Automatically detected platform rocm.
[aiter] merge tuned file under model_configs/ and configs/ /usr/local/lib/python3.12/dist-packages/aiter/configs/a8w8_bpreshuffle_tuned_gemm.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_bpreshuffle_tuned_gemm_dsv3.csv
[aiter] merge tuned file under model_configs/ and configs/ /usr/local/lib/python3.12/dist-packages/aiter/configs/a8w8_blockscale_tuned_gemm.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_ds_v3.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_qwen3_235b.csv
[aiter] merge tuned file under model_configs/ and configs/ /usr/local/lib/python3.12/dist-packages/aiter/configs/tuned_fmoe.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[aiter] merge tuned file under model_configs/ and configs/ /usr/local/lib/python3.12/dist-packages/aiter/configs/a8w8_blockscale_bpreshuffle_tuned_gemm.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_blockscale_bpreshuffle_tuned_gemm_dsv3.csv
[aiter] import [module_aiter_enum] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_aiter_enum.so
WARNING 02-15 22:27:00 [api_server.py:1191] Torch Profiler is enabled in the API server. This should ONLY be used for local development!
WARNING 02-15 22:27:00 [__init__.py:1707] argument '--disable-log-requests' is deprecated and replaced with '--enable-log-requests'. This will be removed in v0.12.0.
(APIServer pid=1) INFO 02-15 22:27:00 [api_server.py:1822] vLLM API server version 0.9.2rc2.dev3441+g1d8c40f20.d20251122
(APIServer pid=1) INFO 02-15 22:27:00 [utils.py:328] non-default args: {'model_tag': 'amd/DeepSeek-R1-MXFP4', 'host': 'localhost', 'model': 'amd/DeepSeek-R1-MXFP4', 'trust_remote_code': True, 'max_model_len': 70000, 'max_seq_len_to_capture': 1024, 'distributed_executor_backend': 'mp', 'tensor_parallel_size': 8, 'block_size': 1, 'gpu_memory_utilization': 0.95, 'swap_space': 64.0, 'kv_cache_dtype': 'fp8', 'max_num_batched_tokens': 163840, 'max_num_seqs': 32, 'async_scheduling': True, 'compilation_config': {"level":null,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["+rms_norm","+silu_and_mul","+quant_fp8"],"splitting_ops":[],"use_inductor":true,"compile_sizes":null,"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":2,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":null,"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":null,"local_cache_dir":null}}
(APIServer pid=1) The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
(APIServer pid=1) INFO 02-15 22:27:00 [config.py:384] Replacing legacy 'type' key with 'rope_type'
(APIServer pid=1) INFO 02-15 22:27:07 [layer.py:42] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_ROPE_ZEROS_KV_CACHE=True
(APIServer pid=1) INFO 02-15 22:27:07 [model.py:550] Resolved architecture: DeepseekV3ForCausalLM
(APIServer pid=1) `torch_dtype` is deprecated! Use `dtype` instead!
(APIServer pid=1) INFO 02-15 22:27:07 [model.py:1576] Using max model len 70000
(APIServer pid=1) INFO 02-15 22:27:07 [cache.py:174] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. Meanwhile, it may cause accuracy drop without a proper scaling factor.
(APIServer pid=1) INFO 02-15 22:27:07 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=163840.
(APIServer pid=1) INFO 02-15 22:27:07 [__init__.py:626] CUDAGraphMode.FULL is not supported with cascade attention currently. Disabling cascadeattention.
(APIServer pid=1) WARNING 02-15 22:27:07 [compilation.py:609] Using piecewise compilation with empty splitting_ops and use_inductor_graph_partition=False.
INFO 02-15 22:27:10 [__init__.py:216] Automatically detected platform rocm.
[aiter] merge tuned file under model_configs/ and configs/ /usr/local/lib/python3.12/dist-packages/aiter/configs/a8w8_bpreshuffle_tuned_gemm.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_bpreshuffle_tuned_gemm_dsv3.csv
[aiter] merge tuned file under model_configs/ and configs/ /usr/local/lib/python3.12/dist-packages/aiter/configs/a8w8_blockscale_tuned_gemm.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_ds_v3.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_qwen3_235b.csv
[aiter] merge tuned file under model_configs/ and configs/ /usr/local/lib/python3.12/dist-packages/aiter/configs/tuned_fmoe.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[aiter] merge tuned file under model_configs/ and configs/ /usr/local/lib/python3.12/dist-packages/aiter/configs/a8w8_blockscale_bpreshuffle_tuned_gemm.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_blockscale_bpreshuffle_tuned_gemm_dsv3.csv
[aiter] import [module_aiter_enum] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_aiter_enum.so
(EngineCore_DP0 pid=303) INFO 02-15 22:27:13 [core.py:644] Waiting for init message from front-end.
(EngineCore_DP0 pid=303) INFO 02-15 22:27:13 [core.py:77] Initializing a V1 LLM engine (v0.9.2rc2.dev3441+g1d8c40f20.d20251122) with config: model='amd/DeepSeek-R1-MXFP4', speculative_config=None, tokenizer='amd/DeepSeek-R1-MXFP4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=70000, download_dir=None, load_format=auto, tensor_parallel_size=8, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=quark, enforce_eager=False, kv_cache_dtype=fp8, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=amd/DeepSeek-R1-MXFP4, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["+rms_norm","+silu_and_mul","+quant_fp8","+rms_norm"],"splitting_ops":[],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":2,"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":64,"local_cache_dir":null}
(EngineCore_DP0 pid=303) WARNING 02-15 22:27:13 [multiproc_executor.py:720] Reducing Torch parallelism from 192 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
(EngineCore_DP0 pid=303) INFO 02-15 22:27:13 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3, 4, 5, 6, 7], buffer_handle=(8, 16777216, 10, 'psm_5344cd3f'), local_subscribe_addr='ipc:///tmp/6d3d8a54-689c-4653-92f9-a9229949425f', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 02-15 22:27:15 [__init__.py:216] Automatically detected platform rocm.
INFO 02-15 22:27:15 [__init__.py:216] Automatically detected platform rocm.
INFO 02-15 22:27:15 [__init__.py:216] Automatically detected platform rocm.
INFO 02-15 22:27:15 [__init__.py:216] Automatically detected platform rocm.
INFO 02-15 22:27:15 [__init__.py:216] Automatically detected platform rocm.
INFO 02-15 22:27:15 [__init__.py:216] Automatically detected platform rocm.
INFO 02-15 22:27:15 [__init__.py:216] Automatically detected platform rocm.
INFO 02-15 22:27:15 [__init__.py:216] Automatically detected platform rocm.
[aiter] merge tuned file under model_configs/ and configs/ /usr/local/lib/python3.12/dist-packages/aiter/configs/a8w8_bpreshuffle_tuned_gemm.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_bpreshuffle_tuned_gemm_dsv3.csv
[aiter] merge tuned file under model_configs/ and configs/ /usr/local/lib/python3.12/dist-packages/aiter/configs/a8w8_blockscale_tuned_gemm.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_ds_v3.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_qwen3_235b.csv
[aiter] merge tuned file under model_configs/ and configs/ /usr/local/lib/python3.12/dist-packages/aiter/configs/tuned_fmoe.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[aiter] merge tuned file under model_configs/ and configs/ /usr/local/lib/python3.12/dist-packages/aiter/configs/a8w8_blockscale_bpreshuffle_tuned_gemm.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_blockscale_bpreshuffle_tuned_gemm_dsv3.csv
[aiter] merge tuned file under model_configs/ and configs/ /usr/local/lib/python3.12/dist-packages/aiter/configs/a8w8_bpreshuffle_tuned_gemm.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_bpreshuffle_tuned_gemm_dsv3.csv
[aiter] merge tuned file under model_configs/ and configs/ /usr/local/lib/python3.12/dist-packages/aiter/configs/a8w8_bpreshuffle_tuned_gemm.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_bpreshuffle_tuned_gemm_dsv3.csv
[aiter] merge tuned file under model_configs/ and configs/ /usr/local/lib/python3.12/dist-packages/aiter/configs/a8w8_bpreshuffle_tuned_gemm.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_bpreshuffle_tuned_gemm_dsv3.csv
[aiter] merge tuned file under model_configs/ and configs/ /usr/local/lib/python3.12/dist-packages/aiter/configs/a8w8_bpreshuffle_tuned_gemm.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_bpreshuffle_tuned_gemm_dsv3.csv
[aiter] merge tuned file under model_configs/ and configs/ /usr/local/lib/python3.12/dist-packages/aiter/configs/a8w8_bpreshuffle_tuned_gemm.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_bpreshuffle_tuned_gemm_dsv3.csv
[aiter] merge tuned file under model_configs/ and configs/ /usr/local/lib/python3.12/dist-packages/aiter/configs/a8w8_bpreshuffle_tuned_gemm.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_bpreshuffle_tuned_gemm_dsv3.csv
[aiter] merge tuned file under model_configs/ and configs/ /usr/local/lib/python3.12/dist-packages/aiter/configs/a8w8_blockscale_tuned_gemm.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_ds_v3.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_qwen3_235b.csv
[aiter] merge tuned file under model_configs/ and configs/ /usr/local/lib/python3.12/dist-packages/aiter/configs/tuned_fmoe.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[aiter] merge tuned file under model_configs/ and configs/ /usr/local/lib/python3.12/dist-packages/aiter/configs/a8w8_blockscale_tuned_gemm.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_ds_v3.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_qwen3_235b.csv
[aiter] merge tuned file under model_configs/ and configs/ /usr/local/lib/python3.12/dist-packages/aiter/configs/a8w8_blockscale_bpreshuffle_tuned_gemm.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_blockscale_bpreshuffle_tuned_gemm_dsv3.csv
[aiter] merge tuned file under model_configs/ and configs/ /usr/local/lib/python3.12/dist-packages/aiter/configs/tuned_fmoe.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[aiter] merge tuned file under model_configs/ and configs/ /usr/local/lib/python3.12/dist-packages/aiter/configs/a8w8_blockscale_bpreshuffle_tuned_gemm.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_blockscale_bpreshuffle_tuned_gemm_dsv3.csv
[aiter] merge tuned file under model_configs/ and configs/ /usr/local/lib/python3.12/dist-packages/aiter/configs/a8w8_blockscale_tuned_gemm.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_ds_v3.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_qwen3_235b.csv
[aiter] merge tuned file under model_configs/ and configs/ /usr/local/lib/python3.12/dist-packages/aiter/configs/tuned_fmoe.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[aiter] merge tuned file under model_configs/ and configs/ /usr/local/lib/python3.12/dist-packages/aiter/configs/a8w8_blockscale_tuned_gemm.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_ds_v3.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_qwen3_235b.csv
[aiter] merge tuned file under model_configs/ and configs/ /usr/local/lib/python3.12/dist-packages/aiter/configs/a8w8_blockscale_bpreshuffle_tuned_gemm.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_blockscale_bpreshuffle_tuned_gemm_dsv3.csv
[aiter] merge tuned file under model_configs/ and configs/ /usr/local/lib/python3.12/dist-packages/aiter/configs/tuned_fmoe.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[aiter] merge tuned file under model_configs/ and configs/ /usr/local/lib/python3.12/dist-packages/aiter/configs/a8w8_blockscale_tuned_gemm.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_ds_v3.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_qwen3_235b.csv
[aiter] merge tuned file under model_configs/ and configs/ /usr/local/lib/python3.12/dist-packages/aiter/configs/a8w8_blockscale_bpreshuffle_tuned_gemm.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_blockscale_bpreshuffle_tuned_gemm_dsv3.csv
[aiter] merge tuned file under model_configs/ and configs/ /usr/local/lib/python3.12/dist-packages/aiter/configs/tuned_fmoe.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[aiter] merge tuned file under model_configs/ and configs/ /usr/local/lib/python3.12/dist-packages/aiter/configs/a8w8_blockscale_bpreshuffle_tuned_gemm.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_blockscale_bpreshuffle_tuned_gemm_dsv3.csv
[aiter] merge tuned file under model_configs/ and configs/ /usr/local/lib/python3.12/dist-packages/aiter/configs/a8w8_blockscale_tuned_gemm.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_ds_v3.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_qwen3_235b.csv
[aiter] merge tuned file under model_configs/ and configs/ /usr/local/lib/python3.12/dist-packages/aiter/configs/tuned_fmoe.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[aiter] merge tuned file under model_configs/ and configs/ /usr/local/lib/python3.12/dist-packages/aiter/configs/a8w8_blockscale_bpreshuffle_tuned_gemm.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_blockscale_bpreshuffle_tuned_gemm_dsv3.csv
[aiter] merge tuned file under model_configs/ and configs/ /usr/local/lib/python3.12/dist-packages/aiter/configs/a8w8_bpreshuffle_tuned_gemm.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_bpreshuffle_tuned_gemm_dsv3.csv
[aiter] import [module_aiter_enum] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_aiter_enum.so
[aiter] merge tuned file under model_configs/ and configs/ /usr/local/lib/python3.12/dist-packages/aiter/configs/a8w8_blockscale_tuned_gemm.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_ds_v3.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_qwen3_235b.csv
[aiter] merge tuned file under model_configs/ and configs/ /usr/local/lib/python3.12/dist-packages/aiter/configs/tuned_fmoe.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[aiter] merge tuned file under model_configs/ and configs/ /usr/local/lib/python3.12/dist-packages/aiter/configs/a8w8_blockscale_bpreshuffle_tuned_gemm.csv:/usr/local/lib/python3.12/dist-packages/aiter/configs/model_configs/a8w8_blockscale_bpreshuffle_tuned_gemm_dsv3.csv
[aiter] import [module_aiter_enum] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_aiter_enum.so
[aiter] import [module_aiter_enum] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_aiter_enum.so
[aiter] import [module_aiter_enum] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_aiter_enum.so
[aiter] import [module_aiter_enum] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_aiter_enum.so
[aiter] import [module_aiter_enum] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_aiter_enum.so
[aiter] import [module_aiter_enum] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_aiter_enum.so
[aiter] import [module_aiter_enum] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_aiter_enum.so
INFO 02-15 22:27:18 [layer.py:42] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_ROPE_ZEROS_KV_CACHE=True
INFO 02-15 22:27:19 [llama.py:73] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_ROPE_ZEROS_KV_CACHE=False VLLM_ROCM_USE_AITER_MHA=True
INFO 02-15 22:27:19 [gpu_worker.py:74] Profiling enabled. Traces will be saved to: /app
INFO 02-15 22:27:19 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_b2e86659'), local_subscribe_addr='ipc:///tmp/e97e0b59-322c-4f69-aa1e-53332b283317', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 02-15 22:27:19 [layer.py:42] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_ROPE_ZEROS_KV_CACHE=True
INFO 02-15 22:27:19 [llama.py:73] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_ROPE_ZEROS_KV_CACHE=False VLLM_ROCM_USE_AITER_MHA=True
INFO 02-15 22:27:19 [layer.py:42] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_ROPE_ZEROS_KV_CACHE=True
INFO 02-15 22:27:19 [layer.py:42] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_ROPE_ZEROS_KV_CACHE=True
INFO 02-15 22:27:19 [gpu_worker.py:74] Profiling enabled. Traces will be saved to: /app
INFO 02-15 22:27:19 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_53a43a73'), local_subscribe_addr='ipc:///tmp/c9ed9c5d-91ed-475b-a57a-b19b1ddff3bf', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 02-15 22:27:19 [llama.py:73] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_ROPE_ZEROS_KV_CACHE=False VLLM_ROCM_USE_AITER_MHA=True
INFO 02-15 22:27:19 [llama.py:73] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_ROPE_ZEROS_KV_CACHE=False VLLM_ROCM_USE_AITER_MHA=True
INFO 02-15 22:27:19 [layer.py:42] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_ROPE_ZEROS_KV_CACHE=True
INFO 02-15 22:27:19 [llama.py:73] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_ROPE_ZEROS_KV_CACHE=False VLLM_ROCM_USE_AITER_MHA=True
INFO 02-15 22:27:19 [gpu_worker.py:74] Profiling enabled. Traces will be saved to: /app
INFO 02-15 22:27:19 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_442d8304'), local_subscribe_addr='ipc:///tmp/431188b7-74da-411b-a903-11e55b9df5bb', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 02-15 22:27:19 [gpu_worker.py:74] Profiling enabled. Traces will be saved to: /app
INFO 02-15 22:27:19 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_dbcfc12c'), local_subscribe_addr='ipc:///tmp/42479ea7-c60e-41d8-8619-cba93cd449d8', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 02-15 22:27:19 [gpu_worker.py:74] Profiling enabled. Traces will be saved to: /app
INFO 02-15 22:27:19 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_e0e429d0'), local_subscribe_addr='ipc:///tmp/1bed02bf-fc08-425f-8105-9b15d076026e', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 02-15 22:27:19 [layer.py:42] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_ROPE_ZEROS_KV_CACHE=True
INFO 02-15 22:27:19 [llama.py:73] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_ROPE_ZEROS_KV_CACHE=False VLLM_ROCM_USE_AITER_MHA=True
INFO 02-15 22:27:19 [layer.py:42] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_ROPE_ZEROS_KV_CACHE=True
INFO 02-15 22:27:19 [llama.py:73] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_ROPE_ZEROS_KV_CACHE=False VLLM_ROCM_USE_AITER_MHA=True
INFO 02-15 22:27:19 [gpu_worker.py:74] Profiling enabled. Traces will be saved to: /app
INFO 02-15 22:27:19 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_fadfa9ef'), local_subscribe_addr='ipc:///tmp/cc8d3da9-3127-42b5-a099-38dd644870e8', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 02-15 22:27:19 [layer.py:42] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_ROPE_ZEROS_KV_CACHE=True
INFO 02-15 22:27:19 [llama.py:73] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_ROPE_ZEROS_KV_CACHE=False VLLM_ROCM_USE_AITER_MHA=True
INFO 02-15 22:27:19 [gpu_worker.py:74] Profiling enabled. Traces will be saved to: /app
INFO 02-15 22:27:19 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_6532f5e5'), local_subscribe_addr='ipc:///tmp/8e4bff22-c598-434d-8da4-d8f2faf9a382', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 02-15 22:27:19 [gpu_worker.py:74] Profiling enabled. Traces will be saved to: /app
INFO 02-15 22:27:19 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_98ef2337'), local_subscribe_addr='ipc:///tmp/594ba7fd-29c7-4ef9-abb8-f63e81e9ff7a', remote_subscribe_addr=None, remote_addr_ipv6=False)
[W215 22:27:20.682435782 ProcessGroupNCCL.cpp:915] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[W215 22:27:20.706077167 ProcessGroupNCCL.cpp:915] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[W215 22:27:20.768285298 ProcessGroupNCCL.cpp:915] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[W215 22:27:20.819762806 ProcessGroupNCCL.cpp:915] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[W215 22:27:20.273821106 ProcessGroupNCCL.cpp:915] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[W215 22:27:20.309921249 ProcessGroupNCCL.cpp:915] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[W215 22:27:21.442486294 ProcessGroupNCCL.cpp:915] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[W215 22:27:21.449899076 ProcessGroupNCCL.cpp:915] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
INFO 02-15 22:27:21 [__init__.py:1382] Found nccl from library librccl.so.1
INFO 02-15 22:27:21 [__init__.py:1382] Found nccl from library librccl.so.1
INFO 02-15 22:27:21 [__init__.py:1382] Found nccl from library librccl.so.1
INFO 02-15 22:27:21 [__init__.py:1382] Found nccl from library librccl.so.1
INFO 02-15 22:27:21 [pynccl.py:70] vLLM is using nccl==2.27.7
INFO 02-15 22:27:21 [pynccl.py:70] vLLM is using nccl==2.27.7
INFO 02-15 22:27:21 [__init__.py:1382] Found nccl from library librccl.so.1
INFO 02-15 22:27:21 [pynccl.py:70] vLLM is using nccl==2.27.7
INFO 02-15 22:27:21 [pynccl.py:70] vLLM is using nccl==2.27.7
INFO 02-15 22:27:21 [pynccl.py:70] vLLM is using nccl==2.27.7
INFO 02-15 22:27:21 [__init__.py:1382] Found nccl from library librccl.so.1
INFO 02-15 22:27:21 [__init__.py:1382] Found nccl from library librccl.so.1
INFO 02-15 22:27:21 [__init__.py:1382] Found nccl from library librccl.so.1
INFO 02-15 22:27:21 [pynccl.py:70] vLLM is using nccl==2.27.7
INFO 02-15 22:27:21 [pynccl.py:70] vLLM is using nccl==2.27.7
INFO 02-15 22:27:21 [pynccl.py:70] vLLM is using nccl==2.27.7
INFO 02-15 22:27:31 [cuda_communicator.py:213] Initializing ROCm allreduce dispatcher.
INFO 02-15 22:27:31 [cuda_communicator.py:213] Initializing ROCm allreduce dispatcher.
INFO 02-15 22:27:31 [cuda_communicator.py:213] Initializing ROCm allreduce dispatcher.
INFO 02-15 22:27:31 [cuda_communicator.py:213] Initializing ROCm allreduce dispatcher.
INFO 02-15 22:27:31 [cuda_communicator.py:213] Initializing ROCm allreduce dispatcher.
INFO 02-15 22:27:31 [cuda_communicator.py:213] Initializing ROCm allreduce dispatcher.
INFO 02-15 22:27:31 [cuda_communicator.py:213] Initializing ROCm allreduce dispatcher.
INFO 02-15 22:27:31 [cuda_communicator.py:213] Initializing ROCm allreduce dispatcher.
INFO 02-15 22:27:31 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer_handle=(7, 4194304, 6, 'psm_0d3381c0'), local_subscribe_addr='ipc:///tmp/594162d5-7404-49c6-9d34-753fe27c6584', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
INFO 02-15 22:27:31 [parallel_state.py:1208] rank 7 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 7, EP rank 7
INFO 02-15 22:27:31 [parallel_state.py:1208] rank 2 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2
INFO 02-15 22:27:31 [parallel_state.py:1208] rank 5 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 5, EP rank 5
INFO 02-15 22:27:31 [parallel_state.py:1208] rank 0 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
INFO 02-15 22:27:31 [parallel_state.py:1208] rank 4 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 4, EP rank 4
INFO 02-15 22:27:31 [parallel_state.py:1208] rank 1 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
INFO 02-15 22:27:31 [parallel_state.py:1208] rank 6 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 6, EP rank 6
INFO 02-15 22:27:31 [parallel_state.py:1208] rank 3 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3
(Worker_TP2 pid=455) INFO 02-15 22:27:35 [gpu_model_runner.py:2529] Starting to load model amd/DeepSeek-R1-MXFP4...
(Worker_TP3 pid=456) INFO 02-15 22:27:35 [gpu_model_runner.py:2529] Starting to load model amd/DeepSeek-R1-MXFP4...
INFO 02-15 22:27:35 [mla.py:146] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_ROPE_ZEROS_KV_CACHE=True VLLM_ROCM_USE_AITER_MLA=False
INFO 02-15 22:27:35 [mla.py:147] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_RMSNORM_FP8_QUANT=True
INFO 02-15 22:27:35 [mla.py:148] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_RMSNORM_FP4_QUANT=True
INFO 02-15 22:27:35 [deepseek_v2.py:263] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_ROPE_ZEROS_KV_CACHE=False VLLM_ROCM_USE_AITER_MLA=False
INFO 02-15 22:27:35 [deepseek_v2.py:264] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_RMSNORM_FP8_QUANT=True
INFO 02-15 22:27:35 [deepseek_v2.py:265] [Aiter] VLLM_ROCM_USE_AITER_TRITON_SILU_MUL_FP8_QUANT=False
INFO 02-15 22:27:35 [deepseek_v2.py:266] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_RMSNORM_FP4_QUANT=True
INFO 02-15 22:27:35 [deepseek_v2.py:267] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_MUL_ADD=True
(Worker_TP0 pid=453) INFO 02-15 22:27:35 [gpu_model_runner.py:2529] Starting to load model amd/DeepSeek-R1-MXFP4...
(Worker_TP1 pid=454) INFO 02-15 22:27:35 [gpu_model_runner.py:2529] Starting to load model amd/DeepSeek-R1-MXFP4...
(Worker_TP0 pid=453) INFO 02-15 22:27:35 [gpu_model_runner.py:2561] Loading model from scratch...
(Worker_TP2 pid=455) INFO 02-15 22:27:35 [gpu_model_runner.py:2561] Loading model from scratch...
(Worker_TP2 pid=455) INFO 02-15 22:27:35 [mla.py:146] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_ROPE_ZEROS_KV_CACHE=True VLLM_ROCM_USE_AITER_MLA=False
(Worker_TP2 pid=455) INFO 02-15 22:27:35 [mla.py:147] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_RMSNORM_FP8_QUANT=True
(Worker_TP2 pid=455) INFO 02-15 22:27:35 [mla.py:148] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_RMSNORM_FP4_QUANT=True
(Worker_TP2 pid=455) INFO 02-15 22:27:35 [deepseek_v2.py:263] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_ROPE_ZEROS_KV_CACHE=False VLLM_ROCM_USE_AITER_MLA=False
(Worker_TP2 pid=455) INFO 02-15 22:27:35 [deepseek_v2.py:264] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_RMSNORM_FP8_QUANT=True
(Worker_TP2 pid=455) INFO 02-15 22:27:35 [deepseek_v2.py:265] [Aiter] VLLM_ROCM_USE_AITER_TRITON_SILU_MUL_FP8_QUANT=False
(Worker_TP2 pid=455) INFO 02-15 22:27:35 [deepseek_v2.py:266] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_RMSNORM_FP4_QUANT=True
(Worker_TP2 pid=455) INFO 02-15 22:27:35 [deepseek_v2.py:267] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_MUL_ADD=True
(Worker_TP3 pid=456) INFO 02-15 22:27:35 [gpu_model_runner.py:2561] Loading model from scratch...
(Worker_TP3 pid=456) INFO 02-15 22:27:35 [mla.py:146] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_ROPE_ZEROS_KV_CACHE=True VLLM_ROCM_USE_AITER_MLA=False
(Worker_TP3 pid=456) INFO 02-15 22:27:35 [mla.py:147] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_RMSNORM_FP8_QUANT=True
(Worker_TP3 pid=456) INFO 02-15 22:27:35 [mla.py:148] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_RMSNORM_FP4_QUANT=True
(Worker_TP3 pid=456) INFO 02-15 22:27:35 [deepseek_v2.py:263] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_ROPE_ZEROS_KV_CACHE=False VLLM_ROCM_USE_AITER_MLA=False
(Worker_TP3 pid=456) INFO 02-15 22:27:35 [deepseek_v2.py:264] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_RMSNORM_FP8_QUANT=True
(Worker_TP3 pid=456) INFO 02-15 22:27:35 [deepseek_v2.py:265] [Aiter] VLLM_ROCM_USE_AITER_TRITON_SILU_MUL_FP8_QUANT=False
(Worker_TP3 pid=456) INFO 02-15 22:27:35 [deepseek_v2.py:266] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_RMSNORM_FP4_QUANT=True
(Worker_TP3 pid=456) INFO 02-15 22:27:35 [deepseek_v2.py:267] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_MUL_ADD=True
(Worker_TP1 pid=454) INFO 02-15 22:27:36 [gpu_model_runner.py:2561] Loading model from scratch...
(Worker_TP1 pid=454) INFO 02-15 22:27:36 [mla.py:146] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_ROPE_ZEROS_KV_CACHE=True VLLM_ROCM_USE_AITER_MLA=False
(Worker_TP1 pid=454) INFO 02-15 22:27:36 [mla.py:147] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_RMSNORM_FP8_QUANT=True
(Worker_TP1 pid=454) INFO 02-15 22:27:36 [mla.py:148] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_RMSNORM_FP4_QUANT=True
(Worker_TP1 pid=454) INFO 02-15 22:27:36 [deepseek_v2.py:263] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_ROPE_ZEROS_KV_CACHE=False VLLM_ROCM_USE_AITER_MLA=False
(Worker_TP1 pid=454) INFO 02-15 22:27:36 [deepseek_v2.py:264] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_RMSNORM_FP8_QUANT=True
(Worker_TP1 pid=454) INFO 02-15 22:27:36 [deepseek_v2.py:265] [Aiter] VLLM_ROCM_USE_AITER_TRITON_SILU_MUL_FP8_QUANT=False
(Worker_TP1 pid=454) INFO 02-15 22:27:36 [deepseek_v2.py:266] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_RMSNORM_FP4_QUANT=True
(Worker_TP1 pid=454) INFO 02-15 22:27:36 [deepseek_v2.py:267] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_MUL_ADD=True
(Worker_TP0 pid=453) >>> MY quant_method in EMBED is NoneType
(Worker_TP0 pid=453) INFO 02-15 22:27:36 [common.py:253] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FP8_BMM=False VLLM_ROCM_USE_AITER_TRITON_FP8_BMM_MAX_BATCH_SIZE=256
(Worker_TP0 pid=453) INFO 02-15 22:27:36 [common.py:254] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_ROPE_ZEROS_KV_CACHE=True
(Worker_TP0 pid=453) INFO 02-15 22:27:36 [rocm.py:222] Using AITER MLA backend on V1 engine.
(Worker_TP0 pid=453) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP0 pid=453) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP0 pid=453) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP0 pid=453) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP0 pid=453) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP0 pid=453) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP0 pid=453) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP0 pid=453) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP0 pid=453) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP0 pid=453) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP3 pid=456) >>> MY quant_method in EMBED is NoneType
(Worker_TP3 pid=456) INFO 02-15 22:27:36 [common.py:253] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FP8_BMM=False VLLM_ROCM_USE_AITER_TRITON_FP8_BMM_MAX_BATCH_SIZE=256
(Worker_TP3 pid=456) INFO 02-15 22:27:36 [common.py:254] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_ROPE_ZEROS_KV_CACHE=True
(Worker_TP0 pid=453) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP3 pid=456) INFO 02-15 22:27:36 [rocm.py:222] Using AITER MLA backend on V1 engine.
(Worker_TP0 pid=453) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP3 pid=456) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP0 pid=453) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP3 pid=456) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP0 pid=453) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP3 pid=456) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP0 pid=453) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP3 pid=456) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP0 pid=453) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP3 pid=456) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP0 pid=453) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP3 pid=456) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP0 pid=453) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP3 pid=456) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP0 pid=453) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP3 pid=456) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP0 pid=453) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP3 pid=456) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP0 pid=453) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP3 pid=456) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP0 pid=453) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP3 pid=456) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP0 pid=453) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP3 pid=456) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP0 pid=453) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP3 pid=456) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP0 pid=453) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP3 pid=456) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP0 pid=453) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP3 pid=456) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP0 pid=453) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP3 pid=456) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP0 pid=453) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP3 pid=456) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP0 pid=453) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP3 pid=456) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP0 pid=453) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP3 pid=456) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP0 pid=453) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP3 pid=456) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP0 pid=453) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP3 pid=456) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP0 pid=453) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP3 pid=456) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP0 pid=453) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP3 pid=456) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP0 pid=453) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP3 pid=456) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP0 pid=453) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP3 pid=456) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP0 pid=453) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP3 pid=456) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP0 pid=453) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP3 pid=456) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP0 pid=453) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP3 pid=456) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP0 pid=453) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP3 pid=456) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP0 pid=453) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP3 pid=456) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP0 pid=453) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP3 pid=456) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP0 pid=453) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP3 pid=456) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP0 pid=453) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP3 pid=456) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP0 pid=453) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP3 pid=456) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP0 pid=453) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP3 pid=456) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP0 pid=453) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP3 pid=456) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP0 pid=453) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP3 pid=456) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP0 pid=453) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP3 pid=456) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP0 pid=453) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP3 pid=456) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP0 pid=453) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP3 pid=456) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP0 pid=453) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP3 pid=456) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP0 pid=453) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP3 pid=456) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP0 pid=453) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP3 pid=456) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP0 pid=453) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP3 pid=456) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP0 pid=453) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP3 pid=456) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP0 pid=453) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP3 pid=456) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP0 pid=453) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP3 pid=456) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP3 pid=456) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP3 pid=456) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP2 pid=455) >>> MY quant_method in EMBED is NoneType
(Worker_TP2 pid=455) INFO 02-15 22:27:36 [common.py:253] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FP8_BMM=False VLLM_ROCM_USE_AITER_TRITON_FP8_BMM_MAX_BATCH_SIZE=256
(Worker_TP2 pid=455) INFO 02-15 22:27:36 [common.py:254] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_ROPE_ZEROS_KV_CACHE=True
(Worker_TP0 pid=453) >>> MY quant_method in EMBED is UnquantizedLinearMethod
(Worker_TP0 pid=453) WARNING 02-15 22:27:36 [compilation.py:683] Op 'quant_fp8' not present in model, enabling with '+quant_fp8' has no effect
(Worker_TP3 pid=456) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP2 pid=455) INFO 02-15 22:27:36 [rocm.py:222] Using AITER MLA backend on V1 engine.
(Worker_TP3 pid=456) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP2 pid=455) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP3 pid=456) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP2 pid=455) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP3 pid=456) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP2 pid=455) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP3 pid=456) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP3 pid=456) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP2 pid=455) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP2 pid=455) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP3 pid=456) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP2 pid=455) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP3 pid=456) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP2 pid=455) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP3 pid=456) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP2 pid=455) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP2 pid=455) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP2 pid=455) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP3 pid=456) >>> MY quant_method in EMBED is UnquantizedLinearMethod
(Worker_TP3 pid=456) WARNING 02-15 22:27:36 [compilation.py:683] Op 'quant_fp8' not present in model, enabling with '+quant_fp8' has no effect
(Worker_TP2 pid=455) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP2 pid=455) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP2 pid=455) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP2 pid=455) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP7 pid=460) INFO 02-15 22:27:36 [gpu_model_runner.py:2529] Starting to load model amd/DeepSeek-R1-MXFP4...
(Worker_TP2 pid=455) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP0 pid=453) INFO 02-15 22:27:36 [weight_utils.py:392] Using model weights format ['*.safetensors']
(Worker_TP2 pid=455) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP2 pid=455) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP2 pid=455) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP2 pid=455) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP2 pid=455) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP2 pid=455) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP2 pid=455) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP2 pid=455) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP2 pid=455) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP2 pid=455) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP3 pid=456) INFO 02-15 22:27:36 [weight_utils.py:392] Using model weights format ['*.safetensors']
(Worker_TP2 pid=455) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP2 pid=455) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP2 pid=455) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP2 pid=455) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP2 pid=455) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP2 pid=455) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP2 pid=455) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP2 pid=455) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP2 pid=455) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP2 pid=455) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP2 pid=455) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP2 pid=455) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
Loading safetensors checkpoint shards:   0% Completed | 0/73 [00:00<?, ?it/s]
(Worker_TP2 pid=455) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP2 pid=455) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP2 pid=455) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP2 pid=455) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP2 pid=455) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP2 pid=455) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP1 pid=454) >>> MY quant_method in EMBED is NoneType
(Worker_TP1 pid=454) INFO 02-15 22:27:36 [common.py:253] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FP8_BMM=False VLLM_ROCM_USE_AITER_TRITON_FP8_BMM_MAX_BATCH_SIZE=256
(Worker_TP1 pid=454) INFO 02-15 22:27:36 [common.py:254] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_ROPE_ZEROS_KV_CACHE=True
(Worker_TP2 pid=455) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP1 pid=454) INFO 02-15 22:27:36 [rocm.py:222] Using AITER MLA backend on V1 engine.
(Worker_TP2 pid=455) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP1 pid=454) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP2 pid=455) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP1 pid=454) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP2 pid=455) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP1 pid=454) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP2 pid=455) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP1 pid=454) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP2 pid=455) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP1 pid=454) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP2 pid=455) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP1 pid=454) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP2 pid=455) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP1 pid=454) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP2 pid=455) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP1 pid=454) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP2 pid=455) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP1 pid=454) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP2 pid=455) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP1 pid=454) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP2 pid=455) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP1 pid=454) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP2 pid=455) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP1 pid=454) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP2 pid=455) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP1 pid=454) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP2 pid=455) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP1 pid=454) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP1 pid=454) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP1 pid=454) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP1 pid=454) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP1 pid=454) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP2 pid=455) >>> MY quant_method in EMBED is UnquantizedLinearMethod
(Worker_TP2 pid=455) WARNING 02-15 22:27:36 [compilation.py:683] Op 'quant_fp8' not present in model, enabling with '+quant_fp8' has no effect
(Worker_TP1 pid=454) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP1 pid=454) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP1 pid=454) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP1 pid=454) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP1 pid=454) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP1 pid=454) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP1 pid=454) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP1 pid=454) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP1 pid=454) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP1 pid=454) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP1 pid=454) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP1 pid=454) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP1 pid=454) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP1 pid=454) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP7 pid=460) INFO 02-15 22:27:36 [gpu_model_runner.py:2561] Loading model from scratch...
(Worker_TP1 pid=454) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP7 pid=460) INFO 02-15 22:27:36 [mla.py:146] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_ROPE_ZEROS_KV_CACHE=True VLLM_ROCM_USE_AITER_MLA=False
(Worker_TP7 pid=460) INFO 02-15 22:27:36 [mla.py:147] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_RMSNORM_FP8_QUANT=True
(Worker_TP7 pid=460) INFO 02-15 22:27:36 [mla.py:148] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_RMSNORM_FP4_QUANT=True
(Worker_TP1 pid=454) INFO 02-15 22:27:36 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP1 pid=454) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP7 pid=460) INFO 02-15 22:27:37 [deepseek_v2.py:263] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_ROPE_ZEROS_KV_CACHE=False VLLM_ROCM_USE_AITER_MLA=False
(Worker_TP7 pid=460) INFO 02-15 22:27:37 [deepseek_v2.py:264] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_RMSNORM_FP8_QUANT=True
(Worker_TP7 pid=460) INFO 02-15 22:27:37 [deepseek_v2.py:265] [Aiter] VLLM_ROCM_USE_AITER_TRITON_SILU_MUL_FP8_QUANT=False
(Worker_TP7 pid=460) INFO 02-15 22:27:37 [deepseek_v2.py:266] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_RMSNORM_FP4_QUANT=True
(Worker_TP7 pid=460) INFO 02-15 22:27:37 [deepseek_v2.py:267] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_MUL_ADD=True
(Worker_TP1 pid=454) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP1 pid=454) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP1 pid=454) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP1 pid=454) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP2 pid=455) INFO 02-15 22:27:37 [weight_utils.py:392] Using model weights format ['*.safetensors']
(Worker_TP1 pid=454) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP4 pid=457) INFO 02-15 22:27:37 [gpu_model_runner.py:2529] Starting to load model amd/DeepSeek-R1-MXFP4...
(Worker_TP1 pid=454) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP1 pid=454) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP1 pid=454) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP6 pid=459) INFO 02-15 22:27:37 [gpu_model_runner.py:2529] Starting to load model amd/DeepSeek-R1-MXFP4...
(Worker_TP5 pid=458) INFO 02-15 22:27:37 [gpu_model_runner.py:2529] Starting to load model amd/DeepSeek-R1-MXFP4...
(Worker_TP1 pid=454) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP1 pid=454) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP1 pid=454) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP1 pid=454) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP1 pid=454) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP1 pid=454) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP1 pid=454) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP1 pid=454) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP1 pid=454) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP1 pid=454) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP1 pid=454) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP1 pid=454) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP1 pid=454) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP1 pid=454) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP1 pid=454) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
Loading safetensors checkpoint shards:   1% Completed | 1/73 [00:00<00:23,  3.08it/s]
(Worker_TP1 pid=454) >>> MY quant_method in EMBED is UnquantizedLinearMethod
(Worker_TP1 pid=454) WARNING 02-15 22:27:37 [compilation.py:683] Op 'quant_fp8' not present in model, enabling with '+quant_fp8' has no effect
(Worker_TP1 pid=454) INFO 02-15 22:27:37 [weight_utils.py:392] Using model weights format ['*.safetensors']
(Worker_TP5 pid=458) INFO 02-15 22:27:37 [gpu_model_runner.py:2561] Loading model from scratch...
(Worker_TP7 pid=460) >>> MY quant_method in EMBED is NoneType
(Worker_TP7 pid=460) INFO 02-15 22:27:37 [common.py:253] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FP8_BMM=False VLLM_ROCM_USE_AITER_TRITON_FP8_BMM_MAX_BATCH_SIZE=256
(Worker_TP7 pid=460) INFO 02-15 22:27:37 [common.py:254] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_ROPE_ZEROS_KV_CACHE=True
(Worker_TP5 pid=458) INFO 02-15 22:27:37 [mla.py:146] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_ROPE_ZEROS_KV_CACHE=True VLLM_ROCM_USE_AITER_MLA=False
(Worker_TP5 pid=458) INFO 02-15 22:27:37 [mla.py:147] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_RMSNORM_FP8_QUANT=True
(Worker_TP5 pid=458) INFO 02-15 22:27:37 [mla.py:148] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_RMSNORM_FP4_QUANT=True
(Worker_TP7 pid=460) INFO 02-15 22:27:37 [rocm.py:222] Using AITER MLA backend on V1 engine.
(Worker_TP5 pid=458) INFO 02-15 22:27:37 [deepseek_v2.py:263] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_ROPE_ZEROS_KV_CACHE=False VLLM_ROCM_USE_AITER_MLA=False
(Worker_TP5 pid=458) INFO 02-15 22:27:37 [deepseek_v2.py:264] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_RMSNORM_FP8_QUANT=True
(Worker_TP5 pid=458) INFO 02-15 22:27:37 [deepseek_v2.py:265] [Aiter] VLLM_ROCM_USE_AITER_TRITON_SILU_MUL_FP8_QUANT=False
(Worker_TP5 pid=458) INFO 02-15 22:27:37 [deepseek_v2.py:266] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_RMSNORM_FP4_QUANT=True
(Worker_TP5 pid=458) INFO 02-15 22:27:37 [deepseek_v2.py:267] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_MUL_ADD=True
(Worker_TP7 pid=460) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP7 pid=460) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP7 pid=460) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP7 pid=460) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP7 pid=460) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP7 pid=460) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP7 pid=460) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP7 pid=460) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP7 pid=460) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP7 pid=460) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP7 pid=460) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP7 pid=460) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP7 pid=460) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP7 pid=460) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP7 pid=460) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP7 pid=460) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP7 pid=460) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP7 pid=460) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP7 pid=460) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP7 pid=460) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP7 pid=460) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP7 pid=460) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP7 pid=460) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP7 pid=460) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP7 pid=460) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP7 pid=460) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP4 pid=457) INFO 02-15 22:27:37 [gpu_model_runner.py:2561] Loading model from scratch...
(Worker_TP7 pid=460) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP4 pid=457) INFO 02-15 22:27:37 [mla.py:146] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_ROPE_ZEROS_KV_CACHE=True VLLM_ROCM_USE_AITER_MLA=False
(Worker_TP4 pid=457) INFO 02-15 22:27:37 [mla.py:147] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_RMSNORM_FP8_QUANT=True
(Worker_TP4 pid=457) INFO 02-15 22:27:37 [mla.py:148] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_RMSNORM_FP4_QUANT=True
(Worker_TP4 pid=457) INFO 02-15 22:27:37 [deepseek_v2.py:263] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_ROPE_ZEROS_KV_CACHE=False VLLM_ROCM_USE_AITER_MLA=False
(Worker_TP4 pid=457) INFO 02-15 22:27:37 [deepseek_v2.py:264] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_RMSNORM_FP8_QUANT=True
(Worker_TP4 pid=457) INFO 02-15 22:27:37 [deepseek_v2.py:265] [Aiter] VLLM_ROCM_USE_AITER_TRITON_SILU_MUL_FP8_QUANT=False
(Worker_TP4 pid=457) INFO 02-15 22:27:37 [deepseek_v2.py:266] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_RMSNORM_FP4_QUANT=True
(Worker_TP4 pid=457) INFO 02-15 22:27:37 [deepseek_v2.py:267] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_MUL_ADD=True
(Worker_TP6 pid=459) INFO 02-15 22:27:37 [gpu_model_runner.py:2561] Loading model from scratch...
(Worker_TP7 pid=460) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP6 pid=459) INFO 02-15 22:27:37 [mla.py:146] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_ROPE_ZEROS_KV_CACHE=True VLLM_ROCM_USE_AITER_MLA=False
(Worker_TP6 pid=459) INFO 02-15 22:27:37 [mla.py:147] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_RMSNORM_FP8_QUANT=True
(Worker_TP6 pid=459) INFO 02-15 22:27:37 [mla.py:148] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_RMSNORM_FP4_QUANT=True
(Worker_TP7 pid=460) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP6 pid=459) INFO 02-15 22:27:37 [deepseek_v2.py:263] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_ROPE_ZEROS_KV_CACHE=False VLLM_ROCM_USE_AITER_MLA=False
(Worker_TP6 pid=459) INFO 02-15 22:27:37 [deepseek_v2.py:264] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_RMSNORM_FP8_QUANT=True
(Worker_TP6 pid=459) INFO 02-15 22:27:37 [deepseek_v2.py:265] [Aiter] VLLM_ROCM_USE_AITER_TRITON_SILU_MUL_FP8_QUANT=False
(Worker_TP6 pid=459) INFO 02-15 22:27:37 [deepseek_v2.py:266] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_RMSNORM_FP4_QUANT=True
(Worker_TP6 pid=459) INFO 02-15 22:27:37 [deepseek_v2.py:267] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_MUL_ADD=True
(Worker_TP7 pid=460) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP7 pid=460) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP7 pid=460) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP7 pid=460) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP7 pid=460) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP7 pid=460) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP7 pid=460) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP7 pid=460) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP7 pid=460) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP7 pid=460) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP7 pid=460) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP7 pid=460) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP7 pid=460) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP7 pid=460) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP7 pid=460) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP7 pid=460) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP7 pid=460) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP7 pid=460) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP7 pid=460) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP7 pid=460) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP7 pid=460) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP7 pid=460) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP7 pid=460) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP7 pid=460) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP7 pid=460) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP7 pid=460) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP7 pid=460) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP7 pid=460) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP7 pid=460) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP7 pid=460) >>> MY quant_method in EMBED is UnquantizedLinearMethod
(Worker_TP7 pid=460) WARNING 02-15 22:27:37 [compilation.py:683] Op 'quant_fp8' not present in model, enabling with '+quant_fp8' has no effect
(Worker_TP5 pid=458) >>> MY quant_method in EMBED is NoneType
(Worker_TP5 pid=458) INFO 02-15 22:27:37 [common.py:253] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FP8_BMM=False VLLM_ROCM_USE_AITER_TRITON_FP8_BMM_MAX_BATCH_SIZE=256
(Worker_TP5 pid=458) INFO 02-15 22:27:37 [common.py:254] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_ROPE_ZEROS_KV_CACHE=True
(Worker_TP5 pid=458) INFO 02-15 22:27:37 [rocm.py:222] Using AITER MLA backend on V1 engine.
(Worker_TP5 pid=458) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP5 pid=458) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP5 pid=458) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP5 pid=458) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP5 pid=458) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP5 pid=458) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP5 pid=458) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP5 pid=458) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP5 pid=458) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP5 pid=458) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP5 pid=458) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP5 pid=458) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP5 pid=458) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP5 pid=458) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP7 pid=460) INFO 02-15 22:27:37 [weight_utils.py:392] Using model weights format ['*.safetensors']
(Worker_TP5 pid=458) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP5 pid=458) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP5 pid=458) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP5 pid=458) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP5 pid=458) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP5 pid=458) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP5 pid=458) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP5 pid=458) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP5 pid=458) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP5 pid=458) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP4 pid=457) >>> MY quant_method in EMBED is NoneType
(Worker_TP4 pid=457) INFO 02-15 22:27:37 [common.py:253] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FP8_BMM=False VLLM_ROCM_USE_AITER_TRITON_FP8_BMM_MAX_BATCH_SIZE=256
(Worker_TP4 pid=457) INFO 02-15 22:27:37 [common.py:254] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_ROPE_ZEROS_KV_CACHE=True
(Worker_TP4 pid=457) INFO 02-15 22:27:37 [rocm.py:222] Using AITER MLA backend on V1 engine.
(Worker_TP5 pid=458) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP5 pid=458) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP4 pid=457) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP5 pid=458) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP6 pid=459) >>> MY quant_method in EMBED is NoneType
(Worker_TP6 pid=459) INFO 02-15 22:27:37 [common.py:253] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FP8_BMM=False VLLM_ROCM_USE_AITER_TRITON_FP8_BMM_MAX_BATCH_SIZE=256
(Worker_TP6 pid=459) INFO 02-15 22:27:37 [common.py:254] [Aiter] VLLM_ROCM_USE_AITER_TRITON_FUSED_ROPE_ZEROS_KV_CACHE=True
(Worker_TP4 pid=457) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP5 pid=458) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP6 pid=459) INFO 02-15 22:27:37 [rocm.py:222] Using AITER MLA backend on V1 engine.
(Worker_TP4 pid=457) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP5 pid=458) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP4 pid=457) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP6 pid=459) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP5 pid=458) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP4 pid=457) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP6 pid=459) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP5 pid=458) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP4 pid=457) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP5 pid=458) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP6 pid=459) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP4 pid=457) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP5 pid=458) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP6 pid=459) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP4 pid=457) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP5 pid=458) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP6 pid=459) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP5 pid=458) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP6 pid=459) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP4 pid=457) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP6 pid=459) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP5 pid=458) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP4 pid=457) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP6 pid=459) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP4 pid=457) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP6 pid=459) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP5 pid=458) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP4 pid=457) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
Loading safetensors checkpoint shards:   3% Completed | 2/73 [00:01<00:45,  1.57it/s]
(Worker_TP5 pid=458) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP4 pid=457) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP6 pid=459) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP5 pid=458) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP4 pid=457) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP6 pid=459) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP5 pid=458) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP4 pid=457) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP6 pid=459) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP5 pid=458) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP4 pid=457) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP6 pid=459) INFO 02-15 22:27:37 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP5 pid=458) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP4 pid=457) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP6 pid=459) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP5 pid=458) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP4 pid=457) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP6 pid=459) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP5 pid=458) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP4 pid=457) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP5 pid=458) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP6 pid=459) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP4 pid=457) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP6 pid=459) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP5 pid=458) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP4 pid=457) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP5 pid=458) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP6 pid=459) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP4 pid=457) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP5 pid=458) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP6 pid=459) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP4 pid=457) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP5 pid=458) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP6 pid=459) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP5 pid=458) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP6 pid=459) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP6 pid=459) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP5 pid=458) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP4 pid=457) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP6 pid=459) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP4 pid=457) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP6 pid=459) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP5 pid=458) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP4 pid=457) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP5 pid=458) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP4 pid=457) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP6 pid=459) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP5 pid=458) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP4 pid=457) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP6 pid=459) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP5 pid=458) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP4 pid=457) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP6 pid=459) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP5 pid=458) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP4 pid=457) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP6 pid=459) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP5 pid=458) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP4 pid=457) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP6 pid=459) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP5 pid=458) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP4 pid=457) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP6 pid=459) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP4 pid=457) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP6 pid=459) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP4 pid=457) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP6 pid=459) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP5 pid=458) >>> MY quant_method in EMBED is UnquantizedLinearMethod
(Worker_TP5 pid=458) WARNING 02-15 22:27:38 [compilation.py:683] Op 'quant_fp8' not present in model, enabling with '+quant_fp8' has no effect
(Worker_TP6 pid=459) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP4 pid=457) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP6 pid=459) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP4 pid=457) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP4 pid=457) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP6 pid=459) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP6 pid=459) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP4 pid=457) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP6 pid=459) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP4 pid=457) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP4 pid=457) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP4 pid=457) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP6 pid=459) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP4 pid=457) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP6 pid=459) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP4 pid=457) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP6 pid=459) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP6 pid=459) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP4 pid=457) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP6 pid=459) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP4 pid=457) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP6 pid=459) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP4 pid=457) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP4 pid=457) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP6 pid=459) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP4 pid=457) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP6 pid=459) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP6 pid=459) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP5 pid=458) INFO 02-15 22:27:38 [weight_utils.py:392] Using model weights format ['*.safetensors']
(Worker_TP6 pid=459) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP4 pid=457) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP6 pid=459) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP4 pid=457) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP4 pid=457) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP4 pid=457) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP6 pid=459) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP4 pid=457) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP6 pid=459) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP4 pid=457) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP6 pid=459) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP4 pid=457) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP6 pid=459) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP4 pid=457) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP6 pid=459) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP4 pid=457) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP6 pid=459) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP4 pid=457) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP6 pid=459) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP6 pid=459) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP6 pid=459) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP6 pid=459) INFO 02-15 22:27:38 [deepseek_v2.py:428] [Aiter] DeepseekV2MoE is registered with rocm_aiter_triton_fused_shared_expert_fp4
(Worker_TP4 pid=457) >>> MY quant_method in EMBED is UnquantizedLinearMethod
(Worker_TP4 pid=457) WARNING 02-15 22:27:38 [compilation.py:683] Op 'quant_fp8' not present in model, enabling with '+quant_fp8' has no effect
(Worker_TP6 pid=459) >>> MY quant_method in EMBED is UnquantizedLinearMethod
(Worker_TP6 pid=459) WARNING 02-15 22:27:38 [compilation.py:683] Op 'quant_fp8' not present in model, enabling with '+quant_fp8' has no effect
(Worker_TP4 pid=457) INFO 02-15 22:27:38 [weight_utils.py:392] Using model weights format ['*.safetensors']
(Worker_TP6 pid=459) INFO 02-15 22:27:38 [weight_utils.py:392] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   4% Completed | 3/73 [00:02<00:53,  1.31it/s]
Loading safetensors checkpoint shards:   5% Completed | 4/73 [00:02<00:56,  1.22it/s]
Loading safetensors checkpoint shards:   7% Completed | 5/73 [00:04<01:00,  1.12it/s]
Loading safetensors checkpoint shards:   8% Completed | 6/73 [00:04<00:49,  1.36it/s]
Loading safetensors checkpoint shards:  10% Completed | 7/73 [00:05<00:52,  1.26it/s]
Loading safetensors checkpoint shards:  11% Completed | 8/73 [00:06<00:54,  1.18it/s]
Loading safetensors checkpoint shards:  12% Completed | 9/73 [00:07<00:56,  1.14it/s]
Loading safetensors checkpoint shards:  14% Completed | 10/73 [00:08<00:56,  1.11it/s]
Loading safetensors checkpoint shards:  15% Completed | 11/73 [00:09<00:58,  1.07it/s]
Loading safetensors checkpoint shards:  16% Completed | 12/73 [00:10<00:57,  1.06it/s]
Loading safetensors checkpoint shards:  18% Completed | 13/73 [00:11<00:56,  1.06it/s]
Loading safetensors checkpoint shards:  19% Completed | 14/73 [00:12<00:56,  1.05it/s]
Loading safetensors checkpoint shards:  21% Completed | 15/73 [00:13<00:55,  1.04it/s]
Loading safetensors checkpoint shards:  22% Completed | 16/73 [00:14<00:57,  1.01s/it]
Loading safetensors checkpoint shards:  23% Completed | 17/73 [00:15<00:58,  1.04s/it]
Loading safetensors checkpoint shards:  25% Completed | 18/73 [00:16<00:58,  1.07s/it]
Loading safetensors checkpoint shards:  26% Completed | 19/73 [00:17<00:58,  1.09s/it]
Loading safetensors checkpoint shards:  27% Completed | 20/73 [00:18<00:57,  1.08s/it]
Loading safetensors checkpoint shards:  29% Completed | 21/73 [00:19<00:46,  1.11it/s]
Loading safetensors checkpoint shards:  30% Completed | 22/73 [00:20<00:46,  1.09it/s]
Loading safetensors checkpoint shards:  32% Completed | 23/73 [00:20<00:38,  1.30it/s]
Loading safetensors checkpoint shards:  33% Completed | 24/73 [00:21<00:40,  1.20it/s]
Loading safetensors checkpoint shards:  34% Completed | 25/73 [00:22<00:41,  1.15it/s]
Loading safetensors checkpoint shards:  36% Completed | 26/73 [00:23<00:45,  1.03it/s]
Loading safetensors checkpoint shards:  37% Completed | 27/73 [00:24<00:45,  1.01it/s]
Loading safetensors checkpoint shards:  40% Completed | 29/73 [00:25<00:34,  1.27it/s]
Loading safetensors checkpoint shards:  41% Completed | 30/73 [00:26<00:36,  1.19it/s]
Loading safetensors checkpoint shards:  42% Completed | 31/73 [00:27<00:30,  1.39it/s]
Loading safetensors checkpoint shards:  44% Completed | 32/73 [00:27<00:29,  1.38it/s]
Loading safetensors checkpoint shards:  45% Completed | 33/73 [00:28<00:30,  1.29it/s]
Loading safetensors checkpoint shards:  47% Completed | 34/73 [00:29<00:25,  1.50it/s]
Loading safetensors checkpoint shards:  48% Completed | 35/73 [00:30<00:29,  1.30it/s]
Loading safetensors checkpoint shards:  49% Completed | 36/73 [00:31<00:30,  1.21it/s]
Loading safetensors checkpoint shards:  51% Completed | 37/73 [00:31<00:26,  1.36it/s]
Loading safetensors checkpoint shards:  52% Completed | 38/73 [00:32<00:22,  1.55it/s]
Loading safetensors checkpoint shards:  53% Completed | 39/73 [00:33<00:25,  1.36it/s]
Loading safetensors checkpoint shards:  55% Completed | 40/73 [00:34<00:26,  1.22it/s]
Loading safetensors checkpoint shards:  56% Completed | 41/73 [00:35<00:28,  1.11it/s]
Loading safetensors checkpoint shards:  58% Completed | 42/73 [00:35<00:24,  1.29it/s]
Loading safetensors checkpoint shards:  59% Completed | 43/73 [00:36<00:25,  1.20it/s]
Loading safetensors checkpoint shards:  60% Completed | 44/73 [00:37<00:20,  1.41it/s]
Loading safetensors checkpoint shards:  62% Completed | 45/73 [00:37<00:21,  1.32it/s]
Loading safetensors checkpoint shards:  63% Completed | 46/73 [00:38<00:22,  1.22it/s]
Loading safetensors checkpoint shards:  64% Completed | 47/73 [00:39<00:22,  1.17it/s]
Loading safetensors checkpoint shards:  66% Completed | 48/73 [00:40<00:22,  1.09it/s]
Loading safetensors checkpoint shards:  67% Completed | 49/73 [00:42<00:23,  1.02it/s]
Loading safetensors checkpoint shards:  68% Completed | 50/73 [00:43<00:24,  1.06s/it]
Loading safetensors checkpoint shards:  70% Completed | 51/73 [00:43<00:20,  1.07it/s]
Loading safetensors checkpoint shards:  71% Completed | 52/73 [00:45<00:20,  1.02it/s]
Loading safetensors checkpoint shards:  73% Completed | 53/73 [00:45<00:17,  1.15it/s]
Loading safetensors checkpoint shards:  74% Completed | 54/73 [00:46<00:18,  1.01it/s]
Loading safetensors checkpoint shards:  75% Completed | 55/73 [00:48<00:18,  1.03s/it]
Loading safetensors checkpoint shards:  77% Completed | 56/73 [00:49<00:17,  1.05s/it]
Loading safetensors checkpoint shards:  78% Completed | 57/73 [00:50<00:17,  1.10s/it]
Loading safetensors checkpoint shards:  79% Completed | 58/73 [00:53<00:24,  1.65s/it]
Loading safetensors checkpoint shards:  81% Completed | 59/73 [00:54<00:20,  1.48s/it]
Loading safetensors checkpoint shards:  82% Completed | 60/73 [00:55<00:17,  1.37s/it]
Loading safetensors checkpoint shards:  84% Completed | 61/73 [00:56<00:15,  1.29s/it]
Loading safetensors checkpoint shards:  85% Completed | 62/73 [00:57<00:13,  1.25s/it]
(Worker_TP1 pid=454) INFO 02-15 22:28:34 [default_loader.py:267] Loading weights took 57.27 seconds
(Worker_TP1 pid=454) WARNING 02-15 22:28:34 [kv_cache.py:86] Checkpoint does not provide a q scaling factor. Setting it to k_scale. This only matters for the flash-attn backend.
(Worker_TP1 pid=454) WARNING 02-15 22:28:34 [kv_cache.py:100] Using KV cache scaling factor 1.0 for fp8_e4m3. This may cause accuracy issues. Please make sure k/v_scale scaling factors are available in the fp8 checkpoint.
(Worker_TP1 pid=454) WARNING 02-15 22:28:34 [kv_cache.py:134] Using uncalibrated q_scale 1.0 and/or prob_scale 1.0 with fp8 attention. This may cause accuracy issues. Please make sure q/prob scaling factors are available in the fp8 checkpoint.
Loading safetensors checkpoint shards:  86% Completed | 63/73 [00:58<00:10,  1.02s/it]
(Worker_TP2 pid=455) INFO 02-15 22:28:35 [default_loader.py:267] Loading weights took 58.09 seconds
(Worker_TP2 pid=455) WARNING 02-15 22:28:35 [kv_cache.py:86] Checkpoint does not provide a q scaling factor. Setting it to k_scale. This only matters for the flash-attn backend.
(Worker_TP2 pid=455) WARNING 02-15 22:28:35 [kv_cache.py:100] Using KV cache scaling factor 1.0 for fp8_e4m3. This may cause accuracy issues. Please make sure k/v_scale scaling factors are available in the fp8 checkpoint.
(Worker_TP2 pid=455) WARNING 02-15 22:28:35 [kv_cache.py:134] Using uncalibrated q_scale 1.0 and/or prob_scale 1.0 with fp8 attention. This may cause accuracy issues. Please make sure q/prob scaling factors are available in the fp8 checkpoint.
(Worker_TP3 pid=456) INFO 02-15 22:28:35 [default_loader.py:267] Loading weights took 58.54 seconds
(Worker_TP3 pid=456) WARNING 02-15 22:28:35 [kv_cache.py:86] Checkpoint does not provide a q scaling factor. Setting it to k_scale. This only matters for the flash-attn backend.
(Worker_TP3 pid=456) WARNING 02-15 22:28:35 [kv_cache.py:100] Using KV cache scaling factor 1.0 for fp8_e4m3. This may cause accuracy issues. Please make sure k/v_scale scaling factors are available in the fp8 checkpoint.
(Worker_TP3 pid=456) WARNING 02-15 22:28:35 [kv_cache.py:134] Using uncalibrated q_scale 1.0 and/or prob_scale 1.0 with fp8 attention. This may cause accuracy issues. Please make sure q/prob scaling factors are available in the fp8 checkpoint.
Loading safetensors checkpoint shards:  88% Completed | 64/73 [00:59<00:09,  1.03s/it]
(Worker_TP7 pid=460) INFO 02-15 22:28:36 [default_loader.py:267] Loading weights took 58.53 seconds
(Worker_TP7 pid=460) WARNING 02-15 22:28:36 [kv_cache.py:86] Checkpoint does not provide a q scaling factor. Setting it to k_scale. This only matters for the flash-attn backend.
(Worker_TP7 pid=460) WARNING 02-15 22:28:36 [kv_cache.py:100] Using KV cache scaling factor 1.0 for fp8_e4m3. This may cause accuracy issues. Please make sure k/v_scale scaling factors are available in the fp8 checkpoint.
(Worker_TP7 pid=460) WARNING 02-15 22:28:36 [kv_cache.py:134] Using uncalibrated q_scale 1.0 and/or prob_scale 1.0 with fp8 attention. This may cause accuracy issues. Please make sure q/prob scaling factors are available in the fp8 checkpoint.
(Worker_TP6 pid=459) INFO 02-15 22:28:36 [default_loader.py:267] Loading weights took 57.75 seconds
(Worker_TP6 pid=459) WARNING 02-15 22:28:36 [kv_cache.py:86] Checkpoint does not provide a q scaling factor. Setting it to k_scale. This only matters for the flash-attn backend.
(Worker_TP6 pid=459) WARNING 02-15 22:28:36 [kv_cache.py:100] Using KV cache scaling factor 1.0 for fp8_e4m3. This may cause accuracy issues. Please make sure k/v_scale scaling factors are available in the fp8 checkpoint.
(Worker_TP6 pid=459) WARNING 02-15 22:28:36 [kv_cache.py:134] Using uncalibrated q_scale 1.0 and/or prob_scale 1.0 with fp8 attention. This may cause accuracy issues. Please make sure q/prob scaling factors are available in the fp8 checkpoint.
(Worker_TP4 pid=457) INFO 02-15 22:28:36 [default_loader.py:267] Loading weights took 58.11 seconds
(Worker_TP4 pid=457) WARNING 02-15 22:28:36 [kv_cache.py:86] Checkpoint does not provide a q scaling factor. Setting it to k_scale. This only matters for the flash-attn backend.
(Worker_TP4 pid=457) WARNING 02-15 22:28:36 [kv_cache.py:100] Using KV cache scaling factor 1.0 for fp8_e4m3. This may cause accuracy issues. Please make sure k/v_scale scaling factors are available in the fp8 checkpoint.
(Worker_TP4 pid=457) WARNING 02-15 22:28:36 [kv_cache.py:134] Using uncalibrated q_scale 1.0 and/or prob_scale 1.0 with fp8 attention. This may cause accuracy issues. Please make sure q/prob scaling factors are available in the fp8 checkpoint.
Loading safetensors checkpoint shards:  89% Completed | 65/73 [01:00<00:08,  1.03s/it]
(Worker_TP3 pid=456) INFO 02-15 22:28:37 [gpu_model_runner.py:2580] Model loading took 42.5977 GiB and 60.660121 seconds
(Worker_TP2 pid=455) INFO 02-15 22:28:37 [gpu_model_runner.py:2580] Model loading took 42.5977 GiB and 60.619985 seconds
(Worker_TP1 pid=454) INFO 02-15 22:28:37 [gpu_model_runner.py:2580] Model loading took 42.5977 GiB and 60.222604 seconds
Loading safetensors checkpoint shards:  90% Completed | 66/73 [01:01<00:06,  1.03it/s]
(Worker_TP7 pid=460) INFO 02-15 22:28:37 [gpu_model_runner.py:2580] Model loading took 42.5977 GiB and 60.408231 seconds
(Worker_TP6 pid=459) INFO 02-15 22:28:38 [gpu_model_runner.py:2580] Model loading took 42.5977 GiB and 60.035132 seconds
(Worker_TP4 pid=457) INFO 02-15 22:28:38 [gpu_model_runner.py:2580] Model loading took 42.5977 GiB and 60.103705 seconds
Loading safetensors checkpoint shards:  92% Completed | 67/73 [01:01<00:05,  1.07it/s]
Loading safetensors checkpoint shards:  93% Completed | 68/73 [01:02<00:04,  1.09it/s]
(Worker_TP5 pid=458) INFO 02-15 22:28:40 [default_loader.py:267] Loading weights took 62.05 seconds
(Worker_TP5 pid=458) WARNING 02-15 22:28:40 [kv_cache.py:86] Checkpoint does not provide a q scaling factor. Setting it to k_scale. This only matters for the flash-attn backend.
(Worker_TP5 pid=458) WARNING 02-15 22:28:40 [kv_cache.py:100] Using KV cache scaling factor 1.0 for fp8_e4m3. This may cause accuracy issues. Please make sure k/v_scale scaling factors are available in the fp8 checkpoint.
(Worker_TP5 pid=458) WARNING 02-15 22:28:40 [kv_cache.py:134] Using uncalibrated q_scale 1.0 and/or prob_scale 1.0 with fp8 attention. This may cause accuracy issues. Please make sure q/prob scaling factors are available in the fp8 checkpoint.
Loading safetensors checkpoint shards:  95% Completed | 69/73 [01:03<00:03,  1.11it/s]
Loading safetensors checkpoint shards:  96% Completed | 70/73 [01:04<00:02,  1.12it/s]
(Worker_TP5 pid=458) INFO 02-15 22:28:42 [gpu_model_runner.py:2580] Model loading took 42.5977 GiB and 63.864218 seconds
Loading safetensors checkpoint shards:  97% Completed | 71/73 [01:05<00:01,  1.16it/s]
Loading safetensors checkpoint shards:  99% Completed | 72/73 [01:06<00:00,  1.18it/s]
Loading safetensors checkpoint shards: 100% Completed | 73/73 [01:06<00:00,  1.21it/s]
Loading safetensors checkpoint shards: 100% Completed | 73/73 [01:06<00:00,  1.09it/s]
(Worker_TP0 pid=453) 
(Worker_TP0 pid=453) INFO 02-15 22:28:43 [default_loader.py:267] Loading weights took 67.00 seconds
(Worker_TP0 pid=453) WARNING 02-15 22:28:43 [kv_cache.py:86] Checkpoint does not provide a q scaling factor. Setting it to k_scale. This only matters for the flash-attn backend.
(Worker_TP0 pid=453) WARNING 02-15 22:28:43 [kv_cache.py:100] Using KV cache scaling factor 1.0 for fp8_e4m3. This may cause accuracy issues. Please make sure k/v_scale scaling factors are available in the fp8 checkpoint.
(Worker_TP0 pid=453) WARNING 02-15 22:28:43 [kv_cache.py:134] Using uncalibrated q_scale 1.0 and/or prob_scale 1.0 with fp8 attention. This may cause accuracy issues. Please make sure q/prob scaling factors are available in the fp8 checkpoint.
(Worker_TP0 pid=453) INFO 02-15 22:28:45 [gpu_model_runner.py:2580] Model loading took 42.5977 GiB and 68.712339 seconds
(Worker_TP7 pid=460) /usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:1652: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
(Worker_TP7 pid=460)   torch._dynamo.utils.warn_once(msg)
(Worker_TP6 pid=459) /usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:1652: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
(Worker_TP6 pid=459)   torch._dynamo.utils.warn_once(msg)
(Worker_TP0 pid=453) /usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:1652: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
(Worker_TP0 pid=453)   torch._dynamo.utils.warn_once(msg)
(Worker_TP3 pid=456) /usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:1652: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
(Worker_TP3 pid=456)   torch._dynamo.utils.warn_once(msg)
(Worker_TP2 pid=455) /usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:1652: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
(Worker_TP2 pid=455)   torch._dynamo.utils.warn_once(msg)
(Worker_TP4 pid=457) /usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:1652: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
(Worker_TP4 pid=457)   torch._dynamo.utils.warn_once(msg)
(Worker_TP5 pid=458) /usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:1652: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
(Worker_TP5 pid=458)   torch._dynamo.utils.warn_once(msg)
(Worker_TP1 pid=454) /usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:1652: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
(Worker_TP1 pid=454)   torch._dynamo.utils.warn_once(msg)
(Worker_TP6 pid=459) INFO 02-15 22:28:49 [backends.py:546] vLLM's torch.compile cache is disabled.
(Worker_TP6 pid=459) INFO 02-15 22:28:49 [backends.py:559] Dynamo bytecode transform time: 4.26 s
(Worker_TP7 pid=460) INFO 02-15 22:28:49 [backends.py:546] vLLM's torch.compile cache is disabled.
(Worker_TP7 pid=460) INFO 02-15 22:28:49 [backends.py:559] Dynamo bytecode transform time: 4.29 s
(Worker_TP0 pid=453) INFO 02-15 22:28:49 [backends.py:546] vLLM's torch.compile cache is disabled.
(Worker_TP0 pid=453) INFO 02-15 22:28:49 [backends.py:559] Dynamo bytecode transform time: 4.37 s
(Worker_TP2 pid=455) INFO 02-15 22:28:49 [backends.py:546] vLLM's torch.compile cache is disabled.
(Worker_TP2 pid=455) INFO 02-15 22:28:49 [backends.py:559] Dynamo bytecode transform time: 4.32 s
(Worker_TP3 pid=456) INFO 02-15 22:28:49 [backends.py:546] vLLM's torch.compile cache is disabled.
(Worker_TP3 pid=456) INFO 02-15 22:28:49 [backends.py:559] Dynamo bytecode transform time: 4.41 s
(Worker_TP4 pid=457) INFO 02-15 22:28:50 [backends.py:546] vLLM's torch.compile cache is disabled.
(Worker_TP4 pid=457) INFO 02-15 22:28:50 [backends.py:559] Dynamo bytecode transform time: 4.41 s
(Worker_TP1 pid=454) INFO 02-15 22:28:50 [backends.py:546] vLLM's torch.compile cache is disabled.
(Worker_TP1 pid=454) INFO 02-15 22:28:50 [backends.py:559] Dynamo bytecode transform time: 4.40 s
(Worker_TP5 pid=458) INFO 02-15 22:28:50 [backends.py:546] vLLM's torch.compile cache is disabled.
(Worker_TP5 pid=458) INFO 02-15 22:28:50 [backends.py:559] Dynamo bytecode transform time: 4.45 s
(Worker_TP6 pid=459) INFO 02-15 22:29:15 [backends.py:218] Compiling a graph for dynamic shape takes 25.61 s
(Worker_TP2 pid=455) INFO 02-15 22:29:16 [backends.py:218] Compiling a graph for dynamic shape takes 26.02 s
(Worker_TP7 pid=460) INFO 02-15 22:29:16 [backends.py:218] Compiling a graph for dynamic shape takes 26.26 s
(Worker_TP0 pid=453) INFO 02-15 22:29:16 [backends.py:218] Compiling a graph for dynamic shape takes 26.30 s
(Worker_TP1 pid=454) INFO 02-15 22:29:16 [backends.py:218] Compiling a graph for dynamic shape takes 26.09 s
(Worker_TP6 pid=459) INFO 02-15 22:29:16 [monitor.py:34] torch.compile takes 29.87 s in total
(Worker_TP3 pid=456) INFO 02-15 22:29:16 [backends.py:218] Compiling a graph for dynamic shape takes 26.34 s
(Worker_TP5 pid=458) INFO 02-15 22:29:16 [backends.py:218] Compiling a graph for dynamic shape takes 26.22 s
(Worker_TP4 pid=457) INFO 02-15 22:29:16 [backends.py:218] Compiling a graph for dynamic shape takes 26.34 s
(Worker_TP2 pid=455) INFO 02-15 22:29:17 [monitor.py:34] torch.compile takes 30.34 s in total
(Worker_TP7 pid=460) INFO 02-15 22:29:17 [monitor.py:34] torch.compile takes 30.55 s in total
(Worker_TP0 pid=453) INFO 02-15 22:29:17 [monitor.py:34] torch.compile takes 30.67 s in total
(Worker_TP1 pid=454) INFO 02-15 22:29:17 [monitor.py:34] torch.compile takes 30.50 s in total
(Worker_TP3 pid=456) INFO 02-15 22:29:17 [monitor.py:34] torch.compile takes 30.74 s in total
(Worker_TP5 pid=458) INFO 02-15 22:29:17 [monitor.py:34] torch.compile takes 30.66 s in total
(Worker_TP4 pid=457) INFO 02-15 22:29:17 [monitor.py:34] torch.compile takes 30.75 s in total
(Worker_TP6 pid=459) [aiter] import [module_moe_asm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_asm.so
(Worker_TP6 pid=459) [aiter] type hints mismatch, override to --> moe_fused_gate(input: torch.Tensor, bias: torch.Tensor, topk_weights: torch.Tensor, topk_ids: torch.Tensor, num_expert_group: int, topk_group: int, topk: int, n_share_experts_fusion: int, routed_scaling_factor: float = 1.0) -> List[torch.Tensor]
(Worker_TP6 pid=459) [aiter] [fused_moe] using 2stage default for (256, 1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
(Worker_TP6 pid=459) [aiter] import [module_moe_sorting] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_sorting.so
(Worker_TP6 pid=459) [aiter] import [module_quant] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_quant.so
(Worker_TP6 pid=459) [aiter] type hints mismatch, override to --> dynamic_per_group_scaled_quant_fp4(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, group_size: int = 32, shuffle_scale: bool = True, num_rows: Optional[torch.Tensor] = None, num_rows_factor: int = 1) -> None
(Worker_TP6 pid=459) [aiter] import [module_moe_ck2stages] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_ck2stages.so
(Worker_TP6 pid=459) [aiter] type hints mismatch, override to --> ck_moe_stage1(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w1_scale: Optional[torch.Tensor] = None, a1_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
(Worker_TP6 pid=459) [aiter] type hints mismatch, override to --> ck_moe_stage2(inter_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w2_scale: Optional[torch.Tensor] = None, a2_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
(Worker_TP2 pid=455) [aiter] import [module_moe_asm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_asm.so
(Worker_TP2 pid=455) [aiter] type hints mismatch, override to --> moe_fused_gate(input: torch.Tensor, bias: torch.Tensor, topk_weights: torch.Tensor, topk_ids: torch.Tensor, num_expert_group: int, topk_group: int, topk: int, n_share_experts_fusion: int, routed_scaling_factor: float = 1.0) -> List[torch.Tensor]
(Worker_TP2 pid=455) [aiter] [fused_moe] using 2stage default for (256, 1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
(Worker_TP2 pid=455) [aiter] import [module_moe_sorting] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_sorting.so
(Worker_TP2 pid=455) [aiter] import [module_quant] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_quant.so
(Worker_TP2 pid=455) [aiter] type hints mismatch, override to --> dynamic_per_group_scaled_quant_fp4(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, group_size: int = 32, shuffle_scale: bool = True, num_rows: Optional[torch.Tensor] = None, num_rows_factor: int = 1) -> None
(Worker_TP0 pid=453) [aiter] import [module_moe_asm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_asm.so
(Worker_TP0 pid=453) [aiter] type hints mismatch, override to --> moe_fused_gate(input: torch.Tensor, bias: torch.Tensor, topk_weights: torch.Tensor, topk_ids: torch.Tensor, num_expert_group: int, topk_group: int, topk: int, n_share_experts_fusion: int, routed_scaling_factor: float = 1.0) -> List[torch.Tensor]
(Worker_TP0 pid=453) [aiter] [fused_moe] using 2stage default for (256, 1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
(Worker_TP0 pid=453) [aiter] import [module_moe_sorting] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_sorting.so
(Worker_TP7 pid=460) [aiter] import [module_moe_asm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_asm.so
(Worker_TP7 pid=460) [aiter] type hints mismatch, override to --> moe_fused_gate(input: torch.Tensor, bias: torch.Tensor, topk_weights: torch.Tensor, topk_ids: torch.Tensor, num_expert_group: int, topk_group: int, topk: int, n_share_experts_fusion: int, routed_scaling_factor: float = 1.0) -> List[torch.Tensor]
(Worker_TP0 pid=453) [aiter] import [module_quant] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_quant.so
(Worker_TP0 pid=453) [aiter] type hints mismatch, override to --> dynamic_per_group_scaled_quant_fp4(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, group_size: int = 32, shuffle_scale: bool = True, num_rows: Optional[torch.Tensor] = None, num_rows_factor: int = 1) -> None
(Worker_TP7 pid=460) [aiter] [fused_moe] using 2stage default for (256, 1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
(Worker_TP7 pid=460) [aiter] import [module_moe_sorting] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_sorting.so
(Worker_TP7 pid=460) [aiter] import [module_quant] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_quant.so
(Worker_TP7 pid=460) [aiter] type hints mismatch, override to --> dynamic_per_group_scaled_quant_fp4(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, group_size: int = 32, shuffle_scale: bool = True, num_rows: Optional[torch.Tensor] = None, num_rows_factor: int = 1) -> None
(Worker_TP2 pid=455) [aiter] import [module_moe_ck2stages] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_ck2stages.so
(Worker_TP2 pid=455) [aiter] type hints mismatch, override to --> ck_moe_stage1(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w1_scale: Optional[torch.Tensor] = None, a1_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
(Worker_TP2 pid=455) [aiter] type hints mismatch, override to --> ck_moe_stage2(inter_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w2_scale: Optional[torch.Tensor] = None, a2_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
(Worker_TP3 pid=456) [aiter] import [module_moe_asm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_asm.so
(Worker_TP3 pid=456) [aiter] type hints mismatch, override to --> moe_fused_gate(input: torch.Tensor, bias: torch.Tensor, topk_weights: torch.Tensor, topk_ids: torch.Tensor, num_expert_group: int, topk_group: int, topk: int, n_share_experts_fusion: int, routed_scaling_factor: float = 1.0) -> List[torch.Tensor]
(Worker_TP1 pid=454) [aiter] import [module_moe_asm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_asm.so
(Worker_TP1 pid=454) [aiter] type hints mismatch, override to --> moe_fused_gate(input: torch.Tensor, bias: torch.Tensor, topk_weights: torch.Tensor, topk_ids: torch.Tensor, num_expert_group: int, topk_group: int, topk: int, n_share_experts_fusion: int, routed_scaling_factor: float = 1.0) -> List[torch.Tensor]
(Worker_TP3 pid=456) [aiter] [fused_moe] using 2stage default for (256, 1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
(Worker_TP3 pid=456) [aiter] import [module_moe_sorting] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_sorting.so
(Worker_TP1 pid=454) [aiter] [fused_moe] using 2stage default for (256, 1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
(Worker_TP3 pid=456) [aiter] import [module_quant] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_quant.so
(Worker_TP3 pid=456) [aiter] type hints mismatch, override to --> dynamic_per_group_scaled_quant_fp4(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, group_size: int = 32, shuffle_scale: bool = True, num_rows: Optional[torch.Tensor] = None, num_rows_factor: int = 1) -> None
(Worker_TP1 pid=454) [aiter] import [module_moe_sorting] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_sorting.so
(Worker_TP1 pid=454) [aiter] import [module_quant] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_quant.so
(Worker_TP1 pid=454) [aiter] type hints mismatch, override to --> dynamic_per_group_scaled_quant_fp4(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, group_size: int = 32, shuffle_scale: bool = True, num_rows: Optional[torch.Tensor] = None, num_rows_factor: int = 1) -> None
(Worker_TP0 pid=453) [aiter] import [module_moe_ck2stages] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_ck2stages.so
(Worker_TP0 pid=453) [aiter] type hints mismatch, override to --> ck_moe_stage1(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w1_scale: Optional[torch.Tensor] = None, a1_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
(Worker_TP6 pid=459) [aiter] import [module_rmsnorm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_rmsnorm.so
(Worker_TP5 pid=458) [aiter] import [module_moe_asm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_asm.so
(Worker_TP5 pid=458) [aiter] type hints mismatch, override to --> moe_fused_gate(input: torch.Tensor, bias: torch.Tensor, topk_weights: torch.Tensor, topk_ids: torch.Tensor, num_expert_group: int, topk_group: int, topk: int, n_share_experts_fusion: int, routed_scaling_factor: float = 1.0) -> List[torch.Tensor]
(Worker_TP5 pid=458) [aiter] [fused_moe] using 2stage default for (256, 1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
(Worker_TP0 pid=453) [aiter] type hints mismatch, override to --> ck_moe_stage2(inter_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w2_scale: Optional[torch.Tensor] = None, a2_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
(Worker_TP5 pid=458) [aiter] import [module_moe_sorting] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_sorting.so
(Worker_TP4 pid=457) [aiter] import [module_moe_asm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_asm.so
(Worker_TP5 pid=458) [aiter] import [module_quant] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_quant.so
(Worker_TP4 pid=457) [aiter] type hints mismatch, override to --> moe_fused_gate(input: torch.Tensor, bias: torch.Tensor, topk_weights: torch.Tensor, topk_ids: torch.Tensor, num_expert_group: int, topk_group: int, topk: int, n_share_experts_fusion: int, routed_scaling_factor: float = 1.0) -> List[torch.Tensor]
(Worker_TP5 pid=458) [aiter] type hints mismatch, override to --> dynamic_per_group_scaled_quant_fp4(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, group_size: int = 32, shuffle_scale: bool = True, num_rows: Optional[torch.Tensor] = None, num_rows_factor: int = 1) -> None
(Worker_TP4 pid=457) [aiter] [fused_moe] using 2stage default for (256, 1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
(Worker_TP4 pid=457) [aiter] import [module_moe_sorting] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_sorting.so
(Worker_TP4 pid=457) [aiter] import [module_quant] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_quant.so
(Worker_TP4 pid=457) [aiter] type hints mismatch, override to --> dynamic_per_group_scaled_quant_fp4(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, group_size: int = 32, shuffle_scale: bool = True, num_rows: Optional[torch.Tensor] = None, num_rows_factor: int = 1) -> None
(Worker_TP3 pid=456) [aiter] import [module_moe_ck2stages] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_ck2stages.so
(Worker_TP3 pid=456) [aiter] type hints mismatch, override to --> ck_moe_stage1(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w1_scale: Optional[torch.Tensor] = None, a1_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
(Worker_TP3 pid=456) [aiter] type hints mismatch, override to --> ck_moe_stage2(inter_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w2_scale: Optional[torch.Tensor] = None, a2_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
(Worker_TP7 pid=460) [aiter] import [module_moe_ck2stages] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_ck2stages.so
(Worker_TP7 pid=460) [aiter] type hints mismatch, override to --> ck_moe_stage1(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w1_scale: Optional[torch.Tensor] = None, a1_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
(Worker_TP2 pid=455) [aiter] import [module_rmsnorm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_rmsnorm.so
(Worker_TP7 pid=460) [aiter] type hints mismatch, override to --> ck_moe_stage2(inter_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w2_scale: Optional[torch.Tensor] = None, a2_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
(Worker_TP1 pid=454) [aiter] import [module_moe_ck2stages] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_ck2stages.so
(Worker_TP1 pid=454) [aiter] type hints mismatch, override to --> ck_moe_stage1(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w1_scale: Optional[torch.Tensor] = None, a1_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
(Worker_TP5 pid=458) [aiter] import [module_moe_ck2stages] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_ck2stages.so
(Worker_TP5 pid=458) [aiter] type hints mismatch, override to --> ck_moe_stage1(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w1_scale: Optional[torch.Tensor] = None, a1_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
(Worker_TP5 pid=458) [aiter] type hints mismatch, override to --> ck_moe_stage2(inter_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w2_scale: Optional[torch.Tensor] = None, a2_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
(Worker_TP1 pid=454) [aiter] type hints mismatch, override to --> ck_moe_stage2(inter_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w2_scale: Optional[torch.Tensor] = None, a2_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
(Worker_TP0 pid=453) [aiter] import [module_rmsnorm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_rmsnorm.so
(Worker_TP4 pid=457) [aiter] import [module_moe_ck2stages] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_moe_ck2stages.so
(Worker_TP4 pid=457) [aiter] type hints mismatch, override to --> ck_moe_stage1(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w1_scale: Optional[torch.Tensor] = None, a1_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
(Worker_TP4 pid=457) [aiter] type hints mismatch, override to --> ck_moe_stage2(inter_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w2_scale: Optional[torch.Tensor] = None, a2_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
(Worker_TP3 pid=456) [aiter] import [module_rmsnorm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_rmsnorm.so
(Worker_TP5 pid=458) [aiter] import [module_rmsnorm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_rmsnorm.so
(Worker_TP7 pid=460) [aiter] import [module_rmsnorm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_rmsnorm.so
(Worker_TP1 pid=454) [aiter] import [module_rmsnorm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_rmsnorm.so
(Worker_TP4 pid=457) [aiter] import [module_rmsnorm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_rmsnorm.so
(Worker_TP6 pid=459) INFO 02-15 22:29:39 [gpu_worker.py:299] Available KV cache memory: 213.86 GiB
(Worker_TP4 pid=457) INFO 02-15 22:29:39 [gpu_worker.py:299] Available KV cache memory: 213.73 GiB
(Worker_TP0 pid=453) INFO 02-15 22:29:39 [gpu_worker.py:299] Available KV cache memory: 214.45 GiB
(Worker_TP5 pid=458) INFO 02-15 22:29:39 [gpu_worker.py:299] Available KV cache memory: 213.90 GiB
(Worker_TP2 pid=455) INFO 02-15 22:29:39 [gpu_worker.py:299] Available KV cache memory: 213.61 GiB
(Worker_TP3 pid=456) INFO 02-15 22:29:39 [gpu_worker.py:299] Available KV cache memory: 213.63 GiB
(Worker_TP1 pid=454) INFO 02-15 22:29:39 [gpu_worker.py:299] Available KV cache memory: 213.62 GiB
(Worker_TP7 pid=460) INFO 02-15 22:29:39 [gpu_worker.py:299] Available KV cache memory: 213.88 GiB
(EngineCore_DP0 pid=303) INFO 02-15 22:29:40 [kv_cache_utils.py:1087] GPU KV cache size: 6,553,531 tokens
(EngineCore_DP0 pid=303) INFO 02-15 22:29:40 [kv_cache_utils.py:1091] Maximum concurrency for 70,000 tokens per request: 93.62x
(EngineCore_DP0 pid=303) INFO 02-15 22:29:40 [kv_cache_utils.py:1087] GPU KV cache size: 6,528,105 tokens
(EngineCore_DP0 pid=303) INFO 02-15 22:29:40 [kv_cache_utils.py:1091] Maximum concurrency for 70,000 tokens per request: 93.26x
(EngineCore_DP0 pid=303) INFO 02-15 22:29:40 [kv_cache_utils.py:1087] GPU KV cache size: 6,527,747 tokens
(EngineCore_DP0 pid=303) INFO 02-15 22:29:40 [kv_cache_utils.py:1091] Maximum concurrency for 70,000 tokens per request: 93.25x
(EngineCore_DP0 pid=303) INFO 02-15 22:29:40 [kv_cache_utils.py:1087] GPU KV cache size: 6,528,463 tokens
(EngineCore_DP0 pid=303) INFO 02-15 22:29:40 [kv_cache_utils.py:1091] Maximum concurrency for 70,000 tokens per request: 93.26x
(EngineCore_DP0 pid=303) INFO 02-15 22:29:40 [kv_cache_utils.py:1087] GPU KV cache size: 6,531,447 tokens
(EngineCore_DP0 pid=303) INFO 02-15 22:29:40 [kv_cache_utils.py:1091] Maximum concurrency for 70,000 tokens per request: 93.31x
(EngineCore_DP0 pid=303) INFO 02-15 22:29:40 [kv_cache_utils.py:1087] GPU KV cache size: 6,536,700 tokens
(EngineCore_DP0 pid=303) INFO 02-15 22:29:40 [kv_cache_utils.py:1091] Maximum concurrency for 70,000 tokens per request: 93.38x
(EngineCore_DP0 pid=303) INFO 02-15 22:29:40 [kv_cache_utils.py:1087] GPU KV cache size: 6,535,625 tokens
(EngineCore_DP0 pid=303) INFO 02-15 22:29:40 [kv_cache_utils.py:1091] Maximum concurrency for 70,000 tokens per request: 93.37x
(EngineCore_DP0 pid=303) INFO 02-15 22:29:40 [kv_cache_utils.py:1087] GPU KV cache size: 6,536,222 tokens
(EngineCore_DP0 pid=303) INFO 02-15 22:29:40 [kv_cache_utils.py:1091] Maximum concurrency for 70,000 tokens per request: 93.37x
(Worker_TP3 pid=456) WARNING 02-15 22:29:41 [gpu_model_runner.py:3562] CUDAGraphMode.FULL is not supported with AiterMLAMetadataBuilder backend (support: AttentionCGSupport.UNIFORM_SINGLE_TOKEN_DECODE); setting cudagraph_mode=FULL_DECODE_ONLY
(Worker_TP0 pid=453) WARNING 02-15 22:29:41 [gpu_model_runner.py:3562] CUDAGraphMode.FULL is not supported with AiterMLAMetadataBuilder backend (support: AttentionCGSupport.UNIFORM_SINGLE_TOKEN_DECODE); setting cudagraph_mode=FULL_DECODE_ONLY
(Worker_TP4 pid=457) WARNING 02-15 22:29:41 [gpu_model_runner.py:3562] CUDAGraphMode.FULL is not supported with AiterMLAMetadataBuilder backend (support: AttentionCGSupport.UNIFORM_SINGLE_TOKEN_DECODE); setting cudagraph_mode=FULL_DECODE_ONLY
(Worker_TP5 pid=458) WARNING 02-15 22:29:41 [gpu_model_runner.py:3562] CUDAGraphMode.FULL is not supported with AiterMLAMetadataBuilder backend (support: AttentionCGSupport.UNIFORM_SINGLE_TOKEN_DECODE); setting cudagraph_mode=FULL_DECODE_ONLY
(Worker_TP6 pid=459) WARNING 02-15 22:29:41 [gpu_model_runner.py:3562] CUDAGraphMode.FULL is not supported with AiterMLAMetadataBuilder backend (support: AttentionCGSupport.UNIFORM_SINGLE_TOKEN_DECODE); setting cudagraph_mode=FULL_DECODE_ONLY
(Worker_TP2 pid=455) WARNING 02-15 22:29:41 [gpu_model_runner.py:3562] CUDAGraphMode.FULL is not supported with AiterMLAMetadataBuilder backend (support: AttentionCGSupport.UNIFORM_SINGLE_TOKEN_DECODE); setting cudagraph_mode=FULL_DECODE_ONLY
(Worker_TP7 pid=460) WARNING 02-15 22:29:41 [gpu_model_runner.py:3562] CUDAGraphMode.FULL is not supported with AiterMLAMetadataBuilder backend (support: AttentionCGSupport.UNIFORM_SINGLE_TOKEN_DECODE); setting cudagraph_mode=FULL_DECODE_ONLY
(Worker_TP1 pid=454) WARNING 02-15 22:29:41 [gpu_model_runner.py:3562] CUDAGraphMode.FULL is not supported with AiterMLAMetadataBuilder backend (support: AttentionCGSupport.UNIFORM_SINGLE_TOKEN_DECODE); setting cudagraph_mode=FULL_DECODE_ONLY
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s](Worker_TP6 pid=459) [aiter] import [module_mla_asm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_mla_asm.so
(Worker_TP6 pid=459) [aiter] type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
[aiter] hipModuleLoad: /usr/local/lib/python3.12/dist-packages/aiter_meta/hsa/gfx950//mla/mla_a8w8_qh16_qseqlen1_gqaratio16.co GetFunction: _ZN5aiter33mla_a8w8_qh16_qseqlen1_gqaratio16E Success
(Worker_TP5 pid=458) [aiter] import [module_mla_asm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_mla_asm.so
(Worker_TP5 pid=458) [aiter] type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
[aiter] hipModuleLoad: /usr/local/lib/python3.12/dist-packages/aiter_meta/hsa/gfx950//mla/mla_a8w8_qh16_qseqlen1_gqaratio16.co GetFunction: _ZN5aiter33mla_a8w8_qh16_qseqlen1_gqaratio16E Success
(Worker_TP4 pid=457) [aiter] import [module_mla_asm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_mla_asm.so
(Worker_TP4 pid=457) [aiter] type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
[aiter] hipModuleLoad: /usr/local/lib/python3.12/dist-packages/aiter_meta/hsa/gfx950//mla/mla_a8w8_qh16_qseqlen1_gqaratio16.co GetFunction: _ZN5aiter33mla_a8w8_qh16_qseqlen1_gqaratio16E Success
(Worker_TP1 pid=454) [aiter] import [module_mla_asm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_mla_asm.so
(Worker_TP1 pid=454) [aiter] type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
[aiter] hipModuleLoad: /usr/local/lib/python3.12/dist-packages/aiter_meta/hsa/gfx950//mla/mla_a8w8_qh16_qseqlen1_gqaratio16.co GetFunction: _ZN5aiter33mla_a8w8_qh16_qseqlen1_gqaratio16E Success
(Worker_TP3 pid=456) [aiter] import [module_mla_asm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_mla_asm.so
(Worker_TP3 pid=456) [aiter] type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
[aiter] hipModuleLoad: /usr/local/lib/python3.12/dist-packages/aiter_meta/hsa/gfx950//mla/mla_a8w8_qh16_qseqlen1_gqaratio16.co GetFunction: _ZN5aiter33mla_a8w8_qh16_qseqlen1_gqaratio16E Success
(Worker_TP2 pid=455) [aiter] import [module_mla_asm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_mla_asm.so
(Worker_TP2 pid=455) [aiter] type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
[aiter] hipModuleLoad: /usr/local/lib/python3.12/dist-packages/aiter_meta/hsa/gfx950//mla/mla_a8w8_qh16_qseqlen1_gqaratio16.co GetFunction: _ZN5aiter33mla_a8w8_qh16_qseqlen1_gqaratio16E Success
[aiter] import [module_mla_asm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_mla_asm.so
(Worker_TP0 pid=453) [aiter] type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
[aiter] hipModuleLoad: /usr/local/lib/python3.12/dist-packages/aiter_meta/hsa/gfx950//mla/mla_a8w8_qh16_qseqlen1_gqaratio16.co GetFunction: _ZN5aiter33mla_a8w8_qh16_qseqlen1_gqaratio16E Success
(Worker_TP7 pid=460) [aiter] import [module_mla_asm] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_mla_asm.so
(Worker_TP7 pid=460) [aiter] type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
[aiter] hipModuleLoad: /usr/local/lib/python3.12/dist-packages/aiter_meta/hsa/gfx950//mla/mla_a8w8_qh16_qseqlen1_gqaratio16.co GetFunction: _ZN5aiter33mla_a8w8_qh16_qseqlen1_gqaratio16E Success
(Worker_TP6 pid=459) [aiter] [fused_moe] using 2stage default for (256, 32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
(Worker_TP5 pid=458) [aiter] [fused_moe] using 2stage default for (256, 32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
(Worker_TP4 pid=457) [aiter] [fused_moe] using 2stage default for (256, 32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
(Worker_TP2 pid=455) [aiter] [fused_moe] using 2stage default for (256, 32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
(Worker_TP0 pid=453) [aiter] [fused_moe] using 2stage default for (256, 32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
(Worker_TP1 pid=454) [aiter] [fused_moe] using 2stage default for (256, 32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
(Worker_TP3 pid=456) [aiter] [fused_moe] using 2stage default for (256, 32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
(Worker_TP7 pid=460) [aiter] [fused_moe] using 2stage default for (256, 32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:05<00:13,  2.63s/it](Worker_TP5 pid=458) [aiter] [fused_moe] using 2stage default for (256, 16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
(Worker_TP6 pid=459) [aiter] [fused_moe] using 2stage default for (256, 16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
(Worker_TP4 pid=457) [aiter] [fused_moe] using 2stage default for (256, 16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
(Worker_TP2 pid=455) [aiter] [fused_moe] using 2stage default for (256, 16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
(Worker_TP1 pid=454) [aiter] [fused_moe] using 2stage default for (256, 16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 3/7 [00:06<00:08,  2.04s/it](Worker_TP7 pid=460) [aiter] [fused_moe] using 2stage default for (256, 16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
(Worker_TP3 pid=456) [aiter] [fused_moe] using 2stage default for (256, 16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:11<00:00,  1.67s/it]
(Worker_TP5 pid=458) INFO 02-15 22:29:53 [custom_all_reduce.py:203] Registering 861 cuda graph addresses
(Worker_TP0 pid=453) INFO 02-15 22:29:53 [custom_all_reduce.py:203] Registering 861 cuda graph addresses
(Worker_TP6 pid=459) INFO 02-15 22:29:53 [custom_all_reduce.py:203] Registering 861 cuda graph addresses
(Worker_TP2 pid=455) INFO 02-15 22:29:53 [custom_all_reduce.py:203] Registering 861 cuda graph addresses
(Worker_TP1 pid=454) INFO 02-15 22:29:53 [custom_all_reduce.py:203] Registering 861 cuda graph addresses
(Worker_TP4 pid=457) INFO 02-15 22:29:53 [custom_all_reduce.py:203] Registering 861 cuda graph addresses
(Worker_TP7 pid=460) INFO 02-15 22:29:54 [custom_all_reduce.py:203] Registering 861 cuda graph addresses
(Worker_TP3 pid=456) INFO 02-15 22:29:54 [custom_all_reduce.py:203] Registering 861 cuda graph addresses
(Worker_TP6 pid=459) INFO 02-15 22:29:55 [gpu_model_runner.py:3378] Graph capturing finished in 14 secs, took 0.24 GiB
(Worker_TP6 pid=459) INFO 02-15 22:29:55 [gpu_worker.py:392] Free memory on device (287.02/287.58 GiB) on startup. Desired GPU memory utilization is (0.95, 273.2 GiB). Actual usage is 42.6 GiB for weight, 9.98 GiB for peak activation, 6.76 GiB for non-torch memory, and 0.24 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=229218418688` to fit into requested memory, or `--kv-cache-memory=244057866240` to fully utilize gpu memory. Current kv cache memory in use is 229635751936 bytes.
(Worker_TP4 pid=457) INFO 02-15 22:29:55 [gpu_model_runner.py:3378] Graph capturing finished in 14 secs, took 0.24 GiB
(Worker_TP4 pid=457) INFO 02-15 22:29:55 [gpu_worker.py:392] Free memory on device (287.02/287.58 GiB) on startup. Desired GPU memory utilization is (0.95, 273.2 GiB). Actual usage is 42.6 GiB for weight, 9.98 GiB for peak activation, 6.9 GiB for non-torch memory, and 0.24 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=229071618048` to fit into requested memory, or `--kv-cache-memory=243911065600` to fully utilize gpu memory. Current kv cache memory in use is 229488951296 bytes.
(Worker_TP5 pid=458) INFO 02-15 22:29:55 [gpu_model_runner.py:3378] Graph capturing finished in 14 secs, took 0.24 GiB
(Worker_TP5 pid=458) INFO 02-15 22:29:55 [gpu_worker.py:392] Free memory on device (287.02/287.58 GiB) on startup. Desired GPU memory utilization is (0.95, 273.2 GiB). Actual usage is 42.6 GiB for weight, 9.98 GiB for peak activation, 6.72 GiB for non-torch memory, and 0.24 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=229256167424` to fit into requested memory, or `--kv-cache-memory=244095614976` to fully utilize gpu memory. Current kv cache memory in use is 229673500672 bytes.
(Worker_TP0 pid=453) INFO 02-15 22:29:55 [gpu_model_runner.py:3378] Graph capturing finished in 14 secs, took 0.24 GiB
(Worker_TP0 pid=453) INFO 02-15 22:29:55 [gpu_worker.py:392] Free memory on device (287.02/287.58 GiB) on startup. Desired GPU memory utilization is (0.95, 273.2 GiB). Actual usage is 42.6 GiB for weight, 9.98 GiB for peak activation, 6.17 GiB for non-torch memory, and 0.24 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=229847564288` to fit into requested memory, or `--kv-cache-memory=244687011840` to fully utilize gpu memory. Current kv cache memory in use is 230264897536 bytes.
(Worker_TP2 pid=455) INFO 02-15 22:29:55 [gpu_model_runner.py:3378] Graph capturing finished in 14 secs, took 0.24 GiB
(Worker_TP2 pid=455) INFO 02-15 22:29:55 [gpu_worker.py:392] Free memory on device (287.02/287.58 GiB) on startup. Desired GPU memory utilization is (0.95, 273.2 GiB). Actual usage is 42.6 GiB for weight, 9.98 GiB for peak activation, 7.02 GiB for non-torch memory, and 0.24 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=228941594624` to fit into requested memory, or `--kv-cache-memory=243781042176` to fully utilize gpu memory. Current kv cache memory in use is 229358927872 bytes.
(Worker_TP3 pid=456) INFO 02-15 22:29:55 [gpu_model_runner.py:3378] Graph capturing finished in 14 secs, took 0.24 GiB
(Worker_TP3 pid=456) INFO 02-15 22:29:55 [gpu_worker.py:392] Free memory on device (287.02/287.58 GiB) on startup. Desired GPU memory utilization is (0.95, 273.2 GiB). Actual usage is 42.6 GiB for weight, 9.98 GiB for peak activation, 6.99 GiB for non-torch memory, and 0.24 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=228966760448` to fit into requested memory, or `--kv-cache-memory=243806208000` to fully utilize gpu memory. Current kv cache memory in use is 229384093696 bytes.
(Worker_TP1 pid=454) INFO 02-15 22:29:55 [gpu_model_runner.py:3378] Graph capturing finished in 14 secs, took 0.24 GiB
(Worker_TP1 pid=454) INFO 02-15 22:29:55 [gpu_worker.py:392] Free memory on device (287.02/287.58 GiB) on startup. Desired GPU memory utilization is (0.95, 273.2 GiB). Actual usage is 42.6 GiB for weight, 9.98 GiB for peak activation, 7.01 GiB for non-torch memory, and 0.24 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=228954177536` to fit into requested memory, or `--kv-cache-memory=243793625088` to fully utilize gpu memory. Current kv cache memory in use is 229371510784 bytes.
(Worker_TP7 pid=460) INFO 02-15 22:29:55 [gpu_model_runner.py:3378] Graph capturing finished in 14 secs, took 0.24 GiB
(Worker_TP7 pid=460) INFO 02-15 22:29:55 [gpu_worker.py:392] Free memory on device (287.02/287.58 GiB) on startup. Desired GPU memory utilization is (0.95, 273.2 GiB). Actual usage is 42.6 GiB for weight, 9.98 GiB for peak activation, 6.74 GiB for non-torch memory, and 0.24 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=229239390208` to fit into requested memory, or `--kv-cache-memory=244078837760` to fully utilize gpu memory. Current kv cache memory in use is 229656723456 bytes.
(EngineCore_DP0 pid=303) INFO 02-15 22:29:55 [core.py:210] init engine (profile, create kv cache, warmup model) took 70.09 seconds
(EngineCore_DP0 pid=303) WARNING 02-15 22:29:55 [core.py:112] Using configured V1 scheduler class vllm.v1.core.sched.async_scheduler.AsyncScheduler. This scheduler interface is not public and compatibility may not be maintained.
(EngineCore_DP0 pid=303) INFO 02-15 22:30:01 [core.py:149] Batch queue is enabled with size 2
(EngineCore_DP0 pid=303) WARNING 02-15 22:30:01 [compilation.py:609] Using piecewise compilation with empty splitting_ops and use_inductor_graph_partition=False.
(APIServer pid=1) INFO 02-15 22:30:01 [loggers.py:148] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 6527747
(APIServer pid=1) INFO 02-15 22:30:01 [async_llm.py:164] Torch profiler enabled. AsyncLLM CPU traces will be collected under /app
(APIServer pid=1) INFO 02-15 22:30:02 [api_server.py:1618] Supported_tasks: ['generate']
(APIServer pid=1) WARNING 02-15 22:30:02 [model.py:1455] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
(APIServer pid=1) INFO 02-15 22:30:02 [serving_responses.py:137] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.95}
(APIServer pid=1) INFO 02-15 22:30:02 [serving_chat.py:137] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.95}
(APIServer pid=1) INFO 02-15 22:30:02 [serving_completion.py:76] Using default completion sampling params from model: {'temperature': 0.6, 'top_p': 0.95}
(APIServer pid=1) INFO 02-15 22:30:02 [api_server.py:1895] Starting vLLM API server 0 on http://localhost:8000
(APIServer pid=1) INFO 02-15 22:30:02 [launcher.py:34] Available routes are:
(APIServer pid=1) INFO 02-15 22:30:02 [launcher.py:42] Route: /openapi.json, Methods: HEAD, GET
(APIServer pid=1) INFO 02-15 22:30:02 [launcher.py:42] Route: /docs, Methods: HEAD, GET
(APIServer pid=1) INFO 02-15 22:30:02 [launcher.py:42] Route: /docs/oauth2-redirect, Methods: HEAD, GET
(APIServer pid=1) INFO 02-15 22:30:02 [launcher.py:42] Route: /redoc, Methods: HEAD, GET
(APIServer pid=1) INFO 02-15 22:30:02 [launcher.py:42] Route: /health, Methods: GET
(APIServer pid=1) INFO 02-15 22:30:02 [launcher.py:42] Route: /load, Methods: GET
(APIServer pid=1) INFO 02-15 22:30:02 [launcher.py:42] Route: /ping, Methods: POST
(APIServer pid=1) INFO 02-15 22:30:02 [launcher.py:42] Route: /ping, Methods: GET
(APIServer pid=1) INFO 02-15 22:30:02 [launcher.py:42] Route: /tokenize, Methods: POST
(APIServer pid=1) INFO 02-15 22:30:02 [launcher.py:42] Route: /detokenize, Methods: POST
(APIServer pid=1) INFO 02-15 22:30:02 [launcher.py:42] Route: /v1/models, Methods: GET
(APIServer pid=1) INFO 02-15 22:30:02 [launcher.py:42] Route: /version, Methods: GET
(APIServer pid=1) INFO 02-15 22:30:02 [launcher.py:42] Route: /v1/responses, Methods: POST
(APIServer pid=1) INFO 02-15 22:30:02 [launcher.py:42] Route: /v1/responses/{response_id}, Methods: GET
(APIServer pid=1) INFO 02-15 22:30:02 [launcher.py:42] Route: /v1/responses/{response_id}/cancel, Methods: POST
(APIServer pid=1) INFO 02-15 22:30:02 [launcher.py:42] Route: /v1/chat/completions, Methods: POST
(APIServer pid=1) INFO 02-15 22:30:02 [launcher.py:42] Route: /v1/completions, Methods: POST
(APIServer pid=1) INFO 02-15 22:30:02 [launcher.py:42] Route: /v1/embeddings, Methods: POST
(APIServer pid=1) INFO 02-15 22:30:02 [launcher.py:42] Route: /pooling, Methods: POST
(APIServer pid=1) INFO 02-15 22:30:02 [launcher.py:42] Route: /classify, Methods: POST
(APIServer pid=1) INFO 02-15 22:30:02 [launcher.py:42] Route: /score, Methods: POST
(APIServer pid=1) INFO 02-15 22:30:02 [launcher.py:42] Route: /v1/score, Methods: POST
(APIServer pid=1) INFO 02-15 22:30:02 [launcher.py:42] Route: /v1/audio/transcriptions, Methods: POST
(APIServer pid=1) INFO 02-15 22:30:02 [launcher.py:42] Route: /v1/audio/translations, Methods: POST
(APIServer pid=1) INFO 02-15 22:30:02 [launcher.py:42] Route: /rerank, Methods: POST
(APIServer pid=1) INFO 02-15 22:30:02 [launcher.py:42] Route: /v1/rerank, Methods: POST
(APIServer pid=1) INFO 02-15 22:30:02 [launcher.py:42] Route: /v2/rerank, Methods: POST
(APIServer pid=1) INFO 02-15 22:30:02 [launcher.py:42] Route: /scale_elastic_ep, Methods: POST
(APIServer pid=1) INFO 02-15 22:30:02 [launcher.py:42] Route: /is_scaling_elastic_ep, Methods: POST
(APIServer pid=1) INFO 02-15 22:30:02 [launcher.py:42] Route: /invocations, Methods: POST
(APIServer pid=1) INFO 02-15 22:30:02 [launcher.py:42] Route: /start_profile, Methods: POST
(APIServer pid=1) INFO 02-15 22:30:02 [launcher.py:42] Route: /stop_profile, Methods: POST
(APIServer pid=1) INFO 02-15 22:30:02 [launcher.py:42] Route: /metrics, Methods: GET
(APIServer pid=1) INFO:     Started server process [1]
(APIServer pid=1) INFO:     Waiting for application startup.
(APIServer pid=1) INFO:     Application startup complete.
(APIServer pid=1) INFO 02-15 22:31:53 [chat_utils.py:555] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
(Worker_TP2 pid=455) [aiter] start build [module_fmha_v3_varlen_fwd] under /usr/local/lib/python3.12/dist-packages/aiter/jit/build/module_fmha_v3_varlen_fwd
(Worker_TP3 pid=456) [aiter] waiting for baton release at /usr/local/lib/python3.12/dist-packages/aiter/jit/build/lock_module_fmha_v3_varlen_fwd
(Worker_TP1 pid=454) [aiter] waiting for baton release at /usr/local/lib/python3.12/dist-packages/aiter/jit/build/lock_module_fmha_v3_varlen_fwd
(Worker_TP6 pid=459) [aiter] waiting for baton release at /usr/local/lib/python3.12/dist-packages/aiter/jit/build/lock_module_fmha_v3_varlen_fwd
(Worker_TP7 pid=460) [aiter] waiting for baton release at /usr/local/lib/python3.12/dist-packages/aiter/jit/build/lock_module_fmha_v3_varlen_fwd
(Worker_TP4 pid=457) [aiter] waiting for baton release at /usr/local/lib/python3.12/dist-packages/aiter/jit/build/lock_module_fmha_v3_varlen_fwd
(Worker_TP0 pid=453) [aiter] waiting for baton release at /usr/local/lib/python3.12/dist-packages/aiter/jit/build/lock_module_fmha_v3_varlen_fwd
(Worker_TP5 pid=458) [aiter] waiting for baton release at /usr/local/lib/python3.12/dist-packages/aiter/jit/build/lock_module_fmha_v3_varlen_fwd
(Worker_TP2 pid=455) [aiter] finish build [module_fmha_v3_varlen_fwd], cost 26.2s 
(Worker_TP2 pid=455) [aiter] import [module_fmha_v3_varlen_fwd] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_fmha_v3_varlen_fwd.so
(Worker_TP2 pid=455) [aiter] type hints mismatch, override to --> fmha_v3_varlen_fwd(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, cu_seqlens_k: Optional[torch.Tensor], max_seqlen_q: int, max_seqlen_k: int, min_seqlen_q: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, how_v3_bf16_cvt: int, out: Optional[torch.Tensor] = None, block_table: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> List[torch.Tensor]
(Worker_TP3 pid=456) [aiter] import [module_fmha_v3_varlen_fwd] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_fmha_v3_varlen_fwd.so
(Worker_TP3 pid=456) [aiter] type hints mismatch, override to --> fmha_v3_varlen_fwd(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, cu_seqlens_k: Optional[torch.Tensor], max_seqlen_q: int, max_seqlen_k: int, min_seqlen_q: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, how_v3_bf16_cvt: int, out: Optional[torch.Tensor] = None, block_table: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> List[torch.Tensor]
(Worker_TP1 pid=454) [aiter] import [module_fmha_v3_varlen_fwd] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_fmha_v3_varlen_fwd.so
(Worker_TP1 pid=454) [aiter] type hints mismatch, override to --> fmha_v3_varlen_fwd(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, cu_seqlens_k: Optional[torch.Tensor], max_seqlen_q: int, max_seqlen_k: int, min_seqlen_q: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, how_v3_bf16_cvt: int, out: Optional[torch.Tensor] = None, block_table: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> List[torch.Tensor]
(Worker_TP6 pid=459) [aiter] import [module_fmha_v3_varlen_fwd] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_fmha_v3_varlen_fwd.so
(Worker_TP7 pid=460) [aiter] import [module_fmha_v3_varlen_fwd] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_fmha_v3_varlen_fwd.so
(Worker_TP6 pid=459) [aiter] type hints mismatch, override to --> fmha_v3_varlen_fwd(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, cu_seqlens_k: Optional[torch.Tensor], max_seqlen_q: int, max_seqlen_k: int, min_seqlen_q: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, how_v3_bf16_cvt: int, out: Optional[torch.Tensor] = None, block_table: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> List[torch.Tensor]
(Worker_TP7 pid=460) [aiter] type hints mismatch, override to --> fmha_v3_varlen_fwd(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, cu_seqlens_k: Optional[torch.Tensor], max_seqlen_q: int, max_seqlen_k: int, min_seqlen_q: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, how_v3_bf16_cvt: int, out: Optional[torch.Tensor] = None, block_table: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> List[torch.Tensor]
(Worker_TP4 pid=457) [aiter] import [module_fmha_v3_varlen_fwd] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_fmha_v3_varlen_fwd.so
(Worker_TP4 pid=457) [aiter] type hints mismatch, override to --> fmha_v3_varlen_fwd(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, cu_seqlens_k: Optional[torch.Tensor], max_seqlen_q: int, max_seqlen_k: int, min_seqlen_q: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, how_v3_bf16_cvt: int, out: Optional[torch.Tensor] = None, block_table: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> List[torch.Tensor]
(Worker_TP0 pid=453) [aiter] import [module_fmha_v3_varlen_fwd] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_fmha_v3_varlen_fwd.so
(Worker_TP0 pid=453) [aiter] type hints mismatch, override to --> fmha_v3_varlen_fwd(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, cu_seqlens_k: Optional[torch.Tensor], max_seqlen_q: int, max_seqlen_k: int, min_seqlen_q: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, how_v3_bf16_cvt: int, out: Optional[torch.Tensor] = None, block_table: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> List[torch.Tensor]
(Worker_TP5 pid=458) [aiter] import [module_fmha_v3_varlen_fwd] under /usr/local/lib/python3.12/dist-packages/aiter/jit/module_fmha_v3_varlen_fwd.so
(Worker_TP5 pid=458) [aiter] type hints mismatch, override to --> fmha_v3_varlen_fwd(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, cu_seqlens_k: Optional[torch.Tensor], max_seqlen_q: int, max_seqlen_k: int, min_seqlen_q: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, how_v3_bf16_cvt: int, out: Optional[torch.Tensor] = None, block_table: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> List[torch.Tensor]
(APIServer pid=1) INFO:     127.0.0.1:39578 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=1) INFO 02-15 22:32:23 [loggers.py:128] Engine 000: Avg prompt throughput: 0.7 tokens/s, Avg generation throughput: 4.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
(APIServer pid=1) INFO:     127.0.0.1:59544 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=1) INFO 02-15 22:32:33 [loggers.py:128] Engine 000: Avg prompt throughput: 1.3 tokens/s, Avg generation throughput: 4.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 10.0%
(APIServer pid=1) INFO 02-15 22:32:43 [loggers.py:128] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 10.0%
(APIServer pid=1) INFO:     127.0.0.1:58510 - "POST /v1/completions HTTP/1.1" 200 OK
(APIServer pid=1) INFO 02-15 22:32:53 [loggers.py:128] Engine 000: Avg prompt throughput: 819.2 tokens/s, Avg generation throughput: 19.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 0.0%
(APIServer pid=1) INFO 02-15 22:33:03 [loggers.py:128] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 80.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 0.0%
(APIServer pid=1) INFO:     127.0.0.1:58510 - "POST /v1/completions HTTP/1.1" 200 OK
(APIServer pid=1) INFO:     127.0.0.1:39144 - "POST /v1/completions HTTP/1.1" 200 OK
(APIServer pid=1) INFO:     127.0.0.1:39152 - "POST /v1/completions HTTP/1.1" 200 OK
(APIServer pid=1) INFO:     127.0.0.1:39166 - "POST /v1/completions HTTP/1.1" 200 OK
(APIServer pid=1) INFO 02-15 22:33:13 [loggers.py:128] Engine 000: Avg prompt throughput: 3277.0 tokens/s, Avg generation throughput: 232.6 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 20.0%
(APIServer pid=1) INFO 02-15 22:33:23 [loggers.py:128] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 179.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 20.0%
(APIServer pid=1) INFO 02-15 22:33:33 [loggers.py:128] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 20.0%